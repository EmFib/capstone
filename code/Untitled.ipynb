{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "\n",
    "import json\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fycharts.SpotifyCharts import SpotifyCharts\n",
    "import sqlalchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"../spotify_credentials.json\", \"r\") as json_file:\n",
    "#     creds = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_client_id = creds['SPOTIPY_CLIENT_ID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_**NB: Ran the myCrawler.py from the terminal with the following parameters:**_\n",
    "\n",
    "```python\n",
    "from fycharts.SpotifyCharts import SpotifyCharts\n",
    "import sqlalchemy\n",
    "\n",
    "api = SpotifyCharts()\n",
    "connector = sqlalchemy.create_engine(\"sqlite:///spotifycharts.db\", echo=False)\n",
    "api.viral50Daily(output_file = \"viral_50_daily.csv\", output_db = connector, webhook = [\"https://mywebhookssite.com/post/\"], start = \"2019-05-01\", end = \"2019-05-31\", region = [\"it\", \"de\"])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Position</th>\n",
       "      <th>Track Name</th>\n",
       "      <th>Artist</th>\n",
       "      <th>date</th>\n",
       "      <th>region</th>\n",
       "      <th>spotify_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>bad guy</td>\n",
       "      <td>Billie Eilish</td>\n",
       "      <td>2019-05-01</td>\n",
       "      <td>it</td>\n",
       "      <td>2Fxmhks0bxGSBdJ92vM42m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Verte Ir</td>\n",
       "      <td>DJ Luian</td>\n",
       "      <td>2019-05-01</td>\n",
       "      <td>it</td>\n",
       "      <td>4lzxJ4jCuFDXXGkE1LmpKR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Old Town Road (feat. Billy Ray Cyrus) - Remix</td>\n",
       "      <td>Lil Nas X</td>\n",
       "      <td>2019-05-01</td>\n",
       "      <td>it</td>\n",
       "      <td>6u7jPi22kF8CTQ3rb9DHE7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Old Town Road</td>\n",
       "      <td>Lil Nas X</td>\n",
       "      <td>2019-05-01</td>\n",
       "      <td>it</td>\n",
       "      <td>53CJANUxooaqGOtdsBTh7O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Te Robaré</td>\n",
       "      <td>Nicky Jam</td>\n",
       "      <td>2019-05-01</td>\n",
       "      <td>it</td>\n",
       "      <td>6v0lAdFF4haL8xjBIUjtOw</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Position                                     Track Name         Artist  \\\n",
       "0         1                                        bad guy  Billie Eilish   \n",
       "1         2                                       Verte Ir       DJ Luian   \n",
       "2         3  Old Town Road (feat. Billy Ray Cyrus) - Remix      Lil Nas X   \n",
       "3         4                                  Old Town Road      Lil Nas X   \n",
       "4         5                                      Te Robaré      Nicky Jam   \n",
       "\n",
       "         date region              spotify_id  \n",
       "0  2019-05-01     it  2Fxmhks0bxGSBdJ92vM42m  \n",
       "1  2019-05-01     it  4lzxJ4jCuFDXXGkE1LmpKR  \n",
       "2  2019-05-01     it  6u7jPi22kF8CTQ3rb9DHE7  \n",
       "3  2019-05-01     it  53CJANUxooaqGOtdsBTh7O  \n",
       "4  2019-05-01     it  6v0lAdFF4haL8xjBIUjtOw  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "may_19_it_de = pd.read_csv('../viral_50_daily.csv')\n",
    "may_19_it_de.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# api = SpotifyCharts()\n",
    "# connector = sqlalchemy.create_engine(\"sqlite:///spotifycharts.db\", echo=False)\n",
    "# api.viral50Daily(output_file = \"viral_50_daily.csv\", output_db = connector, webhook = [\"https://mywebhookssite.com/post/\"], start = \"2019-05-01\", end = \"2019-05-31\", region = [\"it\", \"de\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/emilynaftalin/Data_Science/General Assembly/dsi/capstone/code'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : 09/02/2021 11:38:08 PM : Extracting top 200 daily for 2019-05-01 - us\n",
      "INFO : 09/02/2021 11:38:09 PM : Extracting top 200 daily for 2019-05-02 - us\n",
      "INFO : 09/02/2021 11:38:09 PM : Appending data to the table top_200_daily\n",
      "INFO : 09/02/2021 11:38:09 PM : POSTing data to the endpoint https://mywebhookssite.com/post/\n",
      "INFO : 09/02/2021 11:38:09 PM : Appending data to the file ../data/us_may19_200.csv...\n",
      "INFO : 09/02/2021 11:38:09 PM : Done appending to the file ../data/us_may19_200.csv!!!\n",
      "Exception in thread Thread-9:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/urllib3/connection.py\", line 159, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/urllib3/util/connection.py\", line 61, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/socket.py\", line 918, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 8] nodename nor servname provided, or not known\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 670, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 381, in _make_request\n",
      "INFO : 09/02/2021 11:38:09 PM : Done appending to the table top_200_daily!!!\n",
      "    self._validate_conn(conn)\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 978, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/urllib3/connection.py\", line 309, in connect\n",
      "    conn = self._new_conn()\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/urllib3/connection.py\", line 171, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x7fc393932f10>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/requests/adapters.py\", line 439, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 726, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/urllib3/util/retry.py\", line 446, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='mywebhookssite.com', port=443): Max retries exceeded with url: /post/ (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fc393932f10>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/fycharts/SpotifyCharts.py\", line 88, in __post_to_endpoint_from_queue\n",
      "    postToRestEndpoint(df, url, what_data)\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/fycharts/write_to_outputs.py\", line 61, in postToRestEndpoint\n",
      "    raise(e)\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/fycharts/write_to_outputs.py\", line 58, in postToRestEndpoint\n",
      "    requests.post(url, json = dump)\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/requests/api.py\", line 119, in post\n",
      "    return request('post', url, data=data, json=json, **kwargs)\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/requests/api.py\", line 61, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/requests/sessions.py\", line 530, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/requests/sessions.py\", line 643, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/requests/adapters.py\", line 516, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='mywebhookssite.com', port=443): Max retries exceeded with url: /post/ (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fc393932f10>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/threading.py\", line 932, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/fycharts/SpotifyCharts.py\", line 90, in __post_to_endpoint_from_queue\n",
      "    raise RuntimeError(e)\n",
      "RuntimeError: HTTPSConnectionPool(host='mywebhookssite.com', port=443): Max retries exceeded with url: /post/ (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fc393932f10>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\n",
      "INFO : 09/02/2021 11:38:10 PM : Extracting top 200 daily for 2019-05-03 - us\n",
      "INFO : 09/02/2021 11:38:10 PM : Appending data to the table top_200_daily\n",
      "INFO : 09/02/2021 11:38:10 PM : Appending data to the file ../data/us_may19_200.csv...\n",
      "INFO : 09/02/2021 11:38:10 PM : Done appending to the file ../data/us_may19_200.csv!!!\n",
      "INFO : 09/02/2021 11:38:10 PM : Done appending to the table top_200_daily!!!\n",
      "INFO : 09/02/2021 11:38:10 PM : Extracting top 200 daily for 2019-05-04 - us\n",
      "INFO : 09/02/2021 11:38:10 PM : Appending data to the table top_200_daily\n",
      "INFO : 09/02/2021 11:38:10 PM : Appending data to the file ../data/us_may19_200.csv...\n",
      "INFO : 09/02/2021 11:38:10 PM : Done appending to the file ../data/us_may19_200.csv!!!\n",
      "INFO : 09/02/2021 11:38:10 PM : Done appending to the table top_200_daily!!!\n",
      "INFO : 09/02/2021 11:38:11 PM : Extracting top 200 daily for 2019-05-05 - us\n",
      "INFO : 09/02/2021 11:38:11 PM : Appending data to the table top_200_daily\n",
      "INFO : 09/02/2021 11:38:11 PM : Appending data to the file ../data/us_may19_200.csv...\n",
      "INFO : 09/02/2021 11:38:11 PM : Done appending to the file ../data/us_may19_200.csv!!!\n",
      "INFO : 09/02/2021 11:38:11 PM : Done appending to the table top_200_daily!!!\n",
      "INFO : 09/02/2021 11:38:12 PM : Extracting top 200 daily for 2019-05-06 - us\n",
      "INFO : 09/02/2021 11:38:12 PM : Appending data to the table top_200_daily\n",
      "INFO : 09/02/2021 11:38:12 PM : Appending data to the file ../data/us_may19_200.csv...\n",
      "INFO : 09/02/2021 11:38:12 PM : Done appending to the file ../data/us_may19_200.csv!!!\n",
      "INFO : 09/02/2021 11:38:12 PM : Done appending to the table top_200_daily!!!\n",
      "INFO : 09/02/2021 11:38:13 PM : Extracting top 200 daily for 2019-05-07 - us\n",
      "INFO : 09/02/2021 11:38:13 PM : Appending data to the table top_200_daily\n",
      "INFO : 09/02/2021 11:38:13 PM : Appending data to the file ../data/us_may19_200.csv...\n",
      "INFO : 09/02/2021 11:38:13 PM : Done appending to the table top_200_daily!!!\n",
      "INFO : 09/02/2021 11:38:13 PM : Done appending to the file ../data/us_may19_200.csv!!!\n",
      "INFO : 09/02/2021 11:38:14 PM : Extracting top 200 daily for 2019-05-08 - us\n",
      "INFO : 09/02/2021 11:38:14 PM : Appending data to the table top_200_daily\n",
      "INFO : 09/02/2021 11:38:14 PM : Appending data to the file ../data/us_may19_200.csv...\n",
      "INFO : 09/02/2021 11:38:14 PM : Done appending to the file ../data/us_may19_200.csv!!!\n",
      "INFO : 09/02/2021 11:38:14 PM : Done appending to the table top_200_daily!!!\n",
      "INFO : 09/02/2021 11:38:15 PM : Extracting top 200 daily for 2019-05-09 - us\n",
      "INFO : 09/02/2021 11:38:15 PM : Appending data to the table top_200_daily\n",
      "INFO : 09/02/2021 11:38:15 PM : Appending data to the file ../data/us_may19_200.csv...\n",
      "INFO : 09/02/2021 11:38:15 PM : Done appending to the file ../data/us_may19_200.csv!!!\n",
      "INFO : 09/02/2021 11:38:15 PM : Done appending to the table top_200_daily!!!\n",
      "INFO : 09/02/2021 11:38:15 PM : Extracting top 200 daily for 2019-05-10 - us\n",
      "INFO : 09/02/2021 11:38:15 PM : Appending data to the file ../data/us_may19_200.csv...\n",
      "INFO : 09/02/2021 11:38:15 PM : Appending data to the table top_200_daily\n",
      "INFO : 09/02/2021 11:38:15 PM : Done appending to the file ../data/us_may19_200.csv!!!\n",
      "INFO : 09/02/2021 11:38:15 PM : Done appending to the table top_200_daily!!!\n",
      "INFO : 09/02/2021 11:38:16 PM : Extracting top 200 daily for 2019-05-11 - us\n",
      "INFO : 09/02/2021 11:38:16 PM : Appending data to the table top_200_daily\n",
      "INFO : 09/02/2021 11:38:16 PM : Appending data to the file ../data/us_may19_200.csv...\n",
      "INFO : 09/02/2021 11:38:16 PM : Done appending to the file ../data/us_may19_200.csv!!!\n",
      "INFO : 09/02/2021 11:38:16 PM : Done appending to the table top_200_daily!!!\n",
      "INFO : 09/02/2021 11:38:17 PM : Extracting top 200 daily for 2019-05-12 - us\n",
      "INFO : 09/02/2021 11:38:17 PM : Appending data to the table top_200_daily\n",
      "INFO : 09/02/2021 11:38:17 PM : Appending data to the file ../data/us_may19_200.csv...\n",
      "INFO : 09/02/2021 11:38:17 PM : Done appending to the file ../data/us_may19_200.csv!!!\n",
      "INFO : 09/02/2021 11:38:17 PM : Done appending to the table top_200_daily!!!\n",
      "INFO : 09/02/2021 11:38:18 PM : Extracting top 200 daily for 2019-05-13 - us\n",
      "INFO : 09/02/2021 11:38:18 PM : Appending data to the table top_200_daily\n",
      "INFO : 09/02/2021 11:38:18 PM : Appending data to the file ../data/us_may19_200.csv...\n",
      "INFO : 09/02/2021 11:38:18 PM : Done appending to the table top_200_daily!!!\n",
      "INFO : 09/02/2021 11:38:18 PM : Done appending to the file ../data/us_may19_200.csv!!!\n",
      "INFO : 09/02/2021 11:38:19 PM : Extracting top 200 daily for 2019-05-14 - us\n",
      "INFO : 09/02/2021 11:38:19 PM : Appending data to the table top_200_daily\n",
      "INFO : 09/02/2021 11:38:19 PM : Appending data to the file ../data/us_may19_200.csv...\n",
      "INFO : 09/02/2021 11:38:19 PM : Done appending to the file ../data/us_may19_200.csv!!!\n",
      "INFO : 09/02/2021 11:38:19 PM : Done appending to the table top_200_daily!!!\n",
      "INFO : 09/02/2021 11:38:20 PM : Extracting top 200 daily for 2019-05-15 - us\n",
      "INFO : 09/02/2021 11:38:20 PM : Appending data to the table top_200_daily\n",
      "INFO : 09/02/2021 11:38:20 PM : Appending data to the file ../data/us_may19_200.csv...\n",
      "INFO : 09/02/2021 11:38:20 PM : Done appending to the file ../data/us_may19_200.csv!!!\n",
      "INFO : 09/02/2021 11:38:20 PM : Done appending to the table top_200_daily!!!\n",
      "INFO : 09/02/2021 11:38:20 PM : Extracting top 200 daily for 2019-05-16 - us\n",
      "INFO : 09/02/2021 11:38:20 PM : Appending data to the table top_200_daily\n",
      "INFO : 09/02/2021 11:38:20 PM : Appending data to the file ../data/us_may19_200.csv...\n",
      "INFO : 09/02/2021 11:38:20 PM : Done appending to the file ../data/us_may19_200.csv!!!\n",
      "INFO : 09/02/2021 11:38:20 PM : Done appending to the table top_200_daily!!!\n",
      "INFO : 09/02/2021 11:38:21 PM : Extracting top 200 daily for 2019-05-17 - us\n",
      "INFO : 09/02/2021 11:38:21 PM : Appending data to the table top_200_daily\n",
      "INFO : 09/02/2021 11:38:21 PM : Appending data to the file ../data/us_may19_200.csv...\n",
      "INFO : 09/02/2021 11:38:21 PM : Done appending to the file ../data/us_may19_200.csv!!!\n",
      "INFO : 09/02/2021 11:38:21 PM : Done appending to the table top_200_daily!!!\n",
      "INFO : 09/02/2021 11:38:22 PM : Extracting top 200 daily for 2019-05-18 - us\n",
      "INFO : 09/02/2021 11:38:22 PM : Appending data to the table top_200_daily\n",
      "INFO : 09/02/2021 11:38:22 PM : Appending data to the file ../data/us_may19_200.csv...\n",
      "INFO : 09/02/2021 11:38:22 PM : Done appending to the file ../data/us_may19_200.csv!!!\n",
      "INFO : 09/02/2021 11:38:22 PM : Done appending to the table top_200_daily!!!\n",
      "INFO : 09/02/2021 11:38:23 PM : Extracting top 200 daily for 2019-05-19 - us\n",
      "INFO : 09/02/2021 11:38:23 PM : Appending data to the table top_200_daily\n",
      "INFO : 09/02/2021 11:38:23 PM : Appending data to the file ../data/us_may19_200.csv...\n",
      "INFO : 09/02/2021 11:38:23 PM : Done appending to the file ../data/us_may19_200.csv!!!\n",
      "INFO : 09/02/2021 11:38:23 PM : Done appending to the table top_200_daily!!!\n",
      "INFO : 09/02/2021 11:38:23 PM : Extracting top 200 daily for 2019-05-20 - us\n",
      "INFO : 09/02/2021 11:38:23 PM : Appending data to the table top_200_daily\n",
      "INFO : 09/02/2021 11:38:23 PM : Appending data to the file ../data/us_may19_200.csv...\n",
      "INFO : 09/02/2021 11:38:23 PM : Done appending to the file ../data/us_may19_200.csv!!!\n",
      "INFO : 09/02/2021 11:38:23 PM : Done appending to the table top_200_daily!!!\n",
      "INFO : 09/02/2021 11:38:24 PM : Extracting top 200 daily for 2019-05-21 - us\n",
      "INFO : 09/02/2021 11:38:24 PM : Appending data to the table top_200_daily\n",
      "INFO : 09/02/2021 11:38:24 PM : Appending data to the file ../data/us_may19_200.csv...\n",
      "INFO : 09/02/2021 11:38:24 PM : Done appending to the file ../data/us_may19_200.csv!!!\n",
      "INFO : 09/02/2021 11:38:24 PM : Done appending to the table top_200_daily!!!\n",
      "INFO : 09/02/2021 11:38:25 PM : Extracting top 200 daily for 2019-05-22 - us\n",
      "INFO : 09/02/2021 11:38:25 PM : Appending data to the table top_200_daily\n",
      "INFO : 09/02/2021 11:38:25 PM : Appending data to the file ../data/us_may19_200.csv...\n",
      "INFO : 09/02/2021 11:38:25 PM : Done appending to the file ../data/us_may19_200.csv!!!\n",
      "INFO : 09/02/2021 11:38:25 PM : Done appending to the table top_200_daily!!!\n",
      "INFO : 09/02/2021 11:38:26 PM : Extracting top 200 daily for 2019-05-23 - us\n",
      "INFO : 09/02/2021 11:38:26 PM : Appending data to the table top_200_daily\n",
      "INFO : 09/02/2021 11:38:26 PM : Appending data to the file ../data/us_may19_200.csv...\n",
      "INFO : 09/02/2021 11:38:26 PM : Done appending to the file ../data/us_may19_200.csv!!!\n",
      "INFO : 09/02/2021 11:38:26 PM : Done appending to the table top_200_daily!!!\n",
      "INFO : 09/02/2021 11:38:27 PM : Extracting top 200 daily for 2019-05-24 - us\n",
      "INFO : 09/02/2021 11:38:27 PM : Appending data to the file ../data/us_may19_200.csv...\n",
      "INFO : 09/02/2021 11:38:27 PM : Appending data to the table top_200_daily\n",
      "INFO : 09/02/2021 11:38:27 PM : Done appending to the file ../data/us_may19_200.csv!!!\n",
      "INFO : 09/02/2021 11:38:27 PM : Done appending to the table top_200_daily!!!\n",
      "INFO : 09/02/2021 11:38:27 PM : Extracting top 200 daily for 2019-05-25 - us\n",
      "INFO : 09/02/2021 11:38:27 PM : Appending data to the table top_200_daily\n",
      "INFO : 09/02/2021 11:38:27 PM : Appending data to the file ../data/us_may19_200.csv...\n",
      "INFO : 09/02/2021 11:38:27 PM : Done appending to the file ../data/us_may19_200.csv!!!\n",
      "INFO : 09/02/2021 11:38:27 PM : Done appending to the table top_200_daily!!!\n",
      "INFO : 09/02/2021 11:38:28 PM : Extracting top 200 daily for 2019-05-26 - us\n",
      "INFO : 09/02/2021 11:38:28 PM : Appending data to the table top_200_daily\n",
      "INFO : 09/02/2021 11:38:28 PM : Appending data to the file ../data/us_may19_200.csv...\n",
      "INFO : 09/02/2021 11:38:28 PM : Done appending to the file ../data/us_may19_200.csv!!!\n",
      "INFO : 09/02/2021 11:38:28 PM : Done appending to the table top_200_daily!!!\n",
      "INFO : 09/02/2021 11:38:29 PM : Extracting top 200 daily for 2019-05-27 - us\n",
      "INFO : 09/02/2021 11:38:29 PM : Appending data to the table top_200_daily\n",
      "INFO : 09/02/2021 11:38:29 PM : Appending data to the file ../data/us_may19_200.csv...\n",
      "INFO : 09/02/2021 11:38:29 PM : Done appending to the file ../data/us_may19_200.csv!!!\n",
      "INFO : 09/02/2021 11:38:29 PM : Done appending to the table top_200_daily!!!\n",
      "INFO : 09/02/2021 11:38:30 PM : Extracting top 200 daily for 2019-05-28 - us\n",
      "INFO : 09/02/2021 11:38:30 PM : Appending data to the file ../data/us_may19_200.csv...\n",
      "INFO : 09/02/2021 11:38:30 PM : Appending data to the table top_200_daily\n",
      "INFO : 09/02/2021 11:38:30 PM : Done appending to the file ../data/us_may19_200.csv!!!\n",
      "INFO : 09/02/2021 11:38:30 PM : Done appending to the table top_200_daily!!!\n",
      "INFO : 09/02/2021 11:38:31 PM : Extracting top 200 daily for 2019-05-29 - us\n",
      "INFO : 09/02/2021 11:38:31 PM : Appending data to the table top_200_daily\n",
      "INFO : 09/02/2021 11:38:31 PM : Appending data to the file ../data/us_may19_200.csv...\n",
      "INFO : 09/02/2021 11:38:31 PM : Done appending to the table top_200_daily!!!\n",
      "INFO : 09/02/2021 11:38:31 PM : Done appending to the file ../data/us_may19_200.csv!!!\n",
      "INFO : 09/02/2021 11:38:31 PM : Extracting top 200 daily for 2019-05-30 - us\n",
      "INFO : 09/02/2021 11:38:31 PM : Appending data to the table top_200_daily\n",
      "INFO : 09/02/2021 11:38:31 PM : Appending data to the file ../data/us_may19_200.csv...\n",
      "INFO : 09/02/2021 11:38:31 PM : Done appending to the file ../data/us_may19_200.csv!!!\n",
      "INFO : 09/02/2021 11:38:31 PM : Done appending to the table top_200_daily!!!\n",
      "INFO : 09/02/2021 11:38:32 PM : Extracting top 200 daily for 2019-05-31 - us\n",
      "INFO : 09/02/2021 11:38:32 PM : Appending data to the table top_200_daily\n",
      "INFO : 09/02/2021 11:38:32 PM : Appending data to the file ../data/us_may19_200.csv...\n",
      "INFO : 09/02/2021 11:38:32 PM : Done appending to the file ../data/us_may19_200.csv!!!\n",
      "INFO : 09/02/2021 11:38:32 PM : Done appending to the table top_200_daily!!!\n",
      "INFO : 09/02/2021 11:38:33 PM : Appending data to the table top_200_daily\n",
      "INFO : 09/02/2021 11:38:33 PM : Appending data to the file ../data/us_may19_200.csv...\n",
      "INFO : 09/02/2021 11:38:33 PM : Done appending to the file ../data/us_may19_200.csv!!!\n",
      "INFO : 09/02/2021 11:38:33 PM : Done appending to the table top_200_daily!!!\n"
     ]
    }
   ],
   "source": [
    "api = SpotifyCharts()\n",
    "connector = sqlalchemy.create_engine(\"sqlite:///spotifycharts.db\", echo=False)\n",
    "api.top200Daily(output_file = \"../data/us_may19_200.csv\", output_db = connector, webhook = [\"https://mywebhookssite.com/post/\"], start = \"2019-05-01\", end = \"2019-05-31\", region = \"us\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Position</th>\n",
       "      <th>Track Name</th>\n",
       "      <th>Artist</th>\n",
       "      <th>Streams</th>\n",
       "      <th>date</th>\n",
       "      <th>region</th>\n",
       "      <th>spotify_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Old Town Road (feat. Billy Ray Cyrus) - Remix</td>\n",
       "      <td>Lil Nas X</td>\n",
       "      <td>1490233</td>\n",
       "      <td>2019-05-01</td>\n",
       "      <td>us</td>\n",
       "      <td>6u7jPi22kF8CTQ3rb9DHE7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>bad guy</td>\n",
       "      <td>Billie Eilish</td>\n",
       "      <td>1416174</td>\n",
       "      <td>2019-05-01</td>\n",
       "      <td>us</td>\n",
       "      <td>2Fxmhks0bxGSBdJ92vM42m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>ME! (feat. Brendon Urie of Panic! At The Disco)</td>\n",
       "      <td>Taylor Swift</td>\n",
       "      <td>1407982</td>\n",
       "      <td>2019-05-01</td>\n",
       "      <td>us</td>\n",
       "      <td>4Sib57MmYGJzSvkW84jTwh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Sunflower - Spider-Man: Into the Spider-Verse</td>\n",
       "      <td>Post Malone</td>\n",
       "      <td>1065351</td>\n",
       "      <td>2019-05-01</td>\n",
       "      <td>us</td>\n",
       "      <td>3KkXRkHbMCARz0aVfEt68P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>MIDDLE CHILD</td>\n",
       "      <td>J. Cole</td>\n",
       "      <td>976752</td>\n",
       "      <td>2019-05-01</td>\n",
       "      <td>us</td>\n",
       "      <td>2JvzF1RMd7lE3KmFlsyZD8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Position                                       Track Name         Artist  \\\n",
       "0         1    Old Town Road (feat. Billy Ray Cyrus) - Remix      Lil Nas X   \n",
       "1         2                                          bad guy  Billie Eilish   \n",
       "2         3  ME! (feat. Brendon Urie of Panic! At The Disco)   Taylor Swift   \n",
       "3         4    Sunflower - Spider-Man: Into the Spider-Verse    Post Malone   \n",
       "4         5                                     MIDDLE CHILD        J. Cole   \n",
       "\n",
       "   Streams        date region              spotify_id  \n",
       "0  1490233  2019-05-01     us  6u7jPi22kF8CTQ3rb9DHE7  \n",
       "1  1416174  2019-05-01     us  2Fxmhks0bxGSBdJ92vM42m  \n",
       "2  1407982  2019-05-01     us  4Sib57MmYGJzSvkW84jTwh  \n",
       "3  1065351  2019-05-01     us  3KkXRkHbMCARz0aVfEt68P  \n",
       "4   976752  2019-05-01     us  2JvzF1RMd7lE3KmFlsyZD8  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "us_may19 = pd.read_csv('../data/us_may19_200.csv')\n",
    "us_may19.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6200, 7)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "us_may19.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "6200/200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Position</th>\n",
       "      <th>Track Name</th>\n",
       "      <th>Artist</th>\n",
       "      <th>Streams</th>\n",
       "      <th>date</th>\n",
       "      <th>region</th>\n",
       "      <th>spotify_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Old Town Road (feat. Billy Ray Cyrus) - Remix</td>\n",
       "      <td>Lil Nas X</td>\n",
       "      <td>1490233</td>\n",
       "      <td>2019-05-01</td>\n",
       "      <td>us</td>\n",
       "      <td>6u7jPi22kF8CTQ3rb9DHE7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>1</td>\n",
       "      <td>Old Town Road (feat. Billy Ray Cyrus) - Remix</td>\n",
       "      <td>Lil Nas X</td>\n",
       "      <td>1531705</td>\n",
       "      <td>2019-05-02</td>\n",
       "      <td>us</td>\n",
       "      <td>6u7jPi22kF8CTQ3rb9DHE7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4800</th>\n",
       "      <td>1</td>\n",
       "      <td>Old Town Road (feat. Billy Ray Cyrus) - Remix</td>\n",
       "      <td>Lil Nas X</td>\n",
       "      <td>1671748</td>\n",
       "      <td>2019-05-25</td>\n",
       "      <td>us</td>\n",
       "      <td>6u7jPi22kF8CTQ3rb9DHE7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5000</th>\n",
       "      <td>1</td>\n",
       "      <td>Old Town Road (feat. Billy Ray Cyrus) - Remix</td>\n",
       "      <td>Lil Nas X</td>\n",
       "      <td>1463456</td>\n",
       "      <td>2019-05-26</td>\n",
       "      <td>us</td>\n",
       "      <td>6u7jPi22kF8CTQ3rb9DHE7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5200</th>\n",
       "      <td>1</td>\n",
       "      <td>Old Town Road (feat. Billy Ray Cyrus) - Remix</td>\n",
       "      <td>Lil Nas X</td>\n",
       "      <td>1439533</td>\n",
       "      <td>2019-05-27</td>\n",
       "      <td>us</td>\n",
       "      <td>6u7jPi22kF8CTQ3rb9DHE7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Position                                     Track Name     Artist  \\\n",
       "0            1  Old Town Road (feat. Billy Ray Cyrus) - Remix  Lil Nas X   \n",
       "200          1  Old Town Road (feat. Billy Ray Cyrus) - Remix  Lil Nas X   \n",
       "4800         1  Old Town Road (feat. Billy Ray Cyrus) - Remix  Lil Nas X   \n",
       "5000         1  Old Town Road (feat. Billy Ray Cyrus) - Remix  Lil Nas X   \n",
       "5200         1  Old Town Road (feat. Billy Ray Cyrus) - Remix  Lil Nas X   \n",
       "\n",
       "      Streams        date region              spotify_id  \n",
       "0     1490233  2019-05-01     us  6u7jPi22kF8CTQ3rb9DHE7  \n",
       "200   1531705  2019-05-02     us  6u7jPi22kF8CTQ3rb9DHE7  \n",
       "4800  1671748  2019-05-25     us  6u7jPi22kF8CTQ3rb9DHE7  \n",
       "5000  1463456  2019-05-26     us  6u7jPi22kF8CTQ3rb9DHE7  \n",
       "5200  1439533  2019-05-27     us  6u7jPi22kF8CTQ3rb9DHE7  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "us_may19[ (us_may19['Track Name']=='Old Town Road (feat. Billy Ray Cyrus) - Remix')  & (us_may19['Position']==1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# everything I need to interact with Spotify API and ther\n",
    "\n",
    "import spotipy\n",
    "from spotipy.oauth2 import SpotifyClientCredentials\n",
    "\n",
    "with open(\"../spotify_credentials.json\", \"r\") as json_file:\n",
    "    creds = json.load(json_file)\n",
    "\n",
    "my_client_id = creds['SPOTIPY_CLIENT_ID']\n",
    "my_client_secret = creds['SPOTIPY_CLIENT_SECRET']\n",
    "\n",
    "client_credentials_manager = SpotifyClientCredentials(client_id=my_client_id, client_secret=my_client_secret)\n",
    "sp = spotipy.Spotify(client_credentials_manager=client_credentials_manager)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tracks where no audio features were available: 0\n",
      "Number of usable tracks: 6200\n"
     ]
    }
   ],
   "source": [
    "# trying to get audio features! \n",
    "\n",
    "rows = []          #list to append each track features to \n",
    "batchsize = 100    #max number of track ids we're allowed to submit per query\n",
    "None_counter = 0   #count if there are any songs without any audio features\n",
    "\n",
    "for i in range(0,len(us_may19['spotify_id']), batchsize):    #batchsize = offset\n",
    "    batch = us_may19['spotify_id'][i:i+batchsize]            #batch is now a list of all 929 track_ids\n",
    "    \n",
    "    feature_results = sp.audio_features(batch)              #begins querying the audio features endpoint\n",
    "\n",
    "    for i, t in enumerate(feature_results):\n",
    "        if t == None:                               #if the audio features for a song are missing, count 1        \n",
    "            None_counter += 1          \n",
    "        else:\n",
    "            rows.append(t)  \n",
    "print('Number of tracks where no audio features were available:',None_counter)\n",
    "print('Number of usable tracks:', len(rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
