{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "\n",
    "import json\n",
    "\n",
    "from fycharts.SpotifyCharts import SpotifyCharts\n",
    "import sqlalchemy\n",
    "\n",
    "import spotipy\n",
    "from spotipy.oauth2 import SpotifyClientCredentials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spotify Client Credentials \n",
    "\n",
    "+ I'm obtaining access to Spotify's song data using its Client Credentials Flow, which will allow me to access public data. The public data include the audio features associated with each track which append to the top charting song data for the trend analysis. I use Spotify's wrapper library [Spotipy](https://spotipy.readthedocs.io/en/2.16.1/) which lets me gain authorization and execute requests more easily. \n",
    "\n",
    "+ Note that the Client Credentials is distinct from Spotify's other access option, Authorization Code Flow. The former allows access to public data, while the latter provides access to private user data. \n",
    "\n",
    "+ _**Citation:**_ \n",
    "    - Code for for authorization inspired by a previous GA-DSI student's capstone [CNN_for_Dance_Music_Classification](https://github.com/amytaylor330/CNN_for_Dance_Music_Classification), which is very informative and well-documented. \n",
    "    - Support for how to structure the `json.load(json_file)` control flow came from Hovanes Gasparian. Thanks Hov!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../spotify_credentials.json\", \"r\") as json_file:\n",
    "    creds = json.load(json_file)\n",
    "\n",
    "my_client_id = creds['SPOTIPY_CLIENT_ID']\n",
    "my_client_secret = creds['SPOTIPY_CLIENT_SECRET']\n",
    "\n",
    "client_credentials_manager = SpotifyClientCredentials(client_id=my_client_id, client_secret=my_client_secret)\n",
    "sp = spotipy.Spotify(client_credentials_manager=client_credentials_manager)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Italy's top songs 2017-2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ _All lists of top 200 / viral 50 song gathered from [Spotify Charts Regional](https://spotifycharts.com/regional/)_\n",
    "\n",
    "+ _Data are written as both a csv file and a SQLLite db._ \n",
    "\n",
    "+ _**Citation:** Code for how to scrape [Spotify Charts Regional](https://spotifycharts.com/regional/) is inspired by the excellent documentation for the [Unofficial Spotify Charts API](https://github.com/kelvingakuo/fycharts) called `fycharts`._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Italy Top 200 Weekly, 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : 10/02/2021 11:08:24 PM : The start date 2017-01-01 provided for top200Weekly is invalid. Wanna give one these a try? ['2017-01-06', '2017-01-13', '2017-01-20', '2017-01-27', '2017-02-03']\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter (1) to use the first suggestion, or (2) to quit and set yourself:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : 10/02/2021 11:08:52 PM : Extracting top 200 weekly for 2017-01-06--2017-01-13 - it\n",
      "INFO : 10/02/2021 11:08:54 PM : Extracting top 200 weekly for 2017-01-13--2017-01-20 - it\n",
      "INFO : 10/02/2021 11:08:54 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:08:54 PM : POSTing data to the endpoint https://mywebhookssite.com/post/\n",
      "INFO : 10/02/2021 11:08:54 PM : Appending data to the file ../data/italy_2017.csv...\n",
      "INFO : 10/02/2021 11:08:54 PM : Done appending to the file ../data/italy_2017.csv!!!\n",
      "Exception in thread Thread-6:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/urllib3/connection.py\", line 159, in _new_conn\n",
      "INFO : 10/02/2021 11:08:54 PM : Done appending to the table top_200_weekly!!!\n",
      "    conn = connection.create_connection(\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/urllib3/util/connection.py\", line 61, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/socket.py\", line 918, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 8] nodename nor servname provided, or not known\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 670, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 381, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 978, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/urllib3/connection.py\", line 309, in connect\n",
      "    conn = self._new_conn()\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/urllib3/connection.py\", line 171, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x7f98a4c47a90>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/requests/adapters.py\", line 439, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 726, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/urllib3/util/retry.py\", line 446, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='mywebhookssite.com', port=443): Max retries exceeded with url: /post/ (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f98a4c47a90>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/fycharts/SpotifyCharts.py\", line 88, in __post_to_endpoint_from_queue\n",
      "    postToRestEndpoint(df, url, what_data)\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/fycharts/write_to_outputs.py\", line 61, in postToRestEndpoint\n",
      "    raise(e)\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/fycharts/write_to_outputs.py\", line 58, in postToRestEndpoint\n",
      "    requests.post(url, json = dump)\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/requests/api.py\", line 119, in post\n",
      "    return request('post', url, data=data, json=json, **kwargs)\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/requests/api.py\", line 61, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/requests/sessions.py\", line 530, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/requests/sessions.py\", line 643, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/requests/adapters.py\", line 516, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='mywebhookssite.com', port=443): Max retries exceeded with url: /post/ (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f98a4c47a90>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/threading.py\", line 932, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/fycharts/SpotifyCharts.py\", line 90, in __post_to_endpoint_from_queue\n",
      "    raise RuntimeError(e)\n",
      "RuntimeError: HTTPSConnectionPool(host='mywebhookssite.com', port=443): Max retries exceeded with url: /post/ (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f98a4c47a90>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\n",
      "INFO : 10/02/2021 11:08:55 PM : Extracting top 200 weekly for 2017-01-20--2017-01-27 - it\n",
      "INFO : 10/02/2021 11:08:55 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:08:55 PM : Appending data to the file ../data/italy_2017.csv...\n",
      "INFO : 10/02/2021 11:08:55 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:08:55 PM : Done appending to the file ../data/italy_2017.csv!!!\n",
      "INFO : 10/02/2021 11:08:56 PM : Extracting top 200 weekly for 2017-01-27--2017-02-03 - it\n",
      "INFO : 10/02/2021 11:08:56 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:08:56 PM : Appending data to the file ../data/italy_2017.csv...\n",
      "INFO : 10/02/2021 11:08:56 PM : Done appending to the file ../data/italy_2017.csv!!!\n",
      "INFO : 10/02/2021 11:08:56 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:08:57 PM : Extracting top 200 weekly for 2017-02-03--2017-02-10 - it\n",
      "INFO : 10/02/2021 11:08:57 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:08:57 PM : Appending data to the file ../data/italy_2017.csv...\n",
      "INFO : 10/02/2021 11:08:57 PM : Done appending to the file ../data/italy_2017.csv!!!\n",
      "INFO : 10/02/2021 11:08:57 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:08:58 PM : Extracting top 200 weekly for 2017-02-10--2017-02-17 - it\n",
      "INFO : 10/02/2021 11:08:58 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:08:58 PM : Appending data to the file ../data/italy_2017.csv...\n",
      "INFO : 10/02/2021 11:08:58 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:08:58 PM : Done appending to the file ../data/italy_2017.csv!!!\n",
      "INFO : 10/02/2021 11:08:59 PM : Extracting top 200 weekly for 2017-02-17--2017-02-24 - it\n",
      "INFO : 10/02/2021 11:08:59 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:08:59 PM : Appending data to the file ../data/italy_2017.csv...\n",
      "INFO : 10/02/2021 11:08:59 PM : Done appending to the file ../data/italy_2017.csv!!!\n",
      "INFO : 10/02/2021 11:08:59 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:09:00 PM : Extracting top 200 weekly for 2017-02-24--2017-03-03 - it\n",
      "INFO : 10/02/2021 11:09:00 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:09:00 PM : Appending data to the file ../data/italy_2017.csv...\n",
      "INFO : 10/02/2021 11:09:00 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:09:00 PM : Done appending to the file ../data/italy_2017.csv!!!\n",
      "INFO : 10/02/2021 11:09:01 PM : Extracting top 200 weekly for 2017-03-03--2017-03-10 - it\n",
      "INFO : 10/02/2021 11:09:01 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:09:01 PM : Appending data to the file ../data/italy_2017.csv...\n",
      "INFO : 10/02/2021 11:09:01 PM : Done appending to the file ../data/italy_2017.csv!!!\n",
      "INFO : 10/02/2021 11:09:01 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:09:02 PM : Extracting top 200 weekly for 2017-03-10--2017-03-17 - it\n",
      "INFO : 10/02/2021 11:09:02 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:09:02 PM : Appending data to the file ../data/italy_2017.csv...\n",
      "INFO : 10/02/2021 11:09:02 PM : Done appending to the file ../data/italy_2017.csv!!!\n",
      "INFO : 10/02/2021 11:09:02 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:09:02 PM : Extracting top 200 weekly for 2017-03-17--2017-03-24 - it\n",
      "INFO : 10/02/2021 11:09:02 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:09:02 PM : Appending data to the file ../data/italy_2017.csv...\n",
      "INFO : 10/02/2021 11:09:02 PM : Done appending to the file ../data/italy_2017.csv!!!\n",
      "INFO : 10/02/2021 11:09:02 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:09:03 PM : Extracting top 200 weekly for 2017-03-24--2017-03-31 - it\n",
      "INFO : 10/02/2021 11:09:03 PM : Appending data to the file ../data/italy_2017.csv...\n",
      "INFO : 10/02/2021 11:09:03 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:09:03 PM : Done appending to the file ../data/italy_2017.csv!!!\n",
      "INFO : 10/02/2021 11:09:03 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:09:04 PM : Extracting top 200 weekly for 2017-03-31--2017-04-07 - it\n",
      "INFO : 10/02/2021 11:09:04 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:09:04 PM : Appending data to the file ../data/italy_2017.csv...\n",
      "INFO : 10/02/2021 11:09:04 PM : Done appending to the file ../data/italy_2017.csv!!!\n",
      "INFO : 10/02/2021 11:09:04 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:09:05 PM : Extracting top 200 weekly for 2017-04-07--2017-04-14 - it\n",
      "INFO : 10/02/2021 11:09:05 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:09:05 PM : Appending data to the file ../data/italy_2017.csv...\n",
      "INFO : 10/02/2021 11:09:05 PM : Done appending to the file ../data/italy_2017.csv!!!\n",
      "INFO : 10/02/2021 11:09:05 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:09:06 PM : Extracting top 200 weekly for 2017-04-14--2017-04-21 - it\n",
      "INFO : 10/02/2021 11:09:06 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:09:06 PM : Appending data to the file ../data/italy_2017.csv...\n",
      "INFO : 10/02/2021 11:09:06 PM : Done appending to the file ../data/italy_2017.csv!!!\n",
      "INFO : 10/02/2021 11:09:06 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:09:06 PM : Extracting top 200 weekly for 2017-04-21--2017-04-28 - it\n",
      "INFO : 10/02/2021 11:09:06 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:09:06 PM : Appending data to the file ../data/italy_2017.csv...\n",
      "INFO : 10/02/2021 11:09:07 PM : Done appending to the file ../data/italy_2017.csv!!!\n",
      "INFO : 10/02/2021 11:09:07 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:09:07 PM : Extracting top 200 weekly for 2017-04-28--2017-05-05 - it\n",
      "INFO : 10/02/2021 11:09:07 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:09:07 PM : Appending data to the file ../data/italy_2017.csv...\n",
      "INFO : 10/02/2021 11:09:07 PM : Done appending to the file ../data/italy_2017.csv!!!\n",
      "INFO : 10/02/2021 11:09:07 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:09:08 PM : Extracting top 200 weekly for 2017-05-05--2017-05-12 - it\n",
      "INFO : 10/02/2021 11:09:08 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:09:08 PM : Appending data to the file ../data/italy_2017.csv...\n",
      "INFO : 10/02/2021 11:09:08 PM : Done appending to the file ../data/italy_2017.csv!!!\n",
      "INFO : 10/02/2021 11:09:08 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:09:09 PM : Extracting top 200 weekly for 2017-05-12--2017-05-19 - it\n",
      "INFO : 10/02/2021 11:09:09 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:09:09 PM : Appending data to the file ../data/italy_2017.csv...\n",
      "INFO : 10/02/2021 11:09:09 PM : Done appending to the file ../data/italy_2017.csv!!!\n",
      "INFO : 10/02/2021 11:09:09 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:09:09 PM : Extracting top 200 weekly for 2017-05-19--2017-05-26 - it\n",
      "INFO : 10/02/2021 11:09:09 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:09:09 PM : Appending data to the file ../data/italy_2017.csv...\n",
      "INFO : 10/02/2021 11:09:10 PM : Done appending to the file ../data/italy_2017.csv!!!\n",
      "INFO : 10/02/2021 11:09:10 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:09:10 PM : Extracting top 200 weekly for 2017-05-26--2017-06-02 - it\n",
      "INFO : 10/02/2021 11:09:10 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:09:10 PM : Appending data to the file ../data/italy_2017.csv...\n",
      "INFO : 10/02/2021 11:09:10 PM : Done appending to the file ../data/italy_2017.csv!!!\n",
      "INFO : 10/02/2021 11:09:10 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:09:11 PM : Extracting top 200 weekly for 2017-06-02--2017-06-09 - it\n",
      "INFO : 10/02/2021 11:09:11 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:09:11 PM : Appending data to the file ../data/italy_2017.csv...\n",
      "INFO : 10/02/2021 11:09:11 PM : Done appending to the file ../data/italy_2017.csv!!!\n",
      "INFO : 10/02/2021 11:09:11 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:09:12 PM : Extracting top 200 weekly for 2017-06-09--2017-06-16 - it\n",
      "INFO : 10/02/2021 11:09:12 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:09:12 PM : Appending data to the file ../data/italy_2017.csv...\n",
      "INFO : 10/02/2021 11:09:12 PM : Done appending to the file ../data/italy_2017.csv!!!\n",
      "INFO : 10/02/2021 11:09:12 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:09:13 PM : Extracting top 200 weekly for 2017-06-16--2017-06-23 - it\n",
      "INFO : 10/02/2021 11:09:13 PM : Appending data to the file ../data/italy_2017.csv...\n",
      "INFO : 10/02/2021 11:09:13 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:09:13 PM : Done appending to the file ../data/italy_2017.csv!!!\n",
      "INFO : 10/02/2021 11:09:13 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:09:14 PM : Extracting top 200 weekly for 2017-06-23--2017-06-30 - it\n",
      "INFO : 10/02/2021 11:09:14 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:09:14 PM : Appending data to the file ../data/italy_2017.csv...\n",
      "INFO : 10/02/2021 11:09:14 PM : Done appending to the file ../data/italy_2017.csv!!!\n",
      "INFO : 10/02/2021 11:09:14 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:09:15 PM : Extracting top 200 weekly for 2017-06-30--2017-07-07 - it\n",
      "INFO : 10/02/2021 11:09:15 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:09:15 PM : Appending data to the file ../data/italy_2017.csv...\n",
      "INFO : 10/02/2021 11:09:15 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:09:15 PM : Done appending to the file ../data/italy_2017.csv!!!\n",
      "INFO : 10/02/2021 11:09:16 PM : Extracting top 200 weekly for 2017-07-07--2017-07-14 - it\n",
      "INFO : 10/02/2021 11:09:16 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:09:16 PM : Appending data to the file ../data/italy_2017.csv...\n",
      "INFO : 10/02/2021 11:09:16 PM : Done appending to the file ../data/italy_2017.csv!!!\n",
      "INFO : 10/02/2021 11:09:16 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:09:16 PM : Extracting top 200 weekly for 2017-07-14--2017-07-21 - it\n",
      "INFO : 10/02/2021 11:09:16 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:09:16 PM : Appending data to the file ../data/italy_2017.csv...\n",
      "INFO : 10/02/2021 11:09:16 PM : Done appending to the file ../data/italy_2017.csv!!!\n",
      "INFO : 10/02/2021 11:09:16 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:09:17 PM : Extracting top 200 weekly for 2017-07-21--2017-07-28 - it\n",
      "INFO : 10/02/2021 11:09:17 PM : Appending data to the file ../data/italy_2017.csv...\n",
      "INFO : 10/02/2021 11:09:17 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:09:17 PM : Done appending to the file ../data/italy_2017.csv!!!\n",
      "INFO : 10/02/2021 11:09:17 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:09:18 PM : Extracting top 200 weekly for 2017-07-28--2017-08-04 - it\n",
      "INFO : 10/02/2021 11:09:18 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:09:18 PM : Appending data to the file ../data/italy_2017.csv...\n",
      "INFO : 10/02/2021 11:09:18 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:09:18 PM : Done appending to the file ../data/italy_2017.csv!!!\n",
      "INFO : 10/02/2021 11:09:19 PM : Extracting top 200 weekly for 2017-08-04--2017-08-11 - it\n",
      "INFO : 10/02/2021 11:09:19 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:09:19 PM : Appending data to the file ../data/italy_2017.csv...\n",
      "INFO : 10/02/2021 11:09:19 PM : Done appending to the file ../data/italy_2017.csv!!!\n",
      "INFO : 10/02/2021 11:09:19 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:09:20 PM : Extracting top 200 weekly for 2017-08-11--2017-08-18 - it\n",
      "INFO : 10/02/2021 11:09:20 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:09:20 PM : Appending data to the file ../data/italy_2017.csv...\n",
      "INFO : 10/02/2021 11:09:20 PM : Done appending to the file ../data/italy_2017.csv!!!\n",
      "INFO : 10/02/2021 11:09:20 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:09:21 PM : Extracting top 200 weekly for 2017-08-18--2017-08-25 - it\n",
      "INFO : 10/02/2021 11:09:21 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:09:21 PM : Appending data to the file ../data/italy_2017.csv...\n",
      "INFO : 10/02/2021 11:09:21 PM : Done appending to the file ../data/italy_2017.csv!!!\n",
      "INFO : 10/02/2021 11:09:21 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:09:22 PM : Extracting top 200 weekly for 2017-08-25--2017-09-01 - it\n",
      "INFO : 10/02/2021 11:09:22 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:09:22 PM : Appending data to the file ../data/italy_2017.csv...\n",
      "INFO : 10/02/2021 11:09:22 PM : Done appending to the file ../data/italy_2017.csv!!!\n",
      "INFO : 10/02/2021 11:09:22 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:09:22 PM : Extracting top 200 weekly for 2017-09-01--2017-09-08 - it\n",
      "INFO : 10/02/2021 11:09:22 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:09:22 PM : Appending data to the file ../data/italy_2017.csv...\n",
      "INFO : 10/02/2021 11:09:22 PM : Done appending to the file ../data/italy_2017.csv!!!\n",
      "INFO : 10/02/2021 11:09:22 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:09:23 PM : Extracting top 200 weekly for 2017-09-08--2017-09-15 - it\n",
      "INFO : 10/02/2021 11:09:23 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:09:23 PM : Appending data to the file ../data/italy_2017.csv...\n",
      "INFO : 10/02/2021 11:09:23 PM : Done appending to the file ../data/italy_2017.csv!!!\n",
      "INFO : 10/02/2021 11:09:23 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:09:24 PM : Extracting top 200 weekly for 2017-09-15--2017-09-22 - it\n",
      "INFO : 10/02/2021 11:09:24 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:09:24 PM : Appending data to the file ../data/italy_2017.csv...\n",
      "INFO : 10/02/2021 11:09:24 PM : Done appending to the file ../data/italy_2017.csv!!!\n",
      "INFO : 10/02/2021 11:09:24 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:09:25 PM : Extracting top 200 weekly for 2017-09-22--2017-09-29 - it\n",
      "INFO : 10/02/2021 11:09:25 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:09:25 PM : Appending data to the file ../data/italy_2017.csv...\n",
      "INFO : 10/02/2021 11:09:25 PM : Done appending to the file ../data/italy_2017.csv!!!\n",
      "INFO : 10/02/2021 11:09:25 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:09:26 PM : Extracting top 200 weekly for 2017-09-29--2017-10-06 - it\n",
      "INFO : 10/02/2021 11:09:26 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:09:26 PM : Appending data to the file ../data/italy_2017.csv...\n",
      "INFO : 10/02/2021 11:09:26 PM : Done appending to the file ../data/italy_2017.csv!!!\n",
      "INFO : 10/02/2021 11:09:26 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:09:26 PM : Extracting top 200 weekly for 2017-10-06--2017-10-13 - it\n",
      "INFO : 10/02/2021 11:09:26 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:09:26 PM : Appending data to the file ../data/italy_2017.csv...\n",
      "INFO : 10/02/2021 11:09:26 PM : Done appending to the file ../data/italy_2017.csv!!!\n",
      "INFO : 10/02/2021 11:09:26 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:09:27 PM : Extracting top 200 weekly for 2017-10-13--2017-10-20 - it\n",
      "INFO : 10/02/2021 11:09:27 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:09:27 PM : Appending data to the file ../data/italy_2017.csv...\n",
      "INFO : 10/02/2021 11:09:27 PM : Done appending to the file ../data/italy_2017.csv!!!\n",
      "INFO : 10/02/2021 11:09:27 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:09:28 PM : Extracting top 200 weekly for 2017-10-20--2017-10-27 - it\n",
      "INFO : 10/02/2021 11:09:28 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:09:28 PM : Appending data to the file ../data/italy_2017.csv...\n",
      "INFO : 10/02/2021 11:09:28 PM : Done appending to the file ../data/italy_2017.csv!!!\n",
      "INFO : 10/02/2021 11:09:28 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:09:29 PM : Extracting top 200 weekly for 2017-10-27--2017-11-03 - it\n",
      "INFO : 10/02/2021 11:09:29 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:09:29 PM : Appending data to the file ../data/italy_2017.csv...\n",
      "INFO : 10/02/2021 11:09:29 PM : Done appending to the file ../data/italy_2017.csv!!!\n",
      "INFO : 10/02/2021 11:09:29 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:09:29 PM : Extracting top 200 weekly for 2017-11-03--2017-11-10 - it\n",
      "INFO : 10/02/2021 11:09:29 PM : Appending data to the file ../data/italy_2017.csv...\n",
      "INFO : 10/02/2021 11:09:29 PM : Done appending to the file ../data/italy_2017.csv!!!\n",
      "INFO : 10/02/2021 11:09:29 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:09:30 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:09:30 PM : Extracting top 200 weekly for 2017-11-10--2017-11-17 - it\n",
      "INFO : 10/02/2021 11:09:30 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:09:30 PM : Appending data to the file ../data/italy_2017.csv...\n",
      "INFO : 10/02/2021 11:09:30 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:09:30 PM : Done appending to the file ../data/italy_2017.csv!!!\n",
      "INFO : 10/02/2021 11:09:31 PM : Extracting top 200 weekly for 2017-11-17--2017-11-24 - it\n",
      "INFO : 10/02/2021 11:09:31 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:09:31 PM : Appending data to the file ../data/italy_2017.csv...\n",
      "INFO : 10/02/2021 11:09:31 PM : Done appending to the file ../data/italy_2017.csv!!!\n",
      "INFO : 10/02/2021 11:09:31 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:09:32 PM : Extracting top 200 weekly for 2017-11-24--2017-12-01 - it\n",
      "INFO : 10/02/2021 11:09:32 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:09:32 PM : Appending data to the file ../data/italy_2017.csv...\n",
      "INFO : 10/02/2021 11:09:32 PM : Done appending to the file ../data/italy_2017.csv!!!\n",
      "INFO : 10/02/2021 11:09:32 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:09:33 PM : Extracting top 200 weekly for 2017-12-01--2017-12-08 - it\n",
      "INFO : 10/02/2021 11:09:33 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:09:33 PM : Appending data to the file ../data/italy_2017.csv...\n",
      "INFO : 10/02/2021 11:09:33 PM : Done appending to the file ../data/italy_2017.csv!!!\n",
      "INFO : 10/02/2021 11:09:33 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:09:34 PM : Extracting top 200 weekly for 2017-12-08--2017-12-15 - it\n",
      "INFO : 10/02/2021 11:09:34 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:09:34 PM : Appending data to the file ../data/italy_2017.csv...\n",
      "INFO : 10/02/2021 11:09:34 PM : Done appending to the file ../data/italy_2017.csv!!!\n",
      "INFO : 10/02/2021 11:09:34 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:09:35 PM : Extracting top 200 weekly for 2017-12-15--2017-12-22 - it\n",
      "INFO : 10/02/2021 11:09:35 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:09:35 PM : Appending data to the file ../data/italy_2017.csv...\n",
      "INFO : 10/02/2021 11:09:35 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:09:35 PM : Done appending to the file ../data/italy_2017.csv!!!\n",
      "INFO : 10/02/2021 11:09:36 PM : Extracting top 200 weekly for 2017-12-22--2017-12-29 - it\n",
      "INFO : 10/02/2021 11:09:36 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:09:36 PM : Appending data to the file ../data/italy_2017.csv...\n",
      "INFO : 10/02/2021 11:09:36 PM : Done appending to the file ../data/italy_2017.csv!!!\n",
      "INFO : 10/02/2021 11:09:36 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:09:37 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:09:37 PM : Appending data to the file ../data/italy_2017.csv...\n",
      "INFO : 10/02/2021 11:09:37 PM : Done appending to the file ../data/italy_2017.csv!!!\n",
      "INFO : 10/02/2021 11:09:37 PM : Done appending to the table top_200_weekly!!!\n"
     ]
    }
   ],
   "source": [
    "api = SpotifyCharts()\n",
    "connector = sqlalchemy.create_engine(\"sqlite:///../data/italy_2017.db\", echo=False)\n",
    "api.top200Weekly(output_file = \"../data/italy_2017.csv\", output_db = connector, webhook = [\"https://mywebhookssite.com/post/\"], start = \"2017-01-01\", end = \"2017-12-31\", region = \"it\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Position</th>\n",
       "      <th>Track Name</th>\n",
       "      <th>Artist</th>\n",
       "      <th>Streams</th>\n",
       "      <th>date</th>\n",
       "      <th>region</th>\n",
       "      <th>spotify_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Shape of You</td>\n",
       "      <td>Ed Sheeran</td>\n",
       "      <td>1051142</td>\n",
       "      <td>2017-01-06--2017-01-13</td>\n",
       "      <td>it</td>\n",
       "      <td>7qiZfU4dY1lWllzX7mPBI3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Rockabye (feat. Sean Paul &amp; Anne-Marie)</td>\n",
       "      <td>Clean Bandit</td>\n",
       "      <td>759626</td>\n",
       "      <td>2017-01-06--2017-01-13</td>\n",
       "      <td>it</td>\n",
       "      <td>5knuzwU65gJK7IF5yJsuaW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Castle on the Hill</td>\n",
       "      <td>Ed Sheeran</td>\n",
       "      <td>715171</td>\n",
       "      <td>2017-01-06--2017-01-13</td>\n",
       "      <td>it</td>\n",
       "      <td>6PCUP3dWmTjcTtXY02oFdT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Closer</td>\n",
       "      <td>The Chainsmokers</td>\n",
       "      <td>504232</td>\n",
       "      <td>2017-01-06--2017-01-13</td>\n",
       "      <td>it</td>\n",
       "      <td>7BKLCZ1jbUBVqRi2FVlTVw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Assenzio (feat. Stash &amp; Levante)</td>\n",
       "      <td>J-AX</td>\n",
       "      <td>478975</td>\n",
       "      <td>2017-01-06--2017-01-13</td>\n",
       "      <td>it</td>\n",
       "      <td>0DRKnh0BloxJHyhXkfbiX8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Position                               Track Name            Artist  \\\n",
       "0         1                             Shape of You        Ed Sheeran   \n",
       "1         2  Rockabye (feat. Sean Paul & Anne-Marie)      Clean Bandit   \n",
       "2         3                       Castle on the Hill        Ed Sheeran   \n",
       "3         4                                   Closer  The Chainsmokers   \n",
       "4         5         Assenzio (feat. Stash & Levante)              J-AX   \n",
       "\n",
       "   Streams                    date region              spotify_id  \n",
       "0  1051142  2017-01-06--2017-01-13     it  7qiZfU4dY1lWllzX7mPBI3  \n",
       "1   759626  2017-01-06--2017-01-13     it  5knuzwU65gJK7IF5yJsuaW  \n",
       "2   715171  2017-01-06--2017-01-13     it  6PCUP3dWmTjcTtXY02oFdT  \n",
       "3   504232  2017-01-06--2017-01-13     it  7BKLCZ1jbUBVqRi2FVlTVw  \n",
       "4   478975  2017-01-06--2017-01-13     it  0DRKnh0BloxJHyhXkfbiX8  "
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reading in csv from output file  \n",
    "italy_2017 = pd.read_csv('../data/italy_2017.csv')\n",
    "italy_2017.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Position</th>\n",
       "      <th>Track Name</th>\n",
       "      <th>Artist</th>\n",
       "      <th>Streams</th>\n",
       "      <th>date</th>\n",
       "      <th>region</th>\n",
       "      <th>spotify_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10195</th>\n",
       "      <td>196</td>\n",
       "      <td>Paris</td>\n",
       "      <td>The Chainsmokers</td>\n",
       "      <td>130719</td>\n",
       "      <td>2017-12-22--2017-12-29</td>\n",
       "      <td>it</td>\n",
       "      <td>72jbDTw1piOOj770jWNeaG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10196</th>\n",
       "      <td>197</td>\n",
       "      <td>Jocelyn Flores</td>\n",
       "      <td>XXXTENTACION</td>\n",
       "      <td>129145</td>\n",
       "      <td>2017-12-22--2017-12-29</td>\n",
       "      <td>it</td>\n",
       "      <td>7m9OqQk4RVRkw9JJdeAw96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10197</th>\n",
       "      <td>198</td>\n",
       "      <td>Walk On Water (feat. Beyoncé)</td>\n",
       "      <td>Eminem</td>\n",
       "      <td>128730</td>\n",
       "      <td>2017-12-22--2017-12-29</td>\n",
       "      <td>it</td>\n",
       "      <td>5PIBgH1cUFwu5IiUk9tnN9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10198</th>\n",
       "      <td>199</td>\n",
       "      <td>You Don't Know Me - Radio Edit</td>\n",
       "      <td>Jax Jones</td>\n",
       "      <td>128660</td>\n",
       "      <td>2017-12-22--2017-12-29</td>\n",
       "      <td>it</td>\n",
       "      <td>00lNx0OcTJrS3MKHcB80HY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10199</th>\n",
       "      <td>200</td>\n",
       "      <td>Ric Flair Drip (&amp; Metro Boomin)</td>\n",
       "      <td>Offset</td>\n",
       "      <td>128402</td>\n",
       "      <td>2017-12-22--2017-12-29</td>\n",
       "      <td>it</td>\n",
       "      <td>7sO5G9EABYOXQKNPNiE9NR</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Position                       Track Name            Artist  Streams  \\\n",
       "10195       196                            Paris  The Chainsmokers   130719   \n",
       "10196       197                   Jocelyn Flores      XXXTENTACION   129145   \n",
       "10197       198    Walk On Water (feat. Beyoncé)            Eminem   128730   \n",
       "10198       199   You Don't Know Me - Radio Edit         Jax Jones   128660   \n",
       "10199       200  Ric Flair Drip (& Metro Boomin)            Offset   128402   \n",
       "\n",
       "                         date region              spotify_id  \n",
       "10195  2017-12-22--2017-12-29     it  72jbDTw1piOOj770jWNeaG  \n",
       "10196  2017-12-22--2017-12-29     it  7m9OqQk4RVRkw9JJdeAw96  \n",
       "10197  2017-12-22--2017-12-29     it  5PIBgH1cUFwu5IiUk9tnN9  \n",
       "10198  2017-12-22--2017-12-29     it  00lNx0OcTJrS3MKHcB80HY  \n",
       "10199  2017-12-22--2017-12-29     it  7sO5G9EABYOXQKNPNiE9NR  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "italy_2017.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10200 entries, 0 to 10199\n",
      "Data columns (total 7 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   Position    10200 non-null  int64 \n",
      " 1   Track Name  10195 non-null  object\n",
      " 2   Artist      10195 non-null  object\n",
      " 3   Streams     10200 non-null  int64 \n",
      " 4   date        10200 non-null  object\n",
      " 5   region      10200 non-null  object\n",
      " 6   spotify_id  10200 non-null  object\n",
      "dtypes: int64(2), object(5)\n",
      "memory usage: 557.9+ KB\n"
     ]
    }
   ],
   "source": [
    "italy_2017.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Position</th>\n",
       "      <th>Track Name</th>\n",
       "      <th>Artist</th>\n",
       "      <th>Streams</th>\n",
       "      <th>date</th>\n",
       "      <th>region</th>\n",
       "      <th>spotify_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5407</th>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>710869</td>\n",
       "      <td>2017-07-14--2017-07-21</td>\n",
       "      <td>it</td>\n",
       "      <td>3RXkboS74UYzN14xTqzPyY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5448</th>\n",
       "      <td>49</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>347986</td>\n",
       "      <td>2017-07-14--2017-07-21</td>\n",
       "      <td>it</td>\n",
       "      <td>3bVbQvGVIe4n24AzyXovXh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5455</th>\n",
       "      <td>56</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>276748</td>\n",
       "      <td>2017-07-14--2017-07-21</td>\n",
       "      <td>it</td>\n",
       "      <td>4JAyIDXOqNM6qHuZML01uX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5460</th>\n",
       "      <td>61</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>265932</td>\n",
       "      <td>2017-07-14--2017-07-21</td>\n",
       "      <td>it</td>\n",
       "      <td>3eFJqPe8VUYrABbFjSauuj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8698</th>\n",
       "      <td>99</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>164633</td>\n",
       "      <td>2017-11-03--2017-11-10</td>\n",
       "      <td>it</td>\n",
       "      <td>1YqcGlCHNquxBhlUZsjhMT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Position Track Name Artist  Streams                    date region  \\\n",
       "5407         8        NaN    NaN   710869  2017-07-14--2017-07-21     it   \n",
       "5448        49        NaN    NaN   347986  2017-07-14--2017-07-21     it   \n",
       "5455        56        NaN    NaN   276748  2017-07-14--2017-07-21     it   \n",
       "5460        61        NaN    NaN   265932  2017-07-14--2017-07-21     it   \n",
       "8698        99        NaN    NaN   164633  2017-11-03--2017-11-10     it   \n",
       "\n",
       "                  spotify_id  \n",
       "5407  3RXkboS74UYzN14xTqzPyY  \n",
       "5448  3bVbQvGVIe4n24AzyXovXh  \n",
       "5455  4JAyIDXOqNM6qHuZML01uX  \n",
       "5460  3eFJqPe8VUYrABbFjSauuj  \n",
       "8698  1YqcGlCHNquxBhlUZsjhMT  "
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "italy_2017[italy_2017['Artist'].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Italy Top 200 Weekly, 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : 10/02/2021 11:14:04 PM : The start date 2018-01-01 provided for top200Weekly is invalid. Wanna give one these a try? ['2018-01-05', '2018-01-12', '2018-01-19', '2018-01-26', '2018-02-02']\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter (1) to use the first suggestion, or (2) to quit and set yourself:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : 10/02/2021 11:14:15 PM : Extracting top 200 weekly for 2018-01-05--2018-01-12 - it\n",
      "INFO : 10/02/2021 11:14:16 PM : Extracting top 200 weekly for 2018-01-12--2018-01-19 - it\n",
      "INFO : 10/02/2021 11:14:16 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:14:16 PM : POSTing data to the endpoint https://mywebhookssite.com/post/\n",
      "INFO : 10/02/2021 11:14:16 PM : Appending data to the file ../data/italy_2018.csv...\n",
      "INFO : 10/02/2021 11:14:16 PM : Done appending to the file ../data/italy_2018.csv!!!\n",
      "Exception in thread Thread-9:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/urllib3/connection.py\", line 159, in _new_conn\n",
      "INFO : 10/02/2021 11:14:16 PM : Done appending to the table top_200_weekly!!!\n",
      "    conn = connection.create_connection(\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/urllib3/util/connection.py\", line 61, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/socket.py\", line 918, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 8] nodename nor servname provided, or not known\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 670, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 381, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 978, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/urllib3/connection.py\", line 309, in connect\n",
      "    conn = self._new_conn()\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/urllib3/connection.py\", line 171, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x7f98a5d4d9a0>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/requests/adapters.py\", line 439, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 726, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/urllib3/util/retry.py\", line 446, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='mywebhookssite.com', port=443): Max retries exceeded with url: /post/ (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f98a5d4d9a0>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/fycharts/SpotifyCharts.py\", line 88, in __post_to_endpoint_from_queue\n",
      "    postToRestEndpoint(df, url, what_data)\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/fycharts/write_to_outputs.py\", line 61, in postToRestEndpoint\n",
      "    raise(e)\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/fycharts/write_to_outputs.py\", line 58, in postToRestEndpoint\n",
      "    requests.post(url, json = dump)\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/requests/api.py\", line 119, in post\n",
      "    return request('post', url, data=data, json=json, **kwargs)\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/requests/api.py\", line 61, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/requests/sessions.py\", line 530, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/requests/sessions.py\", line 643, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/requests/adapters.py\", line 516, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='mywebhookssite.com', port=443): Max retries exceeded with url: /post/ (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f98a5d4d9a0>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/threading.py\", line 932, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/fycharts/SpotifyCharts.py\", line 90, in __post_to_endpoint_from_queue\n",
      "    raise RuntimeError(e)\n",
      "RuntimeError: HTTPSConnectionPool(host='mywebhookssite.com', port=443): Max retries exceeded with url: /post/ (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f98a5d4d9a0>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\n",
      "INFO : 10/02/2021 11:14:16 PM : Extracting top 200 weekly for 2018-01-19--2018-01-26 - it\n",
      "INFO : 10/02/2021 11:14:16 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:14:16 PM : Appending data to the file ../data/italy_2018.csv...\n",
      "INFO : 10/02/2021 11:14:16 PM : Done appending to the file ../data/italy_2018.csv!!!\n",
      "INFO : 10/02/2021 11:14:16 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:14:17 PM : Extracting top 200 weekly for 2018-01-26--2018-02-02 - it\n",
      "INFO : 10/02/2021 11:14:17 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:14:17 PM : Appending data to the file ../data/italy_2018.csv...\n",
      "INFO : 10/02/2021 11:14:17 PM : Done appending to the file ../data/italy_2018.csv!!!\n",
      "INFO : 10/02/2021 11:14:17 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:14:18 PM : Extracting top 200 weekly for 2018-02-02--2018-02-09 - it\n",
      "INFO : 10/02/2021 11:14:18 PM : Appending data to the file ../data/italy_2018.csv...\n",
      "INFO : 10/02/2021 11:14:18 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:14:18 PM : Done appending to the file ../data/italy_2018.csv!!!\n",
      "INFO : 10/02/2021 11:14:18 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:14:19 PM : Extracting top 200 weekly for 2018-02-09--2018-02-16 - it\n",
      "INFO : 10/02/2021 11:14:19 PM : Appending data to the file ../data/italy_2018.csv...\n",
      "INFO : 10/02/2021 11:14:19 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:14:19 PM : Done appending to the file ../data/italy_2018.csv!!!\n",
      "INFO : 10/02/2021 11:14:19 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:14:20 PM : Extracting top 200 weekly for 2018-02-16--2018-02-23 - it\n",
      "INFO : 10/02/2021 11:14:20 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:14:20 PM : Appending data to the file ../data/italy_2018.csv...\n",
      "INFO : 10/02/2021 11:14:20 PM : Done appending to the file ../data/italy_2018.csv!!!\n",
      "INFO : 10/02/2021 11:14:20 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:14:21 PM : Extracting top 200 weekly for 2018-02-23--2018-03-02 - it\n",
      "INFO : 10/02/2021 11:14:21 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:14:21 PM : Appending data to the file ../data/italy_2018.csv...\n",
      "INFO : 10/02/2021 11:14:21 PM : Done appending to the file ../data/italy_2018.csv!!!\n",
      "INFO : 10/02/2021 11:14:21 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:14:22 PM : Extracting top 200 weekly for 2018-03-02--2018-03-09 - it\n",
      "INFO : 10/02/2021 11:14:22 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:14:22 PM : Appending data to the file ../data/italy_2018.csv...\n",
      "INFO : 10/02/2021 11:14:22 PM : Done appending to the file ../data/italy_2018.csv!!!\n",
      "INFO : 10/02/2021 11:14:22 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:14:23 PM : Extracting top 200 weekly for 2018-03-09--2018-03-16 - it\n",
      "INFO : 10/02/2021 11:14:23 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:14:23 PM : Appending data to the file ../data/italy_2018.csv...\n",
      "INFO : 10/02/2021 11:14:23 PM : Done appending to the file ../data/italy_2018.csv!!!\n",
      "INFO : 10/02/2021 11:14:23 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:14:24 PM : Extracting top 200 weekly for 2018-03-16--2018-03-23 - it\n",
      "INFO : 10/02/2021 11:14:24 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:14:24 PM : Appending data to the file ../data/italy_2018.csv...\n",
      "INFO : 10/02/2021 11:14:24 PM : Done appending to the file ../data/italy_2018.csv!!!\n",
      "INFO : 10/02/2021 11:14:24 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:14:24 PM : Extracting top 200 weekly for 2018-03-23--2018-03-30 - it\n",
      "INFO : 10/02/2021 11:14:24 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:14:24 PM : Appending data to the file ../data/italy_2018.csv...\n",
      "INFO : 10/02/2021 11:14:24 PM : Done appending to the file ../data/italy_2018.csv!!!\n",
      "INFO : 10/02/2021 11:14:24 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:14:25 PM : Extracting top 200 weekly for 2018-03-30--2018-04-06 - it\n",
      "INFO : 10/02/2021 11:14:25 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:14:25 PM : Appending data to the file ../data/italy_2018.csv...\n",
      "INFO : 10/02/2021 11:14:25 PM : Done appending to the file ../data/italy_2018.csv!!!\n",
      "INFO : 10/02/2021 11:14:25 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:14:26 PM : Extracting top 200 weekly for 2018-04-06--2018-04-13 - it\n",
      "INFO : 10/02/2021 11:14:26 PM : Appending data to the file ../data/italy_2018.csv...\n",
      "INFO : 10/02/2021 11:14:26 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:14:26 PM : Done appending to the file ../data/italy_2018.csv!!!\n",
      "INFO : 10/02/2021 11:14:26 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:14:27 PM : Extracting top 200 weekly for 2018-04-13--2018-04-20 - it\n",
      "INFO : 10/02/2021 11:14:27 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:14:27 PM : Appending data to the file ../data/italy_2018.csv...\n",
      "INFO : 10/02/2021 11:14:27 PM : Done appending to the file ../data/italy_2018.csv!!!\n",
      "INFO : 10/02/2021 11:14:27 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:14:28 PM : Extracting top 200 weekly for 2018-04-20--2018-04-27 - it\n",
      "INFO : 10/02/2021 11:14:28 PM : Appending data to the file ../data/italy_2018.csv...\n",
      "INFO : 10/02/2021 11:14:28 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:14:28 PM : Done appending to the file ../data/italy_2018.csv!!!\n",
      "INFO : 10/02/2021 11:14:28 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:14:29 PM : Extracting top 200 weekly for 2018-04-27--2018-05-04 - it\n",
      "INFO : 10/02/2021 11:14:29 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:14:29 PM : Appending data to the file ../data/italy_2018.csv...\n",
      "INFO : 10/02/2021 11:14:29 PM : Done appending to the file ../data/italy_2018.csv!!!\n",
      "INFO : 10/02/2021 11:14:29 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:14:30 PM : Extracting top 200 weekly for 2018-05-04--2018-05-11 - it\n",
      "INFO : 10/02/2021 11:14:30 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:14:30 PM : Appending data to the file ../data/italy_2018.csv...\n",
      "INFO : 10/02/2021 11:14:30 PM : Done appending to the file ../data/italy_2018.csv!!!\n",
      "INFO : 10/02/2021 11:14:30 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:14:30 PM : Extracting top 200 weekly for 2018-05-11--2018-05-18 - it\n",
      "INFO : 10/02/2021 11:14:30 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:14:30 PM : Appending data to the file ../data/italy_2018.csv...\n",
      "INFO : 10/02/2021 11:14:30 PM : Done appending to the file ../data/italy_2018.csv!!!\n",
      "INFO : 10/02/2021 11:14:30 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:14:32 PM : Extracting top 200 weekly for 2018-05-18--2018-05-25 - it\n",
      "INFO : 10/02/2021 11:14:32 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:14:32 PM : Appending data to the file ../data/italy_2018.csv...\n",
      "INFO : 10/02/2021 11:14:32 PM : Done appending to the file ../data/italy_2018.csv!!!\n",
      "INFO : 10/02/2021 11:14:32 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:14:33 PM : Extracting top 200 weekly for 2018-05-25--2018-06-01 - it\n",
      "INFO : 10/02/2021 11:14:33 PM : Appending data to the file ../data/italy_2018.csv...\n",
      "INFO : 10/02/2021 11:14:33 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:14:33 PM : Done appending to the file ../data/italy_2018.csv!!!\n",
      "INFO : 10/02/2021 11:14:33 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:14:34 PM : Extracting top 200 weekly for 2018-06-01--2018-06-08 - it\n",
      "INFO : 10/02/2021 11:14:34 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:14:34 PM : Appending data to the file ../data/italy_2018.csv...\n",
      "INFO : 10/02/2021 11:14:34 PM : Done appending to the file ../data/italy_2018.csv!!!\n",
      "INFO : 10/02/2021 11:14:34 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:14:35 PM : Extracting top 200 weekly for 2018-06-08--2018-06-15 - it\n",
      "INFO : 10/02/2021 11:14:35 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:14:35 PM : Appending data to the file ../data/italy_2018.csv...\n",
      "INFO : 10/02/2021 11:14:35 PM : Done appending to the file ../data/italy_2018.csv!!!\n",
      "INFO : 10/02/2021 11:14:35 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:14:38 PM : Extracting top 200 weekly for 2018-06-15--2018-06-22 - it\n",
      "INFO : 10/02/2021 11:14:38 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:14:38 PM : Appending data to the file ../data/italy_2018.csv...\n",
      "INFO : 10/02/2021 11:14:38 PM : Done appending to the file ../data/italy_2018.csv!!!\n",
      "INFO : 10/02/2021 11:14:38 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:14:38 PM : Extracting top 200 weekly for 2018-06-22--2018-06-29 - it\n",
      "INFO : 10/02/2021 11:14:38 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:14:38 PM : Appending data to the file ../data/italy_2018.csv...\n",
      "INFO : 10/02/2021 11:14:38 PM : Done appending to the file ../data/italy_2018.csv!!!\n",
      "INFO : 10/02/2021 11:14:38 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:14:39 PM : Extracting top 200 weekly for 2018-06-29--2018-07-06 - it\n",
      "INFO : 10/02/2021 11:14:39 PM : Appending data to the file ../data/italy_2018.csv...\n",
      "INFO : 10/02/2021 11:14:39 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:14:39 PM : Done appending to the file ../data/italy_2018.csv!!!\n",
      "INFO : 10/02/2021 11:14:39 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:14:40 PM : Extracting top 200 weekly for 2018-07-06--2018-07-13 - it\n",
      "INFO : 10/02/2021 11:14:40 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:14:40 PM : Appending data to the file ../data/italy_2018.csv...\n",
      "INFO : 10/02/2021 11:14:40 PM : Done appending to the file ../data/italy_2018.csv!!!\n",
      "INFO : 10/02/2021 11:14:40 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:14:41 PM : Extracting top 200 weekly for 2018-07-13--2018-07-20 - it\n",
      "INFO : 10/02/2021 11:14:41 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:14:41 PM : Appending data to the file ../data/italy_2018.csv...\n",
      "INFO : 10/02/2021 11:14:41 PM : Done appending to the file ../data/italy_2018.csv!!!\n",
      "INFO : 10/02/2021 11:14:41 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:14:42 PM : Extracting top 200 weekly for 2018-07-20--2018-07-27 - it\n",
      "INFO : 10/02/2021 11:14:42 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:14:42 PM : Appending data to the file ../data/italy_2018.csv...\n",
      "INFO : 10/02/2021 11:14:42 PM : Done appending to the file ../data/italy_2018.csv!!!\n",
      "INFO : 10/02/2021 11:14:42 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:14:43 PM : Extracting top 200 weekly for 2018-07-27--2018-08-03 - it\n",
      "INFO : 10/02/2021 11:14:43 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:14:43 PM : Appending data to the file ../data/italy_2018.csv...\n",
      "INFO : 10/02/2021 11:14:43 PM : Done appending to the file ../data/italy_2018.csv!!!\n",
      "INFO : 10/02/2021 11:14:43 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:14:45 PM : Extracting top 200 weekly for 2018-08-03--2018-08-10 - it\n",
      "INFO : 10/02/2021 11:14:45 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:14:45 PM : Appending data to the file ../data/italy_2018.csv...\n",
      "INFO : 10/02/2021 11:14:45 PM : Done appending to the file ../data/italy_2018.csv!!!\n",
      "INFO : 10/02/2021 11:14:45 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:14:46 PM : Extracting top 200 weekly for 2018-08-10--2018-08-17 - it\n",
      "INFO : 10/02/2021 11:14:46 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:14:46 PM : Appending data to the file ../data/italy_2018.csv...\n",
      "INFO : 10/02/2021 11:14:46 PM : Done appending to the file ../data/italy_2018.csv!!!\n",
      "INFO : 10/02/2021 11:14:46 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:14:47 PM : Extracting top 200 weekly for 2018-08-17--2018-08-24 - it\n",
      "INFO : 10/02/2021 11:14:47 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:14:47 PM : Appending data to the file ../data/italy_2018.csv...\n",
      "INFO : 10/02/2021 11:14:47 PM : Done appending to the file ../data/italy_2018.csv!!!\n",
      "INFO : 10/02/2021 11:14:47 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:14:48 PM : Extracting top 200 weekly for 2018-08-24--2018-08-31 - it\n",
      "INFO : 10/02/2021 11:14:48 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:14:48 PM : Appending data to the file ../data/italy_2018.csv...\n",
      "INFO : 10/02/2021 11:14:48 PM : Done appending to the file ../data/italy_2018.csv!!!\n",
      "INFO : 10/02/2021 11:14:48 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:14:49 PM : Extracting top 200 weekly for 2018-08-31--2018-09-07 - it\n",
      "INFO : 10/02/2021 11:14:49 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:14:49 PM : Appending data to the file ../data/italy_2018.csv...\n",
      "INFO : 10/02/2021 11:14:49 PM : Done appending to the file ../data/italy_2018.csv!!!\n",
      "INFO : 10/02/2021 11:14:49 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:14:50 PM : Extracting top 200 weekly for 2018-09-07--2018-09-14 - it\n",
      "INFO : 10/02/2021 11:14:50 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:14:50 PM : Appending data to the file ../data/italy_2018.csv...\n",
      "INFO : 10/02/2021 11:14:50 PM : Done appending to the file ../data/italy_2018.csv!!!\n",
      "INFO : 10/02/2021 11:14:50 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:14:51 PM : Extracting top 200 weekly for 2018-09-14--2018-09-21 - it\n",
      "INFO : 10/02/2021 11:14:51 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:14:51 PM : Appending data to the file ../data/italy_2018.csv...\n",
      "INFO : 10/02/2021 11:14:51 PM : Done appending to the file ../data/italy_2018.csv!!!\n",
      "INFO : 10/02/2021 11:14:51 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:14:52 PM : Extracting top 200 weekly for 2018-09-21--2018-09-28 - it\n",
      "INFO : 10/02/2021 11:14:52 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:14:52 PM : Appending data to the file ../data/italy_2018.csv...\n",
      "INFO : 10/02/2021 11:14:52 PM : Done appending to the file ../data/italy_2018.csv!!!\n",
      "INFO : 10/02/2021 11:14:52 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:14:53 PM : Extracting top 200 weekly for 2018-09-28--2018-10-05 - it\n",
      "INFO : 10/02/2021 11:14:53 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:14:53 PM : Appending data to the file ../data/italy_2018.csv...\n",
      "INFO : 10/02/2021 11:14:53 PM : Done appending to the file ../data/italy_2018.csv!!!\n",
      "INFO : 10/02/2021 11:14:53 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:14:53 PM : Extracting top 200 weekly for 2018-10-05--2018-10-12 - it\n",
      "INFO : 10/02/2021 11:14:53 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:14:53 PM : Appending data to the file ../data/italy_2018.csv...\n",
      "INFO : 10/02/2021 11:14:53 PM : Done appending to the file ../data/italy_2018.csv!!!\n",
      "INFO : 10/02/2021 11:14:53 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:14:54 PM : Extracting top 200 weekly for 2018-10-12--2018-10-19 - it\n",
      "INFO : 10/02/2021 11:14:54 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:14:54 PM : Appending data to the file ../data/italy_2018.csv...\n",
      "INFO : 10/02/2021 11:14:54 PM : Done appending to the file ../data/italy_2018.csv!!!\n",
      "INFO : 10/02/2021 11:14:54 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:14:56 PM : Extracting top 200 weekly for 2018-10-19--2018-10-26 - it\n",
      "INFO : 10/02/2021 11:14:56 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:14:56 PM : Appending data to the file ../data/italy_2018.csv...\n",
      "INFO : 10/02/2021 11:14:56 PM : Done appending to the file ../data/italy_2018.csv!!!\n",
      "INFO : 10/02/2021 11:14:56 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:14:57 PM : Extracting top 200 weekly for 2018-10-26--2018-11-02 - it\n",
      "INFO : 10/02/2021 11:14:57 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:14:57 PM : Appending data to the file ../data/italy_2018.csv...\n",
      "INFO : 10/02/2021 11:14:57 PM : Done appending to the file ../data/italy_2018.csv!!!\n",
      "INFO : 10/02/2021 11:14:57 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:14:57 PM : Extracting top 200 weekly for 2018-11-02--2018-11-09 - it\n",
      "INFO : 10/02/2021 11:14:57 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:14:57 PM : Appending data to the file ../data/italy_2018.csv...\n",
      "INFO : 10/02/2021 11:14:57 PM : Done appending to the file ../data/italy_2018.csv!!!\n",
      "INFO : 10/02/2021 11:14:57 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:14:58 PM : Extracting top 200 weekly for 2018-11-09--2018-11-16 - it\n",
      "INFO : 10/02/2021 11:14:58 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:14:58 PM : Appending data to the file ../data/italy_2018.csv...\n",
      "INFO : 10/02/2021 11:14:58 PM : Done appending to the file ../data/italy_2018.csv!!!\n",
      "INFO : 10/02/2021 11:14:58 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:14:59 PM : Extracting top 200 weekly for 2018-11-16--2018-11-23 - it\n",
      "INFO : 10/02/2021 11:14:59 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:14:59 PM : Appending data to the file ../data/italy_2018.csv...\n",
      "INFO : 10/02/2021 11:14:59 PM : Done appending to the file ../data/italy_2018.csv!!!\n",
      "INFO : 10/02/2021 11:14:59 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:15:00 PM : Extracting top 200 weekly for 2018-11-23--2018-11-30 - it\n",
      "INFO : 10/02/2021 11:15:00 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:15:00 PM : Appending data to the file ../data/italy_2018.csv...\n",
      "INFO : 10/02/2021 11:15:00 PM : Done appending to the file ../data/italy_2018.csv!!!\n",
      "INFO : 10/02/2021 11:15:00 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:15:01 PM : Extracting top 200 weekly for 2018-11-30--2018-12-07 - it\n",
      "INFO : 10/02/2021 11:15:01 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:15:01 PM : Appending data to the file ../data/italy_2018.csv...\n",
      "INFO : 10/02/2021 11:15:01 PM : Done appending to the file ../data/italy_2018.csv!!!\n",
      "INFO : 10/02/2021 11:15:01 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:15:02 PM : Extracting top 200 weekly for 2018-12-07--2018-12-14 - it\n",
      "INFO : 10/02/2021 11:15:02 PM : Appending data to the file ../data/italy_2018.csv...\n",
      "INFO : 10/02/2021 11:15:02 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:15:02 PM : Done appending to the file ../data/italy_2018.csv!!!\n",
      "INFO : 10/02/2021 11:15:02 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:15:03 PM : Extracting top 200 weekly for 2018-12-14--2018-12-21 - it\n",
      "INFO : 10/02/2021 11:15:03 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:15:03 PM : Appending data to the file ../data/italy_2018.csv...\n",
      "INFO : 10/02/2021 11:15:03 PM : Done appending to the file ../data/italy_2018.csv!!!\n",
      "INFO : 10/02/2021 11:15:03 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:15:04 PM : Extracting top 200 weekly for 2018-12-21--2018-12-28 - it\n",
      "INFO : 10/02/2021 11:15:04 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:15:04 PM : Appending data to the file ../data/italy_2018.csv...\n",
      "INFO : 10/02/2021 11:15:04 PM : Done appending to the file ../data/italy_2018.csv!!!\n",
      "INFO : 10/02/2021 11:15:04 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:15:05 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:15:05 PM : Appending data to the file ../data/italy_2018.csv...\n",
      "INFO : 10/02/2021 11:15:05 PM : Done appending to the file ../data/italy_2018.csv!!!\n",
      "INFO : 10/02/2021 11:15:05 PM : Done appending to the table top_200_weekly!!!\n"
     ]
    }
   ],
   "source": [
    "api = SpotifyCharts()\n",
    "connector = sqlalchemy.create_engine(\"sqlite:///../data/italy_2018.db\", echo=False)\n",
    "api.top200Weekly(output_file = \"../data/italy_2018.csv\", output_db = connector, webhook = [\"https://mywebhookssite.com/post/\"], start = \"2018-01-01\", end = \"2018-12-31\", region = \"it\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Position</th>\n",
       "      <th>Track Name</th>\n",
       "      <th>Artist</th>\n",
       "      <th>Streams</th>\n",
       "      <th>date</th>\n",
       "      <th>region</th>\n",
       "      <th>spotify_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10195</th>\n",
       "      <td>196</td>\n",
       "      <td>Generale</td>\n",
       "      <td>Anastasio</td>\n",
       "      <td>247340</td>\n",
       "      <td>2018-12-21--2018-12-28</td>\n",
       "      <td>it</td>\n",
       "      <td>3BnGxlDIw2LK4v0mCVCasU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10196</th>\n",
       "      <td>197</td>\n",
       "      <td>3G</td>\n",
       "      <td>Chadia Rodriguez</td>\n",
       "      <td>245370</td>\n",
       "      <td>2018-12-21--2018-12-28</td>\n",
       "      <td>it</td>\n",
       "      <td>6I3pMkkVkCUpu4zB387E8R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10197</th>\n",
       "      <td>198</td>\n",
       "      <td>Chic</td>\n",
       "      <td>Izi</td>\n",
       "      <td>245196</td>\n",
       "      <td>2018-12-21--2018-12-28</td>\n",
       "      <td>it</td>\n",
       "      <td>7jUJ2RmT4PFHHq4goMWqm3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10198</th>\n",
       "      <td>199</td>\n",
       "      <td>Simba</td>\n",
       "      <td>Ernia</td>\n",
       "      <td>245113</td>\n",
       "      <td>2018-12-21--2018-12-28</td>\n",
       "      <td>it</td>\n",
       "      <td>69d3X2Nv5AWHUeTktfOqpV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10199</th>\n",
       "      <td>200</td>\n",
       "      <td>Merci</td>\n",
       "      <td>Carl Brave</td>\n",
       "      <td>244508</td>\n",
       "      <td>2018-12-21--2018-12-28</td>\n",
       "      <td>it</td>\n",
       "      <td>429kanJEBTPrcTKYBT8yPq</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Position Track Name            Artist  Streams                    date  \\\n",
       "10195       196   Generale         Anastasio   247340  2018-12-21--2018-12-28   \n",
       "10196       197         3G  Chadia Rodriguez   245370  2018-12-21--2018-12-28   \n",
       "10197       198       Chic               Izi   245196  2018-12-21--2018-12-28   \n",
       "10198       199      Simba             Ernia   245113  2018-12-21--2018-12-28   \n",
       "10199       200      Merci        Carl Brave   244508  2018-12-21--2018-12-28   \n",
       "\n",
       "      region              spotify_id  \n",
       "10195     it  3BnGxlDIw2LK4v0mCVCasU  \n",
       "10196     it  6I3pMkkVkCUpu4zB387E8R  \n",
       "10197     it  7jUJ2RmT4PFHHq4goMWqm3  \n",
       "10198     it  69d3X2Nv5AWHUeTktfOqpV  \n",
       "10199     it  429kanJEBTPrcTKYBT8yPq  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reading in csv from output file  \n",
    "italy_2018 = pd.read_csv('../data/italy_2018.csv')\n",
    "italy_2018.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Italy Top 200 Weekly, 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : 10/02/2021 11:16:06 PM : The start date 2019-01-01 provided for top200Weekly is invalid. Wanna give one these a try? ['2019-01-04', '2019-01-11', '2019-01-18', '2019-01-25', '2019-02-01']\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter (1) to use the first suggestion, or (2) to quit and set yourself:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : 10/02/2021 11:16:25 PM : Extracting top 200 weekly for 2019-01-04--2019-01-11 - it\n",
      "INFO : 10/02/2021 11:16:26 PM : Extracting top 200 weekly for 2019-01-11--2019-01-18 - it\n",
      "INFO : 10/02/2021 11:16:26 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:16:26 PM : POSTing data to the endpoint https://mywebhookssite.com/post/\n",
      "INFO : 10/02/2021 11:16:26 PM : Appending data to the file ../data/italy_2019.csv...\n",
      "Exception in thread INFO : 10/02/2021 11:16:26 PM : Done appending to the file ../data/italy_2019.csv!!!\n",
      "Thread-12:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/urllib3/connection.py\", line 159, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/urllib3/util/connection.py\", line 61, in create_connection\n",
      "INFO : 10/02/2021 11:16:26 PM : Done appending to the table top_200_weekly!!!\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/socket.py\", line 918, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 8] nodename nor servname provided, or not known\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 670, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 381, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 978, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/urllib3/connection.py\", line 309, in connect\n",
      "    conn = self._new_conn()\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/urllib3/connection.py\", line 171, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x7f98a6d35a60>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/requests/adapters.py\", line 439, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 726, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/urllib3/util/retry.py\", line 446, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='mywebhookssite.com', port=443): Max retries exceeded with url: /post/ (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f98a6d35a60>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/fycharts/SpotifyCharts.py\", line 88, in __post_to_endpoint_from_queue\n",
      "    postToRestEndpoint(df, url, what_data)\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/fycharts/write_to_outputs.py\", line 61, in postToRestEndpoint\n",
      "    raise(e)\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/fycharts/write_to_outputs.py\", line 58, in postToRestEndpoint\n",
      "    requests.post(url, json = dump)\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/requests/api.py\", line 119, in post\n",
      "    return request('post', url, data=data, json=json, **kwargs)\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/requests/api.py\", line 61, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/requests/sessions.py\", line 530, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/requests/sessions.py\", line 643, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/requests/adapters.py\", line 516, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='mywebhookssite.com', port=443): Max retries exceeded with url: /post/ (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f98a6d35a60>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/threading.py\", line 932, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/fycharts/SpotifyCharts.py\", line 90, in __post_to_endpoint_from_queue\n",
      "    raise RuntimeError(e)\n",
      "RuntimeError: HTTPSConnectionPool(host='mywebhookssite.com', port=443): Max retries exceeded with url: /post/ (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f98a6d35a60>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\n",
      "INFO : 10/02/2021 11:16:27 PM : Extracting top 200 weekly for 2019-01-18--2019-01-25 - it\n",
      "INFO : 10/02/2021 11:16:27 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:16:27 PM : Appending data to the file ../data/italy_2019.csv...\n",
      "INFO : 10/02/2021 11:16:28 PM : Done appending to the file ../data/italy_2019.csv!!!\n",
      "INFO : 10/02/2021 11:16:28 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:16:28 PM : Extracting top 200 weekly for 2019-01-25--2019-02-01 - it\n",
      "INFO : 10/02/2021 11:16:28 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:16:28 PM : Appending data to the file ../data/italy_2019.csv...\n",
      "INFO : 10/02/2021 11:16:28 PM : Done appending to the file ../data/italy_2019.csv!!!\n",
      "INFO : 10/02/2021 11:16:28 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:16:30 PM : Extracting top 200 weekly for 2019-02-01--2019-02-08 - it\n",
      "INFO : 10/02/2021 11:16:30 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:16:30 PM : Appending data to the file ../data/italy_2019.csv...\n",
      "INFO : 10/02/2021 11:16:30 PM : Done appending to the file ../data/italy_2019.csv!!!\n",
      "INFO : 10/02/2021 11:16:30 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:16:31 PM : Extracting top 200 weekly for 2019-02-08--2019-02-15 - it\n",
      "INFO : 10/02/2021 11:16:31 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:16:31 PM : Appending data to the file ../data/italy_2019.csv...\n",
      "INFO : 10/02/2021 11:16:31 PM : Done appending to the file ../data/italy_2019.csv!!!\n",
      "INFO : 10/02/2021 11:16:31 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:16:32 PM : Extracting top 200 weekly for 2019-02-15--2019-02-22 - it\n",
      "INFO : 10/02/2021 11:16:32 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:16:32 PM : Appending data to the file ../data/italy_2019.csv...\n",
      "INFO : 10/02/2021 11:16:32 PM : Done appending to the file ../data/italy_2019.csv!!!\n",
      "INFO : 10/02/2021 11:16:32 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:16:32 PM : Extracting top 200 weekly for 2019-02-22--2019-03-01 - it\n",
      "INFO : 10/02/2021 11:16:32 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:16:32 PM : Appending data to the file ../data/italy_2019.csv...\n",
      "INFO : 10/02/2021 11:16:32 PM : Done appending to the file ../data/italy_2019.csv!!!\n",
      "INFO : 10/02/2021 11:16:32 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:16:33 PM : Extracting top 200 weekly for 2019-03-01--2019-03-08 - it\n",
      "INFO : 10/02/2021 11:16:33 PM : Appending data to the file ../data/italy_2019.csv...\n",
      "INFO : 10/02/2021 11:16:33 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:16:33 PM : Done appending to the file ../data/italy_2019.csv!!!\n",
      "INFO : 10/02/2021 11:16:33 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:16:34 PM : Extracting top 200 weekly for 2019-03-08--2019-03-15 - it\n",
      "INFO : 10/02/2021 11:16:34 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:16:34 PM : Appending data to the file ../data/italy_2019.csv...\n",
      "INFO : 10/02/2021 11:16:34 PM : Done appending to the file ../data/italy_2019.csv!!!\n",
      "INFO : 10/02/2021 11:16:34 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:16:35 PM : Extracting top 200 weekly for 2019-03-15--2019-03-22 - it\n",
      "INFO : 10/02/2021 11:16:35 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:16:35 PM : Appending data to the file ../data/italy_2019.csv...\n",
      "INFO : 10/02/2021 11:16:35 PM : Done appending to the file ../data/italy_2019.csv!!!\n",
      "INFO : 10/02/2021 11:16:35 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:16:36 PM : Extracting top 200 weekly for 2019-03-22--2019-03-29 - it\n",
      "INFO : 10/02/2021 11:16:36 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:16:36 PM : Appending data to the file ../data/italy_2019.csv...\n",
      "INFO : 10/02/2021 11:16:36 PM : Done appending to the file ../data/italy_2019.csv!!!\n",
      "INFO : 10/02/2021 11:16:36 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:16:37 PM : Extracting top 200 weekly for 2019-03-29--2019-04-05 - it\n",
      "INFO : 10/02/2021 11:16:37 PM : Appending data to the file ../data/italy_2019.csv...\n",
      "INFO : 10/02/2021 11:16:37 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:16:37 PM : Done appending to the file ../data/italy_2019.csv!!!\n",
      "INFO : 10/02/2021 11:16:37 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:16:38 PM : Extracting top 200 weekly for 2019-04-05--2019-04-12 - it\n",
      "INFO : 10/02/2021 11:16:38 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:16:38 PM : Appending data to the file ../data/italy_2019.csv...\n",
      "INFO : 10/02/2021 11:16:38 PM : Done appending to the file ../data/italy_2019.csv!!!\n",
      "INFO : 10/02/2021 11:16:38 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:16:39 PM : Extracting top 200 weekly for 2019-04-12--2019-04-19 - it\n",
      "INFO : 10/02/2021 11:16:39 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:16:39 PM : Appending data to the file ../data/italy_2019.csv...\n",
      "INFO : 10/02/2021 11:16:39 PM : Done appending to the file ../data/italy_2019.csv!!!\n",
      "INFO : 10/02/2021 11:16:39 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:16:39 PM : Extracting top 200 weekly for 2019-04-19--2019-04-26 - it\n",
      "INFO : 10/02/2021 11:16:39 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:16:39 PM : Appending data to the file ../data/italy_2019.csv...\n",
      "INFO : 10/02/2021 11:16:39 PM : Done appending to the file ../data/italy_2019.csv!!!\n",
      "INFO : 10/02/2021 11:16:39 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:16:40 PM : Extracting top 200 weekly for 2019-04-26--2019-05-03 - it\n",
      "INFO : 10/02/2021 11:16:40 PM : Appending data to the file ../data/italy_2019.csv...\n",
      "INFO : 10/02/2021 11:16:40 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:16:40 PM : Done appending to the file ../data/italy_2019.csv!!!\n",
      "INFO : 10/02/2021 11:16:40 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:16:41 PM : Extracting top 200 weekly for 2019-05-03--2019-05-10 - it\n",
      "INFO : 10/02/2021 11:16:41 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:16:41 PM : Appending data to the file ../data/italy_2019.csv...\n",
      "INFO : 10/02/2021 11:16:41 PM : Done appending to the file ../data/italy_2019.csv!!!\n",
      "INFO : 10/02/2021 11:16:41 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:16:42 PM : Extracting top 200 weekly for 2019-05-10--2019-05-17 - it\n",
      "INFO : 10/02/2021 11:16:42 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:16:42 PM : Appending data to the file ../data/italy_2019.csv...\n",
      "INFO : 10/02/2021 11:16:42 PM : Done appending to the file ../data/italy_2019.csv!!!\n",
      "INFO : 10/02/2021 11:16:42 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:16:43 PM : Extracting top 200 weekly for 2019-05-17--2019-05-24 - it\n",
      "INFO : 10/02/2021 11:16:43 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:16:43 PM : Appending data to the file ../data/italy_2019.csv...\n",
      "INFO : 10/02/2021 11:16:43 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:16:43 PM : Done appending to the file ../data/italy_2019.csv!!!\n",
      "INFO : 10/02/2021 11:16:44 PM : Extracting top 200 weekly for 2019-05-24--2019-05-31 - it\n",
      "INFO : 10/02/2021 11:16:44 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:16:44 PM : Appending data to the file ../data/italy_2019.csv...\n",
      "INFO : 10/02/2021 11:16:44 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:16:44 PM : Done appending to the file ../data/italy_2019.csv!!!\n",
      "INFO : 10/02/2021 11:16:45 PM : Extracting top 200 weekly for 2019-05-31--2019-06-07 - it\n",
      "INFO : 10/02/2021 11:16:45 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:16:45 PM : Appending data to the file ../data/italy_2019.csv...\n",
      "INFO : 10/02/2021 11:16:45 PM : Done appending to the file ../data/italy_2019.csv!!!\n",
      "INFO : 10/02/2021 11:16:45 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:16:46 PM : Extracting top 200 weekly for 2019-06-07--2019-06-14 - it\n",
      "INFO : 10/02/2021 11:16:46 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:16:46 PM : Appending data to the file ../data/italy_2019.csv...\n",
      "INFO : 10/02/2021 11:16:46 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:16:46 PM : Done appending to the file ../data/italy_2019.csv!!!\n",
      "INFO : 10/02/2021 11:16:47 PM : Extracting top 200 weekly for 2019-06-14--2019-06-21 - it\n",
      "INFO : 10/02/2021 11:16:47 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:16:47 PM : Appending data to the file ../data/italy_2019.csv...\n",
      "INFO : 10/02/2021 11:16:47 PM : Done appending to the file ../data/italy_2019.csv!!!\n",
      "INFO : 10/02/2021 11:16:47 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:16:48 PM : Extracting top 200 weekly for 2019-06-21--2019-06-28 - it\n",
      "INFO : 10/02/2021 11:16:48 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:16:48 PM : Appending data to the file ../data/italy_2019.csv...\n",
      "INFO : 10/02/2021 11:16:48 PM : Done appending to the file ../data/italy_2019.csv!!!\n",
      "INFO : 10/02/2021 11:16:48 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:16:49 PM : Extracting top 200 weekly for 2019-06-28--2019-07-05 - it\n",
      "INFO : 10/02/2021 11:16:49 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:16:49 PM : Appending data to the file ../data/italy_2019.csv...\n",
      "INFO : 10/02/2021 11:16:49 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:16:49 PM : Done appending to the file ../data/italy_2019.csv!!!\n",
      "INFO : 10/02/2021 11:16:50 PM : Extracting top 200 weekly for 2019-07-05--2019-07-12 - it\n",
      "INFO : 10/02/2021 11:16:50 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:16:50 PM : Appending data to the file ../data/italy_2019.csv...\n",
      "INFO : 10/02/2021 11:16:50 PM : Done appending to the file ../data/italy_2019.csv!!!\n",
      "INFO : 10/02/2021 11:16:50 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:16:51 PM : Extracting top 200 weekly for 2019-07-12--2019-07-19 - it\n",
      "INFO : 10/02/2021 11:16:51 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:16:51 PM : Appending data to the file ../data/italy_2019.csv...\n",
      "INFO : 10/02/2021 11:16:51 PM : Done appending to the file ../data/italy_2019.csv!!!\n",
      "INFO : 10/02/2021 11:16:51 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:16:52 PM : Extracting top 200 weekly for 2019-07-19--2019-07-26 - it\n",
      "INFO : 10/02/2021 11:16:52 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:16:52 PM : Appending data to the file ../data/italy_2019.csv...\n",
      "INFO : 10/02/2021 11:16:52 PM : Done appending to the file ../data/italy_2019.csv!!!\n",
      "INFO : 10/02/2021 11:16:52 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:16:52 PM : Extracting top 200 weekly for 2019-07-26--2019-08-02 - it\n",
      "INFO : 10/02/2021 11:16:52 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:16:52 PM : Appending data to the file ../data/italy_2019.csv...\n",
      "INFO : 10/02/2021 11:16:52 PM : Done appending to the file ../data/italy_2019.csv!!!\n",
      "INFO : 10/02/2021 11:16:52 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:16:53 PM : Extracting top 200 weekly for 2019-08-02--2019-08-09 - it\n",
      "INFO : 10/02/2021 11:16:53 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:16:53 PM : Appending data to the file ../data/italy_2019.csv...\n",
      "INFO : 10/02/2021 11:16:53 PM : Done appending to the file ../data/italy_2019.csv!!!\n",
      "INFO : 10/02/2021 11:16:53 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:16:54 PM : Extracting top 200 weekly for 2019-08-09--2019-08-16 - it\n",
      "INFO : 10/02/2021 11:16:54 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:16:54 PM : Appending data to the file ../data/italy_2019.csv...\n",
      "INFO : 10/02/2021 11:16:54 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:16:54 PM : Done appending to the file ../data/italy_2019.csv!!!\n",
      "INFO : 10/02/2021 11:16:55 PM : Extracting top 200 weekly for 2019-08-16--2019-08-23 - it\n",
      "INFO : 10/02/2021 11:16:55 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:16:55 PM : Appending data to the file ../data/italy_2019.csv...\n",
      "INFO : 10/02/2021 11:16:55 PM : Done appending to the file ../data/italy_2019.csv!!!\n",
      "INFO : 10/02/2021 11:16:55 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:16:56 PM : Extracting top 200 weekly for 2019-08-23--2019-08-30 - it\n",
      "INFO : 10/02/2021 11:16:56 PM : Appending data to the file ../data/italy_2019.csv...\n",
      "INFO : 10/02/2021 11:16:56 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:16:56 PM : Done appending to the file ../data/italy_2019.csv!!!\n",
      "INFO : 10/02/2021 11:16:56 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:16:57 PM : Extracting top 200 weekly for 2019-08-30--2019-09-06 - it\n",
      "INFO : 10/02/2021 11:16:57 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:16:57 PM : Appending data to the file ../data/italy_2019.csv...\n",
      "INFO : 10/02/2021 11:16:57 PM : Done appending to the file ../data/italy_2019.csv!!!\n",
      "INFO : 10/02/2021 11:16:57 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:16:58 PM : Extracting top 200 weekly for 2019-09-06--2019-09-13 - it\n",
      "INFO : 10/02/2021 11:16:58 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:16:58 PM : Appending data to the file ../data/italy_2019.csv...\n",
      "INFO : 10/02/2021 11:16:58 PM : Done appending to the file ../data/italy_2019.csv!!!\n",
      "INFO : 10/02/2021 11:16:58 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:16:59 PM : Extracting top 200 weekly for 2019-09-13--2019-09-20 - it\n",
      "INFO : 10/02/2021 11:16:59 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:16:59 PM : Appending data to the file ../data/italy_2019.csv...\n",
      "INFO : 10/02/2021 11:16:59 PM : Done appending to the file ../data/italy_2019.csv!!!\n",
      "INFO : 10/02/2021 11:16:59 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:17:00 PM : Extracting top 200 weekly for 2019-09-20--2019-09-27 - it\n",
      "INFO : 10/02/2021 11:17:00 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:17:00 PM : Appending data to the file ../data/italy_2019.csv...\n",
      "INFO : 10/02/2021 11:17:00 PM : Done appending to the file ../data/italy_2019.csv!!!\n",
      "INFO : 10/02/2021 11:17:00 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:17:01 PM : Extracting top 200 weekly for 2019-09-27--2019-10-04 - it\n",
      "INFO : 10/02/2021 11:17:01 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:17:01 PM : Appending data to the file ../data/italy_2019.csv...\n",
      "INFO : 10/02/2021 11:17:01 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:17:01 PM : Done appending to the file ../data/italy_2019.csv!!!\n",
      "INFO : 10/02/2021 11:17:02 PM : Extracting top 200 weekly for 2019-10-04--2019-10-11 - it\n",
      "INFO : 10/02/2021 11:17:02 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:17:02 PM : Appending data to the file ../data/italy_2019.csv...\n",
      "INFO : 10/02/2021 11:17:02 PM : Done appending to the file ../data/italy_2019.csv!!!\n",
      "INFO : 10/02/2021 11:17:02 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:17:03 PM : Extracting top 200 weekly for 2019-10-11--2019-10-18 - it\n",
      "INFO : 10/02/2021 11:17:03 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:17:03 PM : Appending data to the file ../data/italy_2019.csv...\n",
      "INFO : 10/02/2021 11:17:03 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:17:03 PM : Done appending to the file ../data/italy_2019.csv!!!\n",
      "INFO : 10/02/2021 11:17:03 PM : Extracting top 200 weekly for 2019-10-18--2019-10-25 - it\n",
      "INFO : 10/02/2021 11:17:03 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:17:03 PM : Appending data to the file ../data/italy_2019.csv...\n",
      "INFO : 10/02/2021 11:17:03 PM : Done appending to the file ../data/italy_2019.csv!!!\n",
      "INFO : 10/02/2021 11:17:03 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:17:05 PM : Extracting top 200 weekly for 2019-10-25--2019-11-01 - it\n",
      "INFO : 10/02/2021 11:17:05 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:17:05 PM : Appending data to the file ../data/italy_2019.csv...\n",
      "INFO : 10/02/2021 11:17:05 PM : Done appending to the file ../data/italy_2019.csv!!!\n",
      "INFO : 10/02/2021 11:17:05 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:17:06 PM : Extracting top 200 weekly for 2019-11-01--2019-11-08 - it\n",
      "INFO : 10/02/2021 11:17:06 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:17:06 PM : Appending data to the file ../data/italy_2019.csv...\n",
      "INFO : 10/02/2021 11:17:06 PM : Done appending to the file ../data/italy_2019.csv!!!\n",
      "INFO : 10/02/2021 11:17:06 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:17:06 PM : Extracting top 200 weekly for 2019-11-08--2019-11-15 - it\n",
      "INFO : 10/02/2021 11:17:06 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:17:06 PM : Appending data to the file ../data/italy_2019.csv...\n",
      "INFO : 10/02/2021 11:17:06 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:17:06 PM : Done appending to the file ../data/italy_2019.csv!!!\n",
      "INFO : 10/02/2021 11:17:07 PM : Extracting top 200 weekly for 2019-11-15--2019-11-22 - it\n",
      "INFO : 10/02/2021 11:17:07 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:17:07 PM : Appending data to the file ../data/italy_2019.csv...\n",
      "INFO : 10/02/2021 11:17:07 PM : Done appending to the file ../data/italy_2019.csv!!!\n",
      "INFO : 10/02/2021 11:17:07 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:17:08 PM : Extracting top 200 weekly for 2019-11-22--2019-11-29 - it\n",
      "INFO : 10/02/2021 11:17:08 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:17:08 PM : Appending data to the file ../data/italy_2019.csv...\n",
      "INFO : 10/02/2021 11:17:08 PM : Done appending to the file ../data/italy_2019.csv!!!\n",
      "INFO : 10/02/2021 11:17:08 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:17:09 PM : Extracting top 200 weekly for 2019-11-29--2019-12-06 - it\n",
      "INFO : 10/02/2021 11:17:09 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:17:09 PM : Appending data to the file ../data/italy_2019.csv...\n",
      "INFO : 10/02/2021 11:17:09 PM : Done appending to the file ../data/italy_2019.csv!!!\n",
      "INFO : 10/02/2021 11:17:09 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:17:10 PM : Extracting top 200 weekly for 2019-12-06--2019-12-13 - it\n",
      "INFO : 10/02/2021 11:17:10 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:17:10 PM : Appending data to the file ../data/italy_2019.csv...\n",
      "INFO : 10/02/2021 11:17:10 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:17:10 PM : Done appending to the file ../data/italy_2019.csv!!!\n",
      "INFO : 10/02/2021 11:17:11 PM : Extracting top 200 weekly for 2019-12-13--2019-12-20 - it\n",
      "INFO : 10/02/2021 11:17:11 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:17:11 PM : Appending data to the file ../data/italy_2019.csv...\n",
      "INFO : 10/02/2021 11:17:11 PM : Done appending to the file ../data/italy_2019.csv!!!\n",
      "INFO : 10/02/2021 11:17:11 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:17:11 PM : Extracting top 200 weekly for 2019-12-20--2019-12-27 - it\n",
      "INFO : 10/02/2021 11:17:11 PM : Appending data to the file ../data/italy_2019.csv...\n",
      "INFO : 10/02/2021 11:17:11 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:17:12 PM : Done appending to the file ../data/italy_2019.csv!!!\n",
      "INFO : 10/02/2021 11:17:12 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:17:12 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:17:12 PM : Appending data to the file ../data/italy_2019.csv...\n",
      "INFO : 10/02/2021 11:17:12 PM : Done appending to the file ../data/italy_2019.csv!!!\n",
      "INFO : 10/02/2021 11:17:12 PM : Done appending to the table top_200_weekly!!!\n"
     ]
    }
   ],
   "source": [
    "api = SpotifyCharts()\n",
    "connector = sqlalchemy.create_engine(\"sqlite:///../data/italy_2019.db\", echo=False)\n",
    "api.top200Weekly(output_file = \"../data/italy_2019.csv\", output_db = connector, webhook = [\"https://mywebhookssite.com/post/\"], start = \"2019-01-01\", end = \"2019-12-31\", region = \"it\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Position</th>\n",
       "      <th>Track Name</th>\n",
       "      <th>Artist</th>\n",
       "      <th>Streams</th>\n",
       "      <th>date</th>\n",
       "      <th>region</th>\n",
       "      <th>spotify_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10195</th>\n",
       "      <td>196</td>\n",
       "      <td>CORNFLAKES</td>\n",
       "      <td>COMETE</td>\n",
       "      <td>315265</td>\n",
       "      <td>2019-12-20--2019-12-27</td>\n",
       "      <td>it</td>\n",
       "      <td>1VLZV4idlXeZvpisIvoGMk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10196</th>\n",
       "      <td>197</td>\n",
       "      <td>Hallelujah</td>\n",
       "      <td>Pentatonix</td>\n",
       "      <td>315209</td>\n",
       "      <td>2019-12-20--2019-12-27</td>\n",
       "      <td>it</td>\n",
       "      <td>550rQQCGkrTzvp4SfpOPzx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10197</th>\n",
       "      <td>198</td>\n",
       "      <td>Step Into Christmas</td>\n",
       "      <td>Elton John</td>\n",
       "      <td>314886</td>\n",
       "      <td>2019-12-20--2019-12-27</td>\n",
       "      <td>it</td>\n",
       "      <td>6sBWmE23q6xQHlnEZ8jYPT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10198</th>\n",
       "      <td>199</td>\n",
       "      <td>YOSHI (feat. tha Supreme, Fabri Fibra &amp; Capo P...</td>\n",
       "      <td>MACHETE</td>\n",
       "      <td>313194</td>\n",
       "      <td>2019-12-20--2019-12-27</td>\n",
       "      <td>it</td>\n",
       "      <td>0VU34EBWTAlHVIZE3aWlZA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10199</th>\n",
       "      <td>200</td>\n",
       "      <td>Bandito</td>\n",
       "      <td>Enzo Dong</td>\n",
       "      <td>312497</td>\n",
       "      <td>2019-12-20--2019-12-27</td>\n",
       "      <td>it</td>\n",
       "      <td>0c6mLXIsBWgwAAqkagxY5w</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Position                                         Track Name  \\\n",
       "10195       196                                         CORNFLAKES   \n",
       "10196       197                                         Hallelujah   \n",
       "10197       198                                Step Into Christmas   \n",
       "10198       199  YOSHI (feat. tha Supreme, Fabri Fibra & Capo P...   \n",
       "10199       200                                            Bandito   \n",
       "\n",
       "           Artist  Streams                    date region  \\\n",
       "10195      COMETE   315265  2019-12-20--2019-12-27     it   \n",
       "10196  Pentatonix   315209  2019-12-20--2019-12-27     it   \n",
       "10197  Elton John   314886  2019-12-20--2019-12-27     it   \n",
       "10198     MACHETE   313194  2019-12-20--2019-12-27     it   \n",
       "10199   Enzo Dong   312497  2019-12-20--2019-12-27     it   \n",
       "\n",
       "                   spotify_id  \n",
       "10195  1VLZV4idlXeZvpisIvoGMk  \n",
       "10196  550rQQCGkrTzvp4SfpOPzx  \n",
       "10197  6sBWmE23q6xQHlnEZ8jYPT  \n",
       "10198  0VU34EBWTAlHVIZE3aWlZA  \n",
       "10199  0c6mLXIsBWgwAAqkagxY5w  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reading in csv from output file  \n",
    "italy_2019 = pd.read_csv('../data/italy_2019.csv')\n",
    "italy_2019.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Italy Top 200 Weekly, 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : 10/02/2021 11:22:22 PM : The start date 2020-01-01 provided for top200Weekly is invalid. Wanna give one these a try? ['2020-01-03', '2020-01-10', '2020-01-17', '2020-01-24', '2020-01-31']\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter (1) to use the first suggestion, or (2) to quit and set yourself:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : 10/02/2021 11:22:35 PM : Extracting top 200 weekly for 2020-01-03--2020-01-10 - it\n",
      "INFO : 10/02/2021 11:22:36 PM : Extracting top 200 weekly for 2020-01-10--2020-01-17 - it\n",
      "INFO : 10/02/2021 11:22:36 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:22:36 PM : POSTing data to the endpoint https://mywebhookssite.com/post/\n",
      "INFO : 10/02/2021 11:22:36 PM : Appending data to the file ../data/italy_2020.csv...\n",
      "INFO : 10/02/2021 11:22:36 PM : Done appending to the file ../data/italy_2020.csv!!!\n",
      "INFO : 10/02/2021 11:22:36 PM : Done appending to the table top_200_weekly!!!\n",
      "Exception in thread Thread-15:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/urllib3/connection.py\", line 159, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/urllib3/util/connection.py\", line 61, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/socket.py\", line 918, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 8] nodename nor servname provided, or not known\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 670, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 381, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 978, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/urllib3/connection.py\", line 309, in connect\n",
      "    conn = self._new_conn()\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/urllib3/connection.py\", line 171, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x7f98a5d43100>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/requests/adapters.py\", line 439, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 726, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/urllib3/util/retry.py\", line 446, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='mywebhookssite.com', port=443): Max retries exceeded with url: /post/ (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f98a5d43100>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/fycharts/SpotifyCharts.py\", line 88, in __post_to_endpoint_from_queue\n",
      "    postToRestEndpoint(df, url, what_data)\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/fycharts/write_to_outputs.py\", line 61, in postToRestEndpoint\n",
      "    raise(e)\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/fycharts/write_to_outputs.py\", line 58, in postToRestEndpoint\n",
      "    requests.post(url, json = dump)\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/requests/api.py\", line 119, in post\n",
      "    return request('post', url, data=data, json=json, **kwargs)\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/requests/api.py\", line 61, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/requests/sessions.py\", line 530, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/requests/sessions.py\", line 643, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/requests/adapters.py\", line 516, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='mywebhookssite.com', port=443): Max retries exceeded with url: /post/ (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f98a5d43100>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/threading.py\", line 932, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/fycharts/SpotifyCharts.py\", line 90, in __post_to_endpoint_from_queue\n",
      "    raise RuntimeError(e)\n",
      "RuntimeError: HTTPSConnectionPool(host='mywebhookssite.com', port=443): Max retries exceeded with url: /post/ (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f98a5d43100>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\n",
      "INFO : 10/02/2021 11:22:37 PM : Extracting top 200 weekly for 2020-01-17--2020-01-24 - it\n",
      "INFO : 10/02/2021 11:22:37 PM : Appending data to the file ../data/italy_2020.csv...\n",
      "INFO : 10/02/2021 11:22:37 PM : Done appending to the file ../data/italy_2020.csv!!!\n",
      "INFO : 10/02/2021 11:22:37 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:22:37 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:22:38 PM : Extracting top 200 weekly for 2020-01-24--2020-01-31 - it\n",
      "INFO : 10/02/2021 11:22:38 PM : Appending data to the file ../data/italy_2020.csv...\n",
      "INFO : 10/02/2021 11:22:38 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:22:38 PM : Done appending to the file ../data/italy_2020.csv!!!\n",
      "INFO : 10/02/2021 11:22:38 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:22:39 PM : Extracting top 200 weekly for 2020-01-31--2020-02-07 - it\n",
      "INFO : 10/02/2021 11:22:39 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:22:39 PM : Appending data to the file ../data/italy_2020.csv...\n",
      "INFO : 10/02/2021 11:22:39 PM : Done appending to the file ../data/italy_2020.csv!!!\n",
      "INFO : 10/02/2021 11:22:39 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:22:40 PM : Extracting top 200 weekly for 2020-02-07--2020-02-14 - it\n",
      "INFO : 10/02/2021 11:22:40 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:22:40 PM : Appending data to the file ../data/italy_2020.csv...\n",
      "INFO : 10/02/2021 11:22:40 PM : Done appending to the file ../data/italy_2020.csv!!!\n",
      "INFO : 10/02/2021 11:22:40 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:22:41 PM : Extracting top 200 weekly for 2020-02-14--2020-02-21 - it\n",
      "INFO : 10/02/2021 11:22:41 PM : Appending data to the file ../data/italy_2020.csv...\n",
      "INFO : 10/02/2021 11:22:41 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:22:41 PM : Done appending to the file ../data/italy_2020.csv!!!\n",
      "INFO : 10/02/2021 11:22:41 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:22:42 PM : Extracting top 200 weekly for 2020-02-21--2020-02-28 - it\n",
      "INFO : 10/02/2021 11:22:42 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:22:42 PM : Appending data to the file ../data/italy_2020.csv...\n",
      "INFO : 10/02/2021 11:22:42 PM : Done appending to the file ../data/italy_2020.csv!!!\n",
      "INFO : 10/02/2021 11:22:42 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:22:43 PM : Extracting top 200 weekly for 2020-02-28--2020-03-06 - it\n",
      "INFO : 10/02/2021 11:22:43 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:22:43 PM : Appending data to the file ../data/italy_2020.csv...\n",
      "INFO : 10/02/2021 11:22:43 PM : Done appending to the file ../data/italy_2020.csv!!!\n",
      "INFO : 10/02/2021 11:22:43 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:22:44 PM : Extracting top 200 weekly for 2020-03-06--2020-03-13 - it\n",
      "INFO : 10/02/2021 11:22:44 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:22:44 PM : Appending data to the file ../data/italy_2020.csv...\n",
      "INFO : 10/02/2021 11:22:44 PM : Done appending to the file ../data/italy_2020.csv!!!\n",
      "INFO : 10/02/2021 11:22:44 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:22:44 PM : Extracting top 200 weekly for 2020-03-13--2020-03-20 - it\n",
      "INFO : 10/02/2021 11:22:44 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:22:44 PM : Appending data to the file ../data/italy_2020.csv...\n",
      "INFO : 10/02/2021 11:22:44 PM : Done appending to the file ../data/italy_2020.csv!!!\n",
      "INFO : 10/02/2021 11:22:44 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:22:45 PM : Extracting top 200 weekly for 2020-03-20--2020-03-27 - it\n",
      "INFO : 10/02/2021 11:22:45 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:22:45 PM : Appending data to the file ../data/italy_2020.csv...\n",
      "INFO : 10/02/2021 11:22:45 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:22:45 PM : Done appending to the file ../data/italy_2020.csv!!!\n",
      "INFO : 10/02/2021 11:22:46 PM : Extracting top 200 weekly for 2020-03-27--2020-04-03 - it\n",
      "INFO : 10/02/2021 11:22:46 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:22:46 PM : Appending data to the file ../data/italy_2020.csv...\n",
      "INFO : 10/02/2021 11:22:46 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:22:46 PM : Done appending to the file ../data/italy_2020.csv!!!\n",
      "INFO : 10/02/2021 11:22:47 PM : Extracting top 200 weekly for 2020-04-03--2020-04-10 - it\n",
      "INFO : 10/02/2021 11:22:47 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:22:47 PM : Appending data to the file ../data/italy_2020.csv...\n",
      "INFO : 10/02/2021 11:22:47 PM : Done appending to the file ../data/italy_2020.csv!!!\n",
      "INFO : 10/02/2021 11:22:47 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:22:48 PM : Extracting top 200 weekly for 2020-04-10--2020-04-17 - it\n",
      "INFO : 10/02/2021 11:22:48 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:22:48 PM : Appending data to the file ../data/italy_2020.csv...\n",
      "INFO : 10/02/2021 11:22:48 PM : Done appending to the file ../data/italy_2020.csv!!!\n",
      "INFO : 10/02/2021 11:22:48 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:22:49 PM : Extracting top 200 weekly for 2020-04-17--2020-04-24 - it\n",
      "INFO : 10/02/2021 11:22:49 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:22:49 PM : Appending data to the file ../data/italy_2020.csv...\n",
      "INFO : 10/02/2021 11:22:49 PM : Done appending to the file ../data/italy_2020.csv!!!\n",
      "INFO : 10/02/2021 11:22:49 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:22:50 PM : Extracting top 200 weekly for 2020-04-24--2020-05-01 - it\n",
      "INFO : 10/02/2021 11:22:50 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:22:50 PM : Appending data to the file ../data/italy_2020.csv...\n",
      "INFO : 10/02/2021 11:22:50 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:22:50 PM : Done appending to the file ../data/italy_2020.csv!!!\n",
      "INFO : 10/02/2021 11:22:51 PM : Extracting top 200 weekly for 2020-05-01--2020-05-08 - it\n",
      "INFO : 10/02/2021 11:22:51 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:22:51 PM : Appending data to the file ../data/italy_2020.csv...\n",
      "INFO : 10/02/2021 11:22:51 PM : Done appending to the file ../data/italy_2020.csv!!!\n",
      "INFO : 10/02/2021 11:22:51 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:22:53 PM : Extracting top 200 weekly for 2020-05-08--2020-05-15 - it\n",
      "INFO : 10/02/2021 11:22:53 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:22:53 PM : Appending data to the file ../data/italy_2020.csv...\n",
      "INFO : 10/02/2021 11:22:53 PM : Done appending to the file ../data/italy_2020.csv!!!\n",
      "INFO : 10/02/2021 11:22:53 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:22:54 PM : Extracting top 200 weekly for 2020-05-15--2020-05-22 - it\n",
      "INFO : 10/02/2021 11:22:54 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:22:54 PM : Appending data to the file ../data/italy_2020.csv...\n",
      "INFO : 10/02/2021 11:22:54 PM : Done appending to the file ../data/italy_2020.csv!!!\n",
      "INFO : 10/02/2021 11:22:54 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:22:55 PM : Extracting top 200 weekly for 2020-05-22--2020-05-29 - it\n",
      "INFO : 10/02/2021 11:22:55 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:22:55 PM : Appending data to the file ../data/italy_2020.csv...\n",
      "INFO : 10/02/2021 11:22:55 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:22:55 PM : Done appending to the file ../data/italy_2020.csv!!!\n",
      "INFO : 10/02/2021 11:22:55 PM : Extracting top 200 weekly for 2020-05-29--2020-06-05 - it\n",
      "INFO : 10/02/2021 11:22:55 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:22:55 PM : Appending data to the file ../data/italy_2020.csv...\n",
      "INFO : 10/02/2021 11:22:55 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:22:55 PM : Done appending to the file ../data/italy_2020.csv!!!\n",
      "INFO : 10/02/2021 11:22:56 PM : Extracting top 200 weekly for 2020-06-05--2020-06-12 - it\n",
      "INFO : 10/02/2021 11:22:56 PM : Appending data to the file ../data/italy_2020.csv...\n",
      "INFO : 10/02/2021 11:22:56 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:22:56 PM : Done appending to the file ../data/italy_2020.csv!!!\n",
      "INFO : 10/02/2021 11:22:56 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:22:57 PM : Extracting top 200 weekly for 2020-06-12--2020-06-19 - it\n",
      "INFO : 10/02/2021 11:22:57 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:22:57 PM : Appending data to the file ../data/italy_2020.csv...\n",
      "INFO : 10/02/2021 11:22:57 PM : Done appending to the file ../data/italy_2020.csv!!!\n",
      "INFO : 10/02/2021 11:22:57 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:22:58 PM : Extracting top 200 weekly for 2020-06-19--2020-06-26 - it\n",
      "INFO : 10/02/2021 11:22:58 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:22:58 PM : Appending data to the file ../data/italy_2020.csv...\n",
      "INFO : 10/02/2021 11:22:58 PM : Done appending to the file ../data/italy_2020.csv!!!\n",
      "INFO : 10/02/2021 11:22:58 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:22:59 PM : Extracting top 200 weekly for 2020-06-26--2020-07-03 - it\n",
      "INFO : 10/02/2021 11:22:59 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:22:59 PM : Appending data to the file ../data/italy_2020.csv...\n",
      "INFO : 10/02/2021 11:22:59 PM : Done appending to the file ../data/italy_2020.csv!!!\n",
      "INFO : 10/02/2021 11:22:59 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:23:00 PM : Extracting top 200 weekly for 2020-07-03--2020-07-10 - it\n",
      "INFO : 10/02/2021 11:23:00 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:23:00 PM : Appending data to the file ../data/italy_2020.csv...\n",
      "INFO : 10/02/2021 11:23:00 PM : Done appending to the file ../data/italy_2020.csv!!!\n",
      "INFO : 10/02/2021 11:23:00 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:23:01 PM : Extracting top 200 weekly for 2020-07-10--2020-07-17 - it\n",
      "INFO : 10/02/2021 11:23:01 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:23:01 PM : Appending data to the file ../data/italy_2020.csv...\n",
      "INFO : 10/02/2021 11:23:01 PM : Done appending to the file ../data/italy_2020.csv!!!\n",
      "INFO : 10/02/2021 11:23:01 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:23:02 PM : Extracting top 200 weekly for 2020-07-17--2020-07-24 - it\n",
      "INFO : 10/02/2021 11:23:02 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:23:02 PM : Appending data to the file ../data/italy_2020.csv...\n",
      "INFO : 10/02/2021 11:23:02 PM : Done appending to the file ../data/italy_2020.csv!!!\n",
      "INFO : 10/02/2021 11:23:02 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:23:03 PM : Extracting top 200 weekly for 2020-07-24--2020-07-31 - it\n",
      "INFO : 10/02/2021 11:23:03 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:23:03 PM : Appending data to the file ../data/italy_2020.csv...\n",
      "INFO : 10/02/2021 11:23:03 PM : Done appending to the file ../data/italy_2020.csv!!!\n",
      "INFO : 10/02/2021 11:23:03 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:23:03 PM : Extracting top 200 weekly for 2020-07-31--2020-08-07 - it\n",
      "INFO : 10/02/2021 11:23:03 PM : Appending data to the file ../data/italy_2020.csv...\n",
      "INFO : 10/02/2021 11:23:03 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:23:04 PM : Done appending to the file ../data/italy_2020.csv!!!\n",
      "INFO : 10/02/2021 11:23:04 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:23:04 PM : Extracting top 200 weekly for 2020-08-07--2020-08-14 - it\n",
      "INFO : 10/02/2021 11:23:04 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:23:04 PM : Appending data to the file ../data/italy_2020.csv...\n",
      "INFO : 10/02/2021 11:23:04 PM : Done appending to the file ../data/italy_2020.csv!!!\n",
      "INFO : 10/02/2021 11:23:04 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:23:05 PM : Extracting top 200 weekly for 2020-08-14--2020-08-21 - it\n",
      "INFO : 10/02/2021 11:23:05 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:23:05 PM : Appending data to the file ../data/italy_2020.csv...\n",
      "INFO : 10/02/2021 11:23:05 PM : Done appending to the file ../data/italy_2020.csv!!!\n",
      "INFO : 10/02/2021 11:23:05 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:23:06 PM : Extracting top 200 weekly for 2020-08-21--2020-08-28 - it\n",
      "INFO : 10/02/2021 11:23:06 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:23:06 PM : Appending data to the file ../data/italy_2020.csv...\n",
      "INFO : 10/02/2021 11:23:06 PM : Done appending to the file ../data/italy_2020.csv!!!\n",
      "INFO : 10/02/2021 11:23:06 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:23:07 PM : Extracting top 200 weekly for 2020-08-28--2020-09-04 - it\n",
      "INFO : 10/02/2021 11:23:07 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:23:07 PM : Appending data to the file ../data/italy_2020.csv...\n",
      "INFO : 10/02/2021 11:23:07 PM : Done appending to the file ../data/italy_2020.csv!!!\n",
      "INFO : 10/02/2021 11:23:07 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:23:08 PM : Extracting top 200 weekly for 2020-09-04--2020-09-11 - it\n",
      "INFO : 10/02/2021 11:23:08 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:23:08 PM : Appending data to the file ../data/italy_2020.csv...\n",
      "INFO : 10/02/2021 11:23:08 PM : Done appending to the file ../data/italy_2020.csv!!!\n",
      "INFO : 10/02/2021 11:23:08 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:23:09 PM : Extracting top 200 weekly for 2020-09-11--2020-09-18 - it\n",
      "INFO : 10/02/2021 11:23:09 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:23:09 PM : Appending data to the file ../data/italy_2020.csv...\n",
      "INFO : 10/02/2021 11:23:09 PM : Done appending to the file ../data/italy_2020.csv!!!\n",
      "INFO : 10/02/2021 11:23:09 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:23:10 PM : Extracting top 200 weekly for 2020-09-18--2020-09-25 - it\n",
      "INFO : 10/02/2021 11:23:10 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:23:10 PM : Appending data to the file ../data/italy_2020.csv...\n",
      "INFO : 10/02/2021 11:23:10 PM : Done appending to the file ../data/italy_2020.csv!!!\n",
      "INFO : 10/02/2021 11:23:10 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:23:11 PM : Extracting top 200 weekly for 2020-09-25--2020-10-02 - it\n",
      "INFO : 10/02/2021 11:23:11 PM : Appending data to the file ../data/italy_2020.csv...\n",
      "INFO : 10/02/2021 11:23:11 PM : Done appending to the file ../data/italy_2020.csv!!!\n",
      "INFO : 10/02/2021 11:23:11 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:23:11 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:23:12 PM : Extracting top 200 weekly for 2020-10-02--2020-10-09 - it\n",
      "INFO : 10/02/2021 11:23:12 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:23:12 PM : Appending data to the file ../data/italy_2020.csv...\n",
      "INFO : 10/02/2021 11:23:12 PM : Done appending to the file ../data/italy_2020.csv!!!\n",
      "INFO : 10/02/2021 11:23:12 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:23:13 PM : Extracting top 200 weekly for 2020-10-09--2020-10-16 - it\n",
      "INFO : 10/02/2021 11:23:13 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:23:13 PM : Appending data to the file ../data/italy_2020.csv...\n",
      "INFO : 10/02/2021 11:23:13 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:23:13 PM : Done appending to the file ../data/italy_2020.csv!!!\n",
      "INFO : 10/02/2021 11:23:13 PM : Extracting top 200 weekly for 2020-10-16--2020-10-23 - it\n",
      "INFO : 10/02/2021 11:23:13 PM : Appending data to the file ../data/italy_2020.csv...\n",
      "INFO : 10/02/2021 11:23:13 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:23:13 PM : Done appending to the file ../data/italy_2020.csv!!!\n",
      "INFO : 10/02/2021 11:23:14 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:23:14 PM : Extracting top 200 weekly for 2020-10-23--2020-10-30 - it\n",
      "INFO : 10/02/2021 11:23:14 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:23:14 PM : Appending data to the file ../data/italy_2020.csv...\n",
      "INFO : 10/02/2021 11:23:14 PM : Done appending to the file ../data/italy_2020.csv!!!\n",
      "INFO : 10/02/2021 11:23:14 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:23:16 PM : Extracting top 200 weekly for 2020-10-30--2020-11-06 - it\n",
      "INFO : 10/02/2021 11:23:16 PM : Appending data to the file ../data/italy_2020.csv...\n",
      "INFO : 10/02/2021 11:23:16 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:23:16 PM : Done appending to the file ../data/italy_2020.csv!!!\n",
      "INFO : 10/02/2021 11:23:16 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:23:17 PM : Extracting top 200 weekly for 2020-11-06--2020-11-13 - it\n",
      "INFO : 10/02/2021 11:23:17 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:23:17 PM : Appending data to the file ../data/italy_2020.csv...\n",
      "INFO : 10/02/2021 11:23:17 PM : Done appending to the file ../data/italy_2020.csv!!!\n",
      "INFO : 10/02/2021 11:23:17 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:23:18 PM : Extracting top 200 weekly for 2020-11-13--2020-11-20 - it\n",
      "INFO : 10/02/2021 11:23:18 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:23:18 PM : Appending data to the file ../data/italy_2020.csv...\n",
      "INFO : 10/02/2021 11:23:18 PM : Done appending to the file ../data/italy_2020.csv!!!\n",
      "INFO : 10/02/2021 11:23:18 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:23:19 PM : Extracting top 200 weekly for 2020-11-20--2020-11-27 - it\n",
      "INFO : 10/02/2021 11:23:19 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:23:19 PM : Appending data to the file ../data/italy_2020.csv...\n",
      "INFO : 10/02/2021 11:23:19 PM : Done appending to the file ../data/italy_2020.csv!!!\n",
      "INFO : 10/02/2021 11:23:19 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:23:20 PM : Extracting top 200 weekly for 2020-11-27--2020-12-04 - it\n",
      "INFO : 10/02/2021 11:23:20 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:23:20 PM : Appending data to the file ../data/italy_2020.csv...\n",
      "INFO : 10/02/2021 11:23:20 PM : Done appending to the file ../data/italy_2020.csv!!!\n",
      "INFO : 10/02/2021 11:23:20 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:23:20 PM : Extracting top 200 weekly for 2020-12-04--2020-12-11 - it\n",
      "INFO : 10/02/2021 11:23:20 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:23:20 PM : Appending data to the file ../data/italy_2020.csv...\n",
      "INFO : 10/02/2021 11:23:20 PM : Done appending to the file ../data/italy_2020.csv!!!\n",
      "INFO : 10/02/2021 11:23:20 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:23:21 PM : Extracting top 200 weekly for 2020-12-11--2020-12-18 - it\n",
      "INFO : 10/02/2021 11:23:21 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:23:21 PM : Appending data to the file ../data/italy_2020.csv...\n",
      "INFO : 10/02/2021 11:23:21 PM : Done appending to the file ../data/italy_2020.csv!!!\n",
      "INFO : 10/02/2021 11:23:21 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:23:22 PM : Extracting top 200 weekly for 2020-12-18--2020-12-25 - it\n",
      "INFO : 10/02/2021 11:23:22 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:23:22 PM : Appending data to the file ../data/italy_2020.csv...\n",
      "INFO : 10/02/2021 11:23:23 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 10/02/2021 11:23:23 PM : Done appending to the file ../data/italy_2020.csv!!!\n",
      "INFO : 10/02/2021 11:23:23 PM : Appending data to the table top_200_weekly\n",
      "INFO : 10/02/2021 11:23:23 PM : Appending data to the file ../data/italy_2020.csv...\n",
      "INFO : 10/02/2021 11:23:23 PM : Done appending to the file ../data/italy_2020.csv!!!\n",
      "INFO : 10/02/2021 11:23:23 PM : Done appending to the table top_200_weekly!!!\n"
     ]
    }
   ],
   "source": [
    "api = SpotifyCharts()\n",
    "connector = sqlalchemy.create_engine(\"sqlite:///../data/italy_2020.db\", echo=False)\n",
    "api.top200Weekly(output_file = \"../data/italy_2020.csv\", output_db = connector, webhook = [\"https://mywebhookssite.com/post/\"], start = \"2020-01-01\", end = \"2020-12-31\", region = \"it\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Position</th>\n",
       "      <th>Track Name</th>\n",
       "      <th>Artist</th>\n",
       "      <th>Streams</th>\n",
       "      <th>date</th>\n",
       "      <th>region</th>\n",
       "      <th>spotify_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>blun7 a swishland</td>\n",
       "      <td>tha Supreme</td>\n",
       "      <td>2759917</td>\n",
       "      <td>2020-01-03--2020-01-10</td>\n",
       "      <td>it</td>\n",
       "      <td>7HwvPmK74MBRDhCIyMXReP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>fuck 3x</td>\n",
       "      <td>tha Supreme</td>\n",
       "      <td>1794877</td>\n",
       "      <td>2020-01-03--2020-01-10</td>\n",
       "      <td>it</td>\n",
       "      <td>5YxP1CkunbhUQVvctFOHa7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Dance Monkey</td>\n",
       "      <td>Tones And I</td>\n",
       "      <td>1758744</td>\n",
       "      <td>2020-01-03--2020-01-10</td>\n",
       "      <td>it</td>\n",
       "      <td>1rgnBhdG2JDFTbYkYRZAku</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Ti volevo dedicare (feat. J-AX &amp; Boomdabash)</td>\n",
       "      <td>Rocco Hunt</td>\n",
       "      <td>1551080</td>\n",
       "      <td>2020-01-03--2020-01-10</td>\n",
       "      <td>it</td>\n",
       "      <td>00GxbkrW4m1Tac5xySEJ4M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>ANSIA NO</td>\n",
       "      <td>FSK SATELLITE</td>\n",
       "      <td>1548224</td>\n",
       "      <td>2020-01-03--2020-01-10</td>\n",
       "      <td>it</td>\n",
       "      <td>2yuYI5NFhevxa05se7Qht9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Position                                    Track Name         Artist  \\\n",
       "0         1                             blun7 a swishland    tha Supreme   \n",
       "1         2                                       fuck 3x    tha Supreme   \n",
       "2         3                                  Dance Monkey    Tones And I   \n",
       "3         4  Ti volevo dedicare (feat. J-AX & Boomdabash)     Rocco Hunt   \n",
       "4         5                                      ANSIA NO  FSK SATELLITE   \n",
       "\n",
       "   Streams                    date region              spotify_id  \n",
       "0  2759917  2020-01-03--2020-01-10     it  7HwvPmK74MBRDhCIyMXReP  \n",
       "1  1794877  2020-01-03--2020-01-10     it  5YxP1CkunbhUQVvctFOHa7  \n",
       "2  1758744  2020-01-03--2020-01-10     it  1rgnBhdG2JDFTbYkYRZAku  \n",
       "3  1551080  2020-01-03--2020-01-10     it  00GxbkrW4m1Tac5xySEJ4M  \n",
       "4  1548224  2020-01-03--2020-01-10     it  2yuYI5NFhevxa05se7Qht9  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reading in csv from output file  \n",
    "italy_2020 = pd.read_csv('../data/italy_2020.csv')\n",
    "italy_2020.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10200, 7)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "italy_2017.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10200, 7)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "italy_2018.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10200, 7)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "italy_2019.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10200, 7)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "italy_2020.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_I scraped the charts for each separately in case I wanted to separate DataFrames, but now I will now merge into one DataFrame years 2017-2019 because this is more useful for EDA and the start of time series modeling._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Position</th>\n",
       "      <th>Track Name</th>\n",
       "      <th>Artist</th>\n",
       "      <th>Streams</th>\n",
       "      <th>date</th>\n",
       "      <th>region</th>\n",
       "      <th>spotify_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Shape of You</td>\n",
       "      <td>Ed Sheeran</td>\n",
       "      <td>1051142</td>\n",
       "      <td>2017-01-06--2017-01-13</td>\n",
       "      <td>it</td>\n",
       "      <td>7qiZfU4dY1lWllzX7mPBI3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Rockabye (feat. Sean Paul &amp; Anne-Marie)</td>\n",
       "      <td>Clean Bandit</td>\n",
       "      <td>759626</td>\n",
       "      <td>2017-01-06--2017-01-13</td>\n",
       "      <td>it</td>\n",
       "      <td>5knuzwU65gJK7IF5yJsuaW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Castle on the Hill</td>\n",
       "      <td>Ed Sheeran</td>\n",
       "      <td>715171</td>\n",
       "      <td>2017-01-06--2017-01-13</td>\n",
       "      <td>it</td>\n",
       "      <td>6PCUP3dWmTjcTtXY02oFdT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Closer</td>\n",
       "      <td>The Chainsmokers</td>\n",
       "      <td>504232</td>\n",
       "      <td>2017-01-06--2017-01-13</td>\n",
       "      <td>it</td>\n",
       "      <td>7BKLCZ1jbUBVqRi2FVlTVw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Assenzio (feat. Stash &amp; Levante)</td>\n",
       "      <td>J-AX</td>\n",
       "      <td>478975</td>\n",
       "      <td>2017-01-06--2017-01-13</td>\n",
       "      <td>it</td>\n",
       "      <td>0DRKnh0BloxJHyhXkfbiX8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Position                               Track Name            Artist  \\\n",
       "0         1                             Shape of You        Ed Sheeran   \n",
       "1         2  Rockabye (feat. Sean Paul & Anne-Marie)      Clean Bandit   \n",
       "2         3                       Castle on the Hill        Ed Sheeran   \n",
       "3         4                                   Closer  The Chainsmokers   \n",
       "4         5         Assenzio (feat. Stash & Levante)              J-AX   \n",
       "\n",
       "   Streams                    date region              spotify_id  \n",
       "0  1051142  2017-01-06--2017-01-13     it  7qiZfU4dY1lWllzX7mPBI3  \n",
       "1   759626  2017-01-06--2017-01-13     it  5knuzwU65gJK7IF5yJsuaW  \n",
       "2   715171  2017-01-06--2017-01-13     it  6PCUP3dWmTjcTtXY02oFdT  \n",
       "3   504232  2017-01-06--2017-01-13     it  7BKLCZ1jbUBVqRi2FVlTVw  \n",
       "4   478975  2017-01-06--2017-01-13     it  0DRKnh0BloxJHyhXkfbiX8  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "italy_17_19 = pd.concat([italy_2017, italy_2018, italy_2019])\n",
    "italy_17_19.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Position</th>\n",
       "      <th>Track Name</th>\n",
       "      <th>Artist</th>\n",
       "      <th>Streams</th>\n",
       "      <th>date</th>\n",
       "      <th>region</th>\n",
       "      <th>spotify_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30595</th>\n",
       "      <td>196</td>\n",
       "      <td>CORNFLAKES</td>\n",
       "      <td>COMETE</td>\n",
       "      <td>315265</td>\n",
       "      <td>2019-12-20--2019-12-27</td>\n",
       "      <td>it</td>\n",
       "      <td>1VLZV4idlXeZvpisIvoGMk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30596</th>\n",
       "      <td>197</td>\n",
       "      <td>Hallelujah</td>\n",
       "      <td>Pentatonix</td>\n",
       "      <td>315209</td>\n",
       "      <td>2019-12-20--2019-12-27</td>\n",
       "      <td>it</td>\n",
       "      <td>550rQQCGkrTzvp4SfpOPzx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30597</th>\n",
       "      <td>198</td>\n",
       "      <td>Step Into Christmas</td>\n",
       "      <td>Elton John</td>\n",
       "      <td>314886</td>\n",
       "      <td>2019-12-20--2019-12-27</td>\n",
       "      <td>it</td>\n",
       "      <td>6sBWmE23q6xQHlnEZ8jYPT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30598</th>\n",
       "      <td>199</td>\n",
       "      <td>YOSHI (feat. tha Supreme, Fabri Fibra &amp; Capo P...</td>\n",
       "      <td>MACHETE</td>\n",
       "      <td>313194</td>\n",
       "      <td>2019-12-20--2019-12-27</td>\n",
       "      <td>it</td>\n",
       "      <td>0VU34EBWTAlHVIZE3aWlZA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30599</th>\n",
       "      <td>200</td>\n",
       "      <td>Bandito</td>\n",
       "      <td>Enzo Dong</td>\n",
       "      <td>312497</td>\n",
       "      <td>2019-12-20--2019-12-27</td>\n",
       "      <td>it</td>\n",
       "      <td>0c6mLXIsBWgwAAqkagxY5w</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Position                                         Track Name  \\\n",
       "30595       196                                         CORNFLAKES   \n",
       "30596       197                                         Hallelujah   \n",
       "30597       198                                Step Into Christmas   \n",
       "30598       199  YOSHI (feat. tha Supreme, Fabri Fibra & Capo P...   \n",
       "30599       200                                            Bandito   \n",
       "\n",
       "           Artist  Streams                    date region  \\\n",
       "30595      COMETE   315265  2019-12-20--2019-12-27     it   \n",
       "30596  Pentatonix   315209  2019-12-20--2019-12-27     it   \n",
       "30597  Elton John   314886  2019-12-20--2019-12-27     it   \n",
       "30598     MACHETE   313194  2019-12-20--2019-12-27     it   \n",
       "30599   Enzo Dong   312497  2019-12-20--2019-12-27     it   \n",
       "\n",
       "                   spotify_id  \n",
       "30595  1VLZV4idlXeZvpisIvoGMk  \n",
       "30596  550rQQCGkrTzvp4SfpOPzx  \n",
       "30597  6sBWmE23q6xQHlnEZ8jYPT  \n",
       "30598  0VU34EBWTAlHVIZE3aWlZA  \n",
       "30599  0c6mLXIsBWgwAAqkagxY5w  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# resettting index so it can be concatenated with features dataframe later on\n",
    "italy_17_19.reset_index(drop=True, inplace=True)\n",
    "italy_17_19.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30600, 7)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "italy_17_19.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 30600 entries, 0 to 30599\n",
      "Data columns (total 7 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   Position    30600 non-null  int64 \n",
      " 1   Track Name  30595 non-null  object\n",
      " 2   Artist      30595 non-null  object\n",
      " 3   Streams     30600 non-null  int64 \n",
      " 4   date        30600 non-null  object\n",
      " 5   region      30600 non-null  object\n",
      " 6   spotify_id  30600 non-null  object\n",
      "dtypes: int64(2), object(5)\n",
      "memory usage: 1.6+ MB\n"
     ]
    }
   ],
   "source": [
    "italy_17_19.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audio Features "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtaining access to audio features for each track using Spotify's Client Credentials Flow (see above) and a wrapper library [Spotipy](https://spotipy.readthedocs.io/en/2.16.1/). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_**Citation: Code for scraping audio features borrowed from [CNN_for_Dance_Music_Classification](https://github.com/amytaylor330/CNN_for_Dance_Music_Classification)._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tracks where no audio features were available: 0\n",
      "Number of usable tracks: 30600\n"
     ]
    }
   ],
   "source": [
    "features_list_italy_17_19 = []          # will create a list of each track's audio features, each appended as a separate dictionary \n",
    "batchsize = 100    # max number of track ids we're allowed to submit per query\n",
    "None_counter = 0   # count if there are any songs without any audio features\n",
    "\n",
    "for i in range(0,len(italy_17_19['spotify_id']), batchsize):    \n",
    "    batch = italy_17_19['spotify_id'][i:i+batchsize]           # offsetting batchsize to acquire more tracks \n",
    "    \n",
    "    feature_results = sp.audio_features(batch)              #begins querying the audio features endpoint\n",
    "\n",
    "    for i, t in enumerate(feature_results):\n",
    "        if t == None:                               #if the audio features for a song are missing, count 1        \n",
    "            None_counter += 1          \n",
    "        else:\n",
    "            features_list_italy_17_19.append(t)  \n",
    "            \n",
    "print('Number of tracks where no audio features were available:', None_counter)\n",
    "print('Number of usable tracks:', len(features_list_italy_17_19))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>key</th>\n",
       "      <th>loudness</th>\n",
       "      <th>mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>valence</th>\n",
       "      <th>tempo</th>\n",
       "      <th>type</th>\n",
       "      <th>id</th>\n",
       "      <th>uri</th>\n",
       "      <th>track_href</th>\n",
       "      <th>analysis_url</th>\n",
       "      <th>duration_ms</th>\n",
       "      <th>time_signature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.825</td>\n",
       "      <td>0.652</td>\n",
       "      <td>1</td>\n",
       "      <td>-3.183</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0802</td>\n",
       "      <td>0.5810</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0931</td>\n",
       "      <td>0.931</td>\n",
       "      <td>95.977</td>\n",
       "      <td>audio_features</td>\n",
       "      <td>7qiZfU4dY1lWllzX7mPBI3</td>\n",
       "      <td>spotify:track:7qiZfU4dY1lWllzX7mPBI3</td>\n",
       "      <td>https://api.spotify.com/v1/tracks/7qiZfU4dY1lW...</td>\n",
       "      <td>https://api.spotify.com/v1/audio-analysis/7qiZ...</td>\n",
       "      <td>233713</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.720</td>\n",
       "      <td>0.763</td>\n",
       "      <td>9</td>\n",
       "      <td>-4.068</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0523</td>\n",
       "      <td>0.4060</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1800</td>\n",
       "      <td>0.742</td>\n",
       "      <td>101.965</td>\n",
       "      <td>audio_features</td>\n",
       "      <td>5knuzwU65gJK7IF5yJsuaW</td>\n",
       "      <td>spotify:track:5knuzwU65gJK7IF5yJsuaW</td>\n",
       "      <td>https://api.spotify.com/v1/tracks/5knuzwU65gJK...</td>\n",
       "      <td>https://api.spotify.com/v1/audio-analysis/5knu...</td>\n",
       "      <td>251088</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.461</td>\n",
       "      <td>0.834</td>\n",
       "      <td>2</td>\n",
       "      <td>-4.868</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0989</td>\n",
       "      <td>0.0232</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.1400</td>\n",
       "      <td>0.471</td>\n",
       "      <td>135.007</td>\n",
       "      <td>audio_features</td>\n",
       "      <td>6PCUP3dWmTjcTtXY02oFdT</td>\n",
       "      <td>spotify:track:6PCUP3dWmTjcTtXY02oFdT</td>\n",
       "      <td>https://api.spotify.com/v1/tracks/6PCUP3dWmTjc...</td>\n",
       "      <td>https://api.spotify.com/v1/audio-analysis/6PCU...</td>\n",
       "      <td>261154</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.748</td>\n",
       "      <td>0.524</td>\n",
       "      <td>8</td>\n",
       "      <td>-5.599</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0338</td>\n",
       "      <td>0.4140</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1110</td>\n",
       "      <td>0.661</td>\n",
       "      <td>95.010</td>\n",
       "      <td>audio_features</td>\n",
       "      <td>7BKLCZ1jbUBVqRi2FVlTVw</td>\n",
       "      <td>spotify:track:7BKLCZ1jbUBVqRi2FVlTVw</td>\n",
       "      <td>https://api.spotify.com/v1/tracks/7BKLCZ1jbUBV...</td>\n",
       "      <td>https://api.spotify.com/v1/audio-analysis/7BKL...</td>\n",
       "      <td>244960</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.554</td>\n",
       "      <td>0.640</td>\n",
       "      <td>0</td>\n",
       "      <td>-7.587</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1870</td>\n",
       "      <td>0.3410</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1080</td>\n",
       "      <td>0.641</td>\n",
       "      <td>160.009</td>\n",
       "      <td>audio_features</td>\n",
       "      <td>0DRKnh0BloxJHyhXkfbiX8</td>\n",
       "      <td>spotify:track:0DRKnh0BloxJHyhXkfbiX8</td>\n",
       "      <td>https://api.spotify.com/v1/tracks/0DRKnh0BloxJ...</td>\n",
       "      <td>https://api.spotify.com/v1/audio-analysis/0DRK...</td>\n",
       "      <td>250533</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   danceability  energy  key  loudness  mode  speechiness  acousticness  \\\n",
       "0         0.825   0.652    1    -3.183     0       0.0802        0.5810   \n",
       "1         0.720   0.763    9    -4.068     0       0.0523        0.4060   \n",
       "2         0.461   0.834    2    -4.868     1       0.0989        0.0232   \n",
       "3         0.748   0.524    8    -5.599     1       0.0338        0.4140   \n",
       "4         0.554   0.640    0    -7.587     1       0.1870        0.3410   \n",
       "\n",
       "   instrumentalness  liveness  valence    tempo            type  \\\n",
       "0          0.000000    0.0931    0.931   95.977  audio_features   \n",
       "1          0.000000    0.1800    0.742  101.965  audio_features   \n",
       "2          0.000011    0.1400    0.471  135.007  audio_features   \n",
       "3          0.000000    0.1110    0.661   95.010  audio_features   \n",
       "4          0.000000    0.1080    0.641  160.009  audio_features   \n",
       "\n",
       "                       id                                   uri  \\\n",
       "0  7qiZfU4dY1lWllzX7mPBI3  spotify:track:7qiZfU4dY1lWllzX7mPBI3   \n",
       "1  5knuzwU65gJK7IF5yJsuaW  spotify:track:5knuzwU65gJK7IF5yJsuaW   \n",
       "2  6PCUP3dWmTjcTtXY02oFdT  spotify:track:6PCUP3dWmTjcTtXY02oFdT   \n",
       "3  7BKLCZ1jbUBVqRi2FVlTVw  spotify:track:7BKLCZ1jbUBVqRi2FVlTVw   \n",
       "4  0DRKnh0BloxJHyhXkfbiX8  spotify:track:0DRKnh0BloxJHyhXkfbiX8   \n",
       "\n",
       "                                          track_href  \\\n",
       "0  https://api.spotify.com/v1/tracks/7qiZfU4dY1lW...   \n",
       "1  https://api.spotify.com/v1/tracks/5knuzwU65gJK...   \n",
       "2  https://api.spotify.com/v1/tracks/6PCUP3dWmTjc...   \n",
       "3  https://api.spotify.com/v1/tracks/7BKLCZ1jbUBV...   \n",
       "4  https://api.spotify.com/v1/tracks/0DRKnh0BloxJ...   \n",
       "\n",
       "                                        analysis_url  duration_ms  \\\n",
       "0  https://api.spotify.com/v1/audio-analysis/7qiZ...       233713   \n",
       "1  https://api.spotify.com/v1/audio-analysis/5knu...       251088   \n",
       "2  https://api.spotify.com/v1/audio-analysis/6PCU...       261154   \n",
       "3  https://api.spotify.com/v1/audio-analysis/7BKL...       244960   \n",
       "4  https://api.spotify.com/v1/audio-analysis/0DRK...       250533   \n",
       "\n",
       "   time_signature  \n",
       "0               4  \n",
       "1               4  \n",
       "2               4  \n",
       "3               4  \n",
       "4               4  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# building dataframe from features_list, which is a list of dictionaries (each dictionary represents the audio features from one song)\n",
    "italy_17_19_features = pd.DataFrame(features_list_italy_17_19)\n",
    "italy_17_19_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merging audio features into song charts DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30600, 7)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "italy_17_19.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30600, 18)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "italy_17_19_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Position</th>\n",
       "      <th>Track Name</th>\n",
       "      <th>Artist</th>\n",
       "      <th>Streams</th>\n",
       "      <th>date</th>\n",
       "      <th>region</th>\n",
       "      <th>spotify_id</th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>key</th>\n",
       "      <th>...</th>\n",
       "      <th>liveness</th>\n",
       "      <th>valence</th>\n",
       "      <th>tempo</th>\n",
       "      <th>type</th>\n",
       "      <th>id</th>\n",
       "      <th>uri</th>\n",
       "      <th>track_href</th>\n",
       "      <th>analysis_url</th>\n",
       "      <th>duration_ms</th>\n",
       "      <th>time_signature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Shape of You</td>\n",
       "      <td>Ed Sheeran</td>\n",
       "      <td>1051142</td>\n",
       "      <td>2017-01-06--2017-01-13</td>\n",
       "      <td>it</td>\n",
       "      <td>7qiZfU4dY1lWllzX7mPBI3</td>\n",
       "      <td>0.825</td>\n",
       "      <td>0.652</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0931</td>\n",
       "      <td>0.931</td>\n",
       "      <td>95.977</td>\n",
       "      <td>audio_features</td>\n",
       "      <td>7qiZfU4dY1lWllzX7mPBI3</td>\n",
       "      <td>spotify:track:7qiZfU4dY1lWllzX7mPBI3</td>\n",
       "      <td>https://api.spotify.com/v1/tracks/7qiZfU4dY1lW...</td>\n",
       "      <td>https://api.spotify.com/v1/audio-analysis/7qiZ...</td>\n",
       "      <td>233713</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Rockabye (feat. Sean Paul &amp; Anne-Marie)</td>\n",
       "      <td>Clean Bandit</td>\n",
       "      <td>759626</td>\n",
       "      <td>2017-01-06--2017-01-13</td>\n",
       "      <td>it</td>\n",
       "      <td>5knuzwU65gJK7IF5yJsuaW</td>\n",
       "      <td>0.720</td>\n",
       "      <td>0.763</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1800</td>\n",
       "      <td>0.742</td>\n",
       "      <td>101.965</td>\n",
       "      <td>audio_features</td>\n",
       "      <td>5knuzwU65gJK7IF5yJsuaW</td>\n",
       "      <td>spotify:track:5knuzwU65gJK7IF5yJsuaW</td>\n",
       "      <td>https://api.spotify.com/v1/tracks/5knuzwU65gJK...</td>\n",
       "      <td>https://api.spotify.com/v1/audio-analysis/5knu...</td>\n",
       "      <td>251088</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Castle on the Hill</td>\n",
       "      <td>Ed Sheeran</td>\n",
       "      <td>715171</td>\n",
       "      <td>2017-01-06--2017-01-13</td>\n",
       "      <td>it</td>\n",
       "      <td>6PCUP3dWmTjcTtXY02oFdT</td>\n",
       "      <td>0.461</td>\n",
       "      <td>0.834</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1400</td>\n",
       "      <td>0.471</td>\n",
       "      <td>135.007</td>\n",
       "      <td>audio_features</td>\n",
       "      <td>6PCUP3dWmTjcTtXY02oFdT</td>\n",
       "      <td>spotify:track:6PCUP3dWmTjcTtXY02oFdT</td>\n",
       "      <td>https://api.spotify.com/v1/tracks/6PCUP3dWmTjc...</td>\n",
       "      <td>https://api.spotify.com/v1/audio-analysis/6PCU...</td>\n",
       "      <td>261154</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Closer</td>\n",
       "      <td>The Chainsmokers</td>\n",
       "      <td>504232</td>\n",
       "      <td>2017-01-06--2017-01-13</td>\n",
       "      <td>it</td>\n",
       "      <td>7BKLCZ1jbUBVqRi2FVlTVw</td>\n",
       "      <td>0.748</td>\n",
       "      <td>0.524</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1110</td>\n",
       "      <td>0.661</td>\n",
       "      <td>95.010</td>\n",
       "      <td>audio_features</td>\n",
       "      <td>7BKLCZ1jbUBVqRi2FVlTVw</td>\n",
       "      <td>spotify:track:7BKLCZ1jbUBVqRi2FVlTVw</td>\n",
       "      <td>https://api.spotify.com/v1/tracks/7BKLCZ1jbUBV...</td>\n",
       "      <td>https://api.spotify.com/v1/audio-analysis/7BKL...</td>\n",
       "      <td>244960</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Assenzio (feat. Stash &amp; Levante)</td>\n",
       "      <td>J-AX</td>\n",
       "      <td>478975</td>\n",
       "      <td>2017-01-06--2017-01-13</td>\n",
       "      <td>it</td>\n",
       "      <td>0DRKnh0BloxJHyhXkfbiX8</td>\n",
       "      <td>0.554</td>\n",
       "      <td>0.640</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1080</td>\n",
       "      <td>0.641</td>\n",
       "      <td>160.009</td>\n",
       "      <td>audio_features</td>\n",
       "      <td>0DRKnh0BloxJHyhXkfbiX8</td>\n",
       "      <td>spotify:track:0DRKnh0BloxJHyhXkfbiX8</td>\n",
       "      <td>https://api.spotify.com/v1/tracks/0DRKnh0BloxJ...</td>\n",
       "      <td>https://api.spotify.com/v1/audio-analysis/0DRK...</td>\n",
       "      <td>250533</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Position                               Track Name            Artist  \\\n",
       "0         1                             Shape of You        Ed Sheeran   \n",
       "1         2  Rockabye (feat. Sean Paul & Anne-Marie)      Clean Bandit   \n",
       "2         3                       Castle on the Hill        Ed Sheeran   \n",
       "3         4                                   Closer  The Chainsmokers   \n",
       "4         5         Assenzio (feat. Stash & Levante)              J-AX   \n",
       "\n",
       "   Streams                    date region              spotify_id  \\\n",
       "0  1051142  2017-01-06--2017-01-13     it  7qiZfU4dY1lWllzX7mPBI3   \n",
       "1   759626  2017-01-06--2017-01-13     it  5knuzwU65gJK7IF5yJsuaW   \n",
       "2   715171  2017-01-06--2017-01-13     it  6PCUP3dWmTjcTtXY02oFdT   \n",
       "3   504232  2017-01-06--2017-01-13     it  7BKLCZ1jbUBVqRi2FVlTVw   \n",
       "4   478975  2017-01-06--2017-01-13     it  0DRKnh0BloxJHyhXkfbiX8   \n",
       "\n",
       "   danceability  energy  key  ...  liveness  valence    tempo            type  \\\n",
       "0         0.825   0.652    1  ...    0.0931    0.931   95.977  audio_features   \n",
       "1         0.720   0.763    9  ...    0.1800    0.742  101.965  audio_features   \n",
       "2         0.461   0.834    2  ...    0.1400    0.471  135.007  audio_features   \n",
       "3         0.748   0.524    8  ...    0.1110    0.661   95.010  audio_features   \n",
       "4         0.554   0.640    0  ...    0.1080    0.641  160.009  audio_features   \n",
       "\n",
       "                       id                                   uri  \\\n",
       "0  7qiZfU4dY1lWllzX7mPBI3  spotify:track:7qiZfU4dY1lWllzX7mPBI3   \n",
       "1  5knuzwU65gJK7IF5yJsuaW  spotify:track:5knuzwU65gJK7IF5yJsuaW   \n",
       "2  6PCUP3dWmTjcTtXY02oFdT  spotify:track:6PCUP3dWmTjcTtXY02oFdT   \n",
       "3  7BKLCZ1jbUBVqRi2FVlTVw  spotify:track:7BKLCZ1jbUBVqRi2FVlTVw   \n",
       "4  0DRKnh0BloxJHyhXkfbiX8  spotify:track:0DRKnh0BloxJHyhXkfbiX8   \n",
       "\n",
       "                                          track_href  \\\n",
       "0  https://api.spotify.com/v1/tracks/7qiZfU4dY1lW...   \n",
       "1  https://api.spotify.com/v1/tracks/5knuzwU65gJK...   \n",
       "2  https://api.spotify.com/v1/tracks/6PCUP3dWmTjc...   \n",
       "3  https://api.spotify.com/v1/tracks/7BKLCZ1jbUBV...   \n",
       "4  https://api.spotify.com/v1/tracks/0DRKnh0BloxJ...   \n",
       "\n",
       "                                        analysis_url duration_ms  \\\n",
       "0  https://api.spotify.com/v1/audio-analysis/7qiZ...      233713   \n",
       "1  https://api.spotify.com/v1/audio-analysis/5knu...      251088   \n",
       "2  https://api.spotify.com/v1/audio-analysis/6PCU...      261154   \n",
       "3  https://api.spotify.com/v1/audio-analysis/7BKL...      244960   \n",
       "4  https://api.spotify.com/v1/audio-analysis/0DRK...      250533   \n",
       "\n",
       "  time_signature  \n",
       "0              4  \n",
       "1              4  \n",
       "2              4  \n",
       "3              4  \n",
       "4              4  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "italy_17_19_df = pd.concat([italy_17_19, italy_17_19_features], axis=1)\n",
    "italy_17_19_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Position</th>\n",
       "      <th>Track Name</th>\n",
       "      <th>Artist</th>\n",
       "      <th>Streams</th>\n",
       "      <th>date</th>\n",
       "      <th>region</th>\n",
       "      <th>spotify_id</th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>key</th>\n",
       "      <th>...</th>\n",
       "      <th>liveness</th>\n",
       "      <th>valence</th>\n",
       "      <th>tempo</th>\n",
       "      <th>type</th>\n",
       "      <th>id</th>\n",
       "      <th>uri</th>\n",
       "      <th>track_href</th>\n",
       "      <th>analysis_url</th>\n",
       "      <th>duration_ms</th>\n",
       "      <th>time_signature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30595</th>\n",
       "      <td>196</td>\n",
       "      <td>CORNFLAKES</td>\n",
       "      <td>COMETE</td>\n",
       "      <td>315265</td>\n",
       "      <td>2019-12-20--2019-12-27</td>\n",
       "      <td>it</td>\n",
       "      <td>1VLZV4idlXeZvpisIvoGMk</td>\n",
       "      <td>0.562</td>\n",
       "      <td>0.734</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0939</td>\n",
       "      <td>0.470</td>\n",
       "      <td>155.961</td>\n",
       "      <td>audio_features</td>\n",
       "      <td>1VLZV4idlXeZvpisIvoGMk</td>\n",
       "      <td>spotify:track:1VLZV4idlXeZvpisIvoGMk</td>\n",
       "      <td>https://api.spotify.com/v1/tracks/1VLZV4idlXeZ...</td>\n",
       "      <td>https://api.spotify.com/v1/audio-analysis/1VLZ...</td>\n",
       "      <td>193630</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30596</th>\n",
       "      <td>197</td>\n",
       "      <td>Hallelujah</td>\n",
       "      <td>Pentatonix</td>\n",
       "      <td>315209</td>\n",
       "      <td>2019-12-20--2019-12-27</td>\n",
       "      <td>it</td>\n",
       "      <td>550rQQCGkrTzvp4SfpOPzx</td>\n",
       "      <td>0.322</td>\n",
       "      <td>0.377</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>0.3380</td>\n",
       "      <td>0.366</td>\n",
       "      <td>118.669</td>\n",
       "      <td>audio_features</td>\n",
       "      <td>550rQQCGkrTzvp4SfpOPzx</td>\n",
       "      <td>spotify:track:550rQQCGkrTzvp4SfpOPzx</td>\n",
       "      <td>https://api.spotify.com/v1/tracks/550rQQCGkrTz...</td>\n",
       "      <td>https://api.spotify.com/v1/audio-analysis/550r...</td>\n",
       "      <td>268960</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30597</th>\n",
       "      <td>198</td>\n",
       "      <td>Step Into Christmas</td>\n",
       "      <td>Elton John</td>\n",
       "      <td>314886</td>\n",
       "      <td>2019-12-20--2019-12-27</td>\n",
       "      <td>it</td>\n",
       "      <td>6sBWmE23q6xQHlnEZ8jYPT</td>\n",
       "      <td>0.526</td>\n",
       "      <td>0.925</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.3050</td>\n",
       "      <td>0.819</td>\n",
       "      <td>140.308</td>\n",
       "      <td>audio_features</td>\n",
       "      <td>6sBWmE23q6xQHlnEZ8jYPT</td>\n",
       "      <td>spotify:track:6sBWmE23q6xQHlnEZ8jYPT</td>\n",
       "      <td>https://api.spotify.com/v1/tracks/6sBWmE23q6xQ...</td>\n",
       "      <td>https://api.spotify.com/v1/audio-analysis/6sBW...</td>\n",
       "      <td>272394</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30598</th>\n",
       "      <td>199</td>\n",
       "      <td>YOSHI (feat. tha Supreme, Fabri Fibra &amp; Capo P...</td>\n",
       "      <td>MACHETE</td>\n",
       "      <td>313194</td>\n",
       "      <td>2019-12-20--2019-12-27</td>\n",
       "      <td>it</td>\n",
       "      <td>0VU34EBWTAlHVIZE3aWlZA</td>\n",
       "      <td>0.790</td>\n",
       "      <td>0.714</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1140</td>\n",
       "      <td>0.773</td>\n",
       "      <td>83.593</td>\n",
       "      <td>audio_features</td>\n",
       "      <td>0VU34EBWTAlHVIZE3aWlZA</td>\n",
       "      <td>spotify:track:0VU34EBWTAlHVIZE3aWlZA</td>\n",
       "      <td>https://api.spotify.com/v1/tracks/0VU34EBWTAlH...</td>\n",
       "      <td>https://api.spotify.com/v1/audio-analysis/0VU3...</td>\n",
       "      <td>293591</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30599</th>\n",
       "      <td>200</td>\n",
       "      <td>Bandito</td>\n",
       "      <td>Enzo Dong</td>\n",
       "      <td>312497</td>\n",
       "      <td>2019-12-20--2019-12-27</td>\n",
       "      <td>it</td>\n",
       "      <td>0c6mLXIsBWgwAAqkagxY5w</td>\n",
       "      <td>0.793</td>\n",
       "      <td>0.762</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1280</td>\n",
       "      <td>0.447</td>\n",
       "      <td>132.097</td>\n",
       "      <td>audio_features</td>\n",
       "      <td>0c6mLXIsBWgwAAqkagxY5w</td>\n",
       "      <td>spotify:track:0c6mLXIsBWgwAAqkagxY5w</td>\n",
       "      <td>https://api.spotify.com/v1/tracks/0c6mLXIsBWgw...</td>\n",
       "      <td>https://api.spotify.com/v1/audio-analysis/0c6m...</td>\n",
       "      <td>195455</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Position                                         Track Name  \\\n",
       "30595       196                                         CORNFLAKES   \n",
       "30596       197                                         Hallelujah   \n",
       "30597       198                                Step Into Christmas   \n",
       "30598       199  YOSHI (feat. tha Supreme, Fabri Fibra & Capo P...   \n",
       "30599       200                                            Bandito   \n",
       "\n",
       "           Artist  Streams                    date region  \\\n",
       "30595      COMETE   315265  2019-12-20--2019-12-27     it   \n",
       "30596  Pentatonix   315209  2019-12-20--2019-12-27     it   \n",
       "30597  Elton John   314886  2019-12-20--2019-12-27     it   \n",
       "30598     MACHETE   313194  2019-12-20--2019-12-27     it   \n",
       "30599   Enzo Dong   312497  2019-12-20--2019-12-27     it   \n",
       "\n",
       "                   spotify_id  danceability  energy  key  ...  liveness  \\\n",
       "30595  1VLZV4idlXeZvpisIvoGMk         0.562   0.734    2  ...    0.0939   \n",
       "30596  550rQQCGkrTzvp4SfpOPzx         0.322   0.377    7  ...    0.3380   \n",
       "30597  6sBWmE23q6xQHlnEZ8jYPT         0.526   0.925    2  ...    0.3050   \n",
       "30598  0VU34EBWTAlHVIZE3aWlZA         0.790   0.714    2  ...    0.1140   \n",
       "30599  0c6mLXIsBWgwAAqkagxY5w         0.793   0.762    8  ...    0.1280   \n",
       "\n",
       "       valence    tempo            type                      id  \\\n",
       "30595    0.470  155.961  audio_features  1VLZV4idlXeZvpisIvoGMk   \n",
       "30596    0.366  118.669  audio_features  550rQQCGkrTzvp4SfpOPzx   \n",
       "30597    0.819  140.308  audio_features  6sBWmE23q6xQHlnEZ8jYPT   \n",
       "30598    0.773   83.593  audio_features  0VU34EBWTAlHVIZE3aWlZA   \n",
       "30599    0.447  132.097  audio_features  0c6mLXIsBWgwAAqkagxY5w   \n",
       "\n",
       "                                        uri  \\\n",
       "30595  spotify:track:1VLZV4idlXeZvpisIvoGMk   \n",
       "30596  spotify:track:550rQQCGkrTzvp4SfpOPzx   \n",
       "30597  spotify:track:6sBWmE23q6xQHlnEZ8jYPT   \n",
       "30598  spotify:track:0VU34EBWTAlHVIZE3aWlZA   \n",
       "30599  spotify:track:0c6mLXIsBWgwAAqkagxY5w   \n",
       "\n",
       "                                              track_href  \\\n",
       "30595  https://api.spotify.com/v1/tracks/1VLZV4idlXeZ...   \n",
       "30596  https://api.spotify.com/v1/tracks/550rQQCGkrTz...   \n",
       "30597  https://api.spotify.com/v1/tracks/6sBWmE23q6xQ...   \n",
       "30598  https://api.spotify.com/v1/tracks/0VU34EBWTAlH...   \n",
       "30599  https://api.spotify.com/v1/tracks/0c6mLXIsBWgw...   \n",
       "\n",
       "                                            analysis_url duration_ms  \\\n",
       "30595  https://api.spotify.com/v1/audio-analysis/1VLZ...      193630   \n",
       "30596  https://api.spotify.com/v1/audio-analysis/550r...      268960   \n",
       "30597  https://api.spotify.com/v1/audio-analysis/6sBW...      272394   \n",
       "30598  https://api.spotify.com/v1/audio-analysis/0VU3...      293591   \n",
       "30599  https://api.spotify.com/v1/audio-analysis/0c6m...      195455   \n",
       "\n",
       "      time_signature  \n",
       "30595              4  \n",
       "30596              4  \n",
       "30597              4  \n",
       "30598              4  \n",
       "30599              4  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "italy_17_19_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 30600 entries, 0 to 30599\n",
      "Data columns (total 25 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   Position          30600 non-null  int64  \n",
      " 1   Track Name        30595 non-null  object \n",
      " 2   Artist            30595 non-null  object \n",
      " 3   Streams           30600 non-null  int64  \n",
      " 4   date              30600 non-null  object \n",
      " 5   region            30600 non-null  object \n",
      " 6   spotify_id        30600 non-null  object \n",
      " 7   danceability      30600 non-null  float64\n",
      " 8   energy            30600 non-null  float64\n",
      " 9   key               30600 non-null  int64  \n",
      " 10  loudness          30600 non-null  float64\n",
      " 11  mode              30600 non-null  int64  \n",
      " 12  speechiness       30600 non-null  float64\n",
      " 13  acousticness      30600 non-null  float64\n",
      " 14  instrumentalness  30600 non-null  float64\n",
      " 15  liveness          30600 non-null  float64\n",
      " 16  valence           30600 non-null  float64\n",
      " 17  tempo             30600 non-null  float64\n",
      " 18  type              30600 non-null  object \n",
      " 19  id                30600 non-null  object \n",
      " 20  uri               30600 non-null  object \n",
      " 21  track_href        30600 non-null  object \n",
      " 22  analysis_url      30600 non-null  object \n",
      " 23  duration_ms       30600 non-null  int64  \n",
      " 24  time_signature    30600 non-null  int64  \n",
      "dtypes: float64(9), int64(6), object(10)\n",
      "memory usage: 5.8+ MB\n"
     ]
    }
   ],
   "source": [
    "italy_17_19_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "audio_features    30600\n",
       "Name: type, dtype: int64"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "italy_17_19_df['type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    https://api.spotify.com/v1/audio-analysis/7qiZ...\n",
       "1    https://api.spotify.com/v1/audio-analysis/5knu...\n",
       "2    https://api.spotify.com/v1/audio-analysis/6PCU...\n",
       "Name: analysis_url, dtype: object"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "italy_17_19_df['analysis_url'][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    https://api.spotify.com/v1/tracks/7qiZfU4dY1lW...\n",
       "1    https://api.spotify.com/v1/tracks/5knuzwU65gJK...\n",
       "2    https://api.spotify.com/v1/tracks/6PCUP3dWmTjc...\n",
       "Name: track_href, dtype: object"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "italy_17_19_df['track_href'][:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dropping extraneous columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Position</th>\n",
       "      <th>Track Name</th>\n",
       "      <th>Artist</th>\n",
       "      <th>Streams</th>\n",
       "      <th>date</th>\n",
       "      <th>region</th>\n",
       "      <th>spotify_id</th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>key</th>\n",
       "      <th>loudness</th>\n",
       "      <th>mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>valence</th>\n",
       "      <th>tempo</th>\n",
       "      <th>duration_ms</th>\n",
       "      <th>time_signature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Shape of You</td>\n",
       "      <td>Ed Sheeran</td>\n",
       "      <td>1051142</td>\n",
       "      <td>2017-01-06--2017-01-13</td>\n",
       "      <td>it</td>\n",
       "      <td>7qiZfU4dY1lWllzX7mPBI3</td>\n",
       "      <td>0.825</td>\n",
       "      <td>0.652</td>\n",
       "      <td>1</td>\n",
       "      <td>-3.183</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0802</td>\n",
       "      <td>0.5810</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0931</td>\n",
       "      <td>0.931</td>\n",
       "      <td>95.977</td>\n",
       "      <td>233713</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Rockabye (feat. Sean Paul &amp; Anne-Marie)</td>\n",
       "      <td>Clean Bandit</td>\n",
       "      <td>759626</td>\n",
       "      <td>2017-01-06--2017-01-13</td>\n",
       "      <td>it</td>\n",
       "      <td>5knuzwU65gJK7IF5yJsuaW</td>\n",
       "      <td>0.720</td>\n",
       "      <td>0.763</td>\n",
       "      <td>9</td>\n",
       "      <td>-4.068</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0523</td>\n",
       "      <td>0.4060</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1800</td>\n",
       "      <td>0.742</td>\n",
       "      <td>101.965</td>\n",
       "      <td>251088</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Castle on the Hill</td>\n",
       "      <td>Ed Sheeran</td>\n",
       "      <td>715171</td>\n",
       "      <td>2017-01-06--2017-01-13</td>\n",
       "      <td>it</td>\n",
       "      <td>6PCUP3dWmTjcTtXY02oFdT</td>\n",
       "      <td>0.461</td>\n",
       "      <td>0.834</td>\n",
       "      <td>2</td>\n",
       "      <td>-4.868</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0989</td>\n",
       "      <td>0.0232</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.1400</td>\n",
       "      <td>0.471</td>\n",
       "      <td>135.007</td>\n",
       "      <td>261154</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Closer</td>\n",
       "      <td>The Chainsmokers</td>\n",
       "      <td>504232</td>\n",
       "      <td>2017-01-06--2017-01-13</td>\n",
       "      <td>it</td>\n",
       "      <td>7BKLCZ1jbUBVqRi2FVlTVw</td>\n",
       "      <td>0.748</td>\n",
       "      <td>0.524</td>\n",
       "      <td>8</td>\n",
       "      <td>-5.599</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0338</td>\n",
       "      <td>0.4140</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1110</td>\n",
       "      <td>0.661</td>\n",
       "      <td>95.010</td>\n",
       "      <td>244960</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Assenzio (feat. Stash &amp; Levante)</td>\n",
       "      <td>J-AX</td>\n",
       "      <td>478975</td>\n",
       "      <td>2017-01-06--2017-01-13</td>\n",
       "      <td>it</td>\n",
       "      <td>0DRKnh0BloxJHyhXkfbiX8</td>\n",
       "      <td>0.554</td>\n",
       "      <td>0.640</td>\n",
       "      <td>0</td>\n",
       "      <td>-7.587</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1870</td>\n",
       "      <td>0.3410</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1080</td>\n",
       "      <td>0.641</td>\n",
       "      <td>160.009</td>\n",
       "      <td>250533</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Position                               Track Name            Artist  \\\n",
       "0         1                             Shape of You        Ed Sheeran   \n",
       "1         2  Rockabye (feat. Sean Paul & Anne-Marie)      Clean Bandit   \n",
       "2         3                       Castle on the Hill        Ed Sheeran   \n",
       "3         4                                   Closer  The Chainsmokers   \n",
       "4         5         Assenzio (feat. Stash & Levante)              J-AX   \n",
       "\n",
       "   Streams                    date region              spotify_id  \\\n",
       "0  1051142  2017-01-06--2017-01-13     it  7qiZfU4dY1lWllzX7mPBI3   \n",
       "1   759626  2017-01-06--2017-01-13     it  5knuzwU65gJK7IF5yJsuaW   \n",
       "2   715171  2017-01-06--2017-01-13     it  6PCUP3dWmTjcTtXY02oFdT   \n",
       "3   504232  2017-01-06--2017-01-13     it  7BKLCZ1jbUBVqRi2FVlTVw   \n",
       "4   478975  2017-01-06--2017-01-13     it  0DRKnh0BloxJHyhXkfbiX8   \n",
       "\n",
       "   danceability  energy  key  loudness  mode  speechiness  acousticness  \\\n",
       "0         0.825   0.652    1    -3.183     0       0.0802        0.5810   \n",
       "1         0.720   0.763    9    -4.068     0       0.0523        0.4060   \n",
       "2         0.461   0.834    2    -4.868     1       0.0989        0.0232   \n",
       "3         0.748   0.524    8    -5.599     1       0.0338        0.4140   \n",
       "4         0.554   0.640    0    -7.587     1       0.1870        0.3410   \n",
       "\n",
       "   instrumentalness  liveness  valence    tempo  duration_ms  time_signature  \n",
       "0          0.000000    0.0931    0.931   95.977       233713               4  \n",
       "1          0.000000    0.1800    0.742  101.965       251088               4  \n",
       "2          0.000011    0.1400    0.471  135.007       261154               4  \n",
       "3          0.000000    0.1110    0.661   95.010       244960               4  \n",
       "4          0.000000    0.1080    0.641  160.009       250533               4  "
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols_to_drop = ['type', 'id', 'uri', 'track_href', 'analysis_url']\n",
    "\n",
    "italy_17_19_df.drop(columns=cols_to_drop, inplace=True)\n",
    "\n",
    "italy_17_19_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converting `date` column to DateTime and setting as index for use in later time series analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2017-01-06--2017-01-13'"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "italy_17_19_df['date'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(italy_17_19_df['date'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2017-01-06'"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "italy_17_19_df['date'][0][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `date` column is currently a range for the week, so I need to convert this to just being a single date. I will make it the first day of the week, which is a Sunday because that's how Spotify charts set up their system. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for date column\n",
    "\n",
    "italy_17_19_df['date'] = italy_17_19_df['date'].apply(lambda x: x[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "italy_17_19_df['date'] = pd.to_datetime(italy_17_19_df['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Position</th>\n",
       "      <th>Track Name</th>\n",
       "      <th>Artist</th>\n",
       "      <th>Streams</th>\n",
       "      <th>region</th>\n",
       "      <th>spotify_id</th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>key</th>\n",
       "      <th>loudness</th>\n",
       "      <th>mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>valence</th>\n",
       "      <th>tempo</th>\n",
       "      <th>duration_ms</th>\n",
       "      <th>time_signature</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-12-20</th>\n",
       "      <td>196</td>\n",
       "      <td>CORNFLAKES</td>\n",
       "      <td>COMETE</td>\n",
       "      <td>315265</td>\n",
       "      <td>it</td>\n",
       "      <td>1VLZV4idlXeZvpisIvoGMk</td>\n",
       "      <td>0.562</td>\n",
       "      <td>0.734</td>\n",
       "      <td>2</td>\n",
       "      <td>-5.256</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0983</td>\n",
       "      <td>0.252</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0939</td>\n",
       "      <td>0.470</td>\n",
       "      <td>155.961</td>\n",
       "      <td>193630</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-20</th>\n",
       "      <td>197</td>\n",
       "      <td>Hallelujah</td>\n",
       "      <td>Pentatonix</td>\n",
       "      <td>315209</td>\n",
       "      <td>it</td>\n",
       "      <td>550rQQCGkrTzvp4SfpOPzx</td>\n",
       "      <td>0.322</td>\n",
       "      <td>0.377</td>\n",
       "      <td>7</td>\n",
       "      <td>-7.385</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0356</td>\n",
       "      <td>0.454</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.3380</td>\n",
       "      <td>0.366</td>\n",
       "      <td>118.669</td>\n",
       "      <td>268960</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-20</th>\n",
       "      <td>198</td>\n",
       "      <td>Step Into Christmas</td>\n",
       "      <td>Elton John</td>\n",
       "      <td>314886</td>\n",
       "      <td>it</td>\n",
       "      <td>6sBWmE23q6xQHlnEZ8jYPT</td>\n",
       "      <td>0.526</td>\n",
       "      <td>0.925</td>\n",
       "      <td>2</td>\n",
       "      <td>-5.584</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0363</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.3050</td>\n",
       "      <td>0.819</td>\n",
       "      <td>140.308</td>\n",
       "      <td>272394</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-20</th>\n",
       "      <td>199</td>\n",
       "      <td>YOSHI (feat. tha Supreme, Fabri Fibra &amp; Capo P...</td>\n",
       "      <td>MACHETE</td>\n",
       "      <td>313194</td>\n",
       "      <td>it</td>\n",
       "      <td>0VU34EBWTAlHVIZE3aWlZA</td>\n",
       "      <td>0.790</td>\n",
       "      <td>0.714</td>\n",
       "      <td>2</td>\n",
       "      <td>-5.119</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2660</td>\n",
       "      <td>0.232</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1140</td>\n",
       "      <td>0.773</td>\n",
       "      <td>83.593</td>\n",
       "      <td>293591</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-20</th>\n",
       "      <td>200</td>\n",
       "      <td>Bandito</td>\n",
       "      <td>Enzo Dong</td>\n",
       "      <td>312497</td>\n",
       "      <td>it</td>\n",
       "      <td>0c6mLXIsBWgwAAqkagxY5w</td>\n",
       "      <td>0.793</td>\n",
       "      <td>0.762</td>\n",
       "      <td>8</td>\n",
       "      <td>-5.252</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0591</td>\n",
       "      <td>0.333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1280</td>\n",
       "      <td>0.447</td>\n",
       "      <td>132.097</td>\n",
       "      <td>195455</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Position                                         Track Name  \\\n",
       "date                                                                      \n",
       "2019-12-20       196                                         CORNFLAKES   \n",
       "2019-12-20       197                                         Hallelujah   \n",
       "2019-12-20       198                                Step Into Christmas   \n",
       "2019-12-20       199  YOSHI (feat. tha Supreme, Fabri Fibra & Capo P...   \n",
       "2019-12-20       200                                            Bandito   \n",
       "\n",
       "                Artist  Streams region              spotify_id  danceability  \\\n",
       "date                                                                           \n",
       "2019-12-20      COMETE   315265     it  1VLZV4idlXeZvpisIvoGMk         0.562   \n",
       "2019-12-20  Pentatonix   315209     it  550rQQCGkrTzvp4SfpOPzx         0.322   \n",
       "2019-12-20  Elton John   314886     it  6sBWmE23q6xQHlnEZ8jYPT         0.526   \n",
       "2019-12-20     MACHETE   313194     it  0VU34EBWTAlHVIZE3aWlZA         0.790   \n",
       "2019-12-20   Enzo Dong   312497     it  0c6mLXIsBWgwAAqkagxY5w         0.793   \n",
       "\n",
       "            energy  key  loudness  mode  speechiness  acousticness  \\\n",
       "date                                                                 \n",
       "2019-12-20   0.734    2    -5.256     1       0.0983         0.252   \n",
       "2019-12-20   0.377    7    -7.385     0       0.0356         0.454   \n",
       "2019-12-20   0.925    2    -5.584     1       0.0363         0.102   \n",
       "2019-12-20   0.714    2    -5.119     1       0.2660         0.232   \n",
       "2019-12-20   0.762    8    -5.252     1       0.0591         0.333   \n",
       "\n",
       "            instrumentalness  liveness  valence    tempo  duration_ms  \\\n",
       "date                                                                    \n",
       "2019-12-20          0.000000    0.0939    0.470  155.961       193630   \n",
       "2019-12-20          0.000000    0.3380    0.366  118.669       268960   \n",
       "2019-12-20          0.000009    0.3050    0.819  140.308       272394   \n",
       "2019-12-20          0.000000    0.1140    0.773   83.593       293591   \n",
       "2019-12-20          0.000000    0.1280    0.447  132.097       195455   \n",
       "\n",
       "            time_signature  \n",
       "date                        \n",
       "2019-12-20               4  \n",
       "2019-12-20               4  \n",
       "2019-12-20               4  \n",
       "2019-12-20               4  \n",
       "2019-12-20               4  "
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "italy_17_19_df.set_index('date', inplace=True)\n",
    "\n",
    "italy_17_19_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 30600 entries, 2017-01-06 to 2019-12-20\n",
      "Data columns (total 19 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   Position          30600 non-null  int64  \n",
      " 1   Track Name        30595 non-null  object \n",
      " 2   Artist            30595 non-null  object \n",
      " 3   Streams           30600 non-null  int64  \n",
      " 4   region            30600 non-null  object \n",
      " 5   spotify_id        30600 non-null  object \n",
      " 6   danceability      30600 non-null  float64\n",
      " 7   energy            30600 non-null  float64\n",
      " 8   key               30600 non-null  int64  \n",
      " 9   loudness          30600 non-null  float64\n",
      " 10  mode              30600 non-null  int64  \n",
      " 11  speechiness       30600 non-null  float64\n",
      " 12  acousticness      30600 non-null  float64\n",
      " 13  instrumentalness  30600 non-null  float64\n",
      " 14  liveness          30600 non-null  float64\n",
      " 15  valence           30600 non-null  float64\n",
      " 16  tempo             30600 non-null  float64\n",
      " 17  duration_ms       30600 non-null  int64  \n",
      " 18  time_signature    30600 non-null  int64  \n",
      "dtypes: float64(9), int64(6), object(4)\n",
      "memory usage: 4.7+ MB\n"
     ]
    }
   ],
   "source": [
    "italy_17_19_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving clean DataFrame to csv and pickle for use in another notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "italy_17_19_df.to_csv('../data/it_17_19_feat.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "italy_17_19_df.to_pickle('../data/it_17_19_feat.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Spain's top songs 2017-2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ _All lists of top 200 / viral 50 song gathered from [Spotify Charts Regional](https://spotifycharts.com/regional/)_\n",
    "\n",
    "+ _Data are written as both a csv file and a SQLLite db._ \n",
    "\n",
    "+ _**Citation:** Code for how to scrape [Spotify Charts Regional](https://spotifycharts.com/regional/) is inspired by the excellent documentation for the [Unofficial Spotify Charts API](https://github.com/kelvingakuo/fycharts) called `fycharts`._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Italy Top 200 Weekly, 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : 11/02/2021 12:33:34 PM : The start date 2017-01-01 provided for top200Weekly is invalid. Wanna give one these a try? ['2017-01-06', '2017-01-13', '2017-01-20', '2017-01-27', '2017-02-03']\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter (1) to use the first suggestion, or (2) to quit and set yourself:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : 11/02/2021 12:33:43 PM : Extracting top 200 weekly for 2017-01-06--2017-01-13 - es\n",
      "INFO : 11/02/2021 12:33:45 PM : Extracting top 200 weekly for 2017-01-13--2017-01-20 - es\n",
      "INFO : 11/02/2021 12:33:45 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:33:45 PM : POSTing data to the endpoint https://mywebhookssite.com/post/\n",
      "INFO : 11/02/2021 12:33:45 PM : Appending data to the file ../data/spain_2017.csv...\n",
      "INFO : 11/02/2021 12:33:45 PM : Done appending to the file ../data/spain_2017.csv!!!\n",
      "Exception in thread Thread-19:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/urllib3/connection.py\", line 159, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/urllib3/util/connection.py\", line 61, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/socket.py\", line 918, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 8] nodename nor servname provided, or not known\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 670, in urlopen\n",
      "INFO : 11/02/2021 12:33:45 PM : Done appending to the table top_200_weekly!!!\n",
      "    httplib_response = self._make_request(\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 381, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 978, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/urllib3/connection.py\", line 309, in connect\n",
      "    conn = self._new_conn()\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/urllib3/connection.py\", line 171, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x7f9877c60d60>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/requests/adapters.py\", line 439, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 726, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/urllib3/util/retry.py\", line 446, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='mywebhookssite.com', port=443): Max retries exceeded with url: /post/ (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f9877c60d60>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/fycharts/SpotifyCharts.py\", line 88, in __post_to_endpoint_from_queue\n",
      "    postToRestEndpoint(df, url, what_data)\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/fycharts/write_to_outputs.py\", line 61, in postToRestEndpoint\n",
      "    raise(e)\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/fycharts/write_to_outputs.py\", line 58, in postToRestEndpoint\n",
      "    requests.post(url, json = dump)\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/requests/api.py\", line 119, in post\n",
      "    return request('post', url, data=data, json=json, **kwargs)\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/requests/api.py\", line 61, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/requests/sessions.py\", line 530, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/requests/sessions.py\", line 643, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/requests/adapters.py\", line 516, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='mywebhookssite.com', port=443): Max retries exceeded with url: /post/ (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f9877c60d60>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/threading.py\", line 932, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/fycharts/SpotifyCharts.py\", line 90, in __post_to_endpoint_from_queue\n",
      "    raise RuntimeError(e)\n",
      "RuntimeError: HTTPSConnectionPool(host='mywebhookssite.com', port=443): Max retries exceeded with url: /post/ (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f9877c60d60>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\n",
      "INFO : 11/02/2021 12:33:46 PM : Extracting top 200 weekly for 2017-01-20--2017-01-27 - es\n",
      "INFO : 11/02/2021 12:33:46 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:33:46 PM : Appending data to the file ../data/spain_2017.csv...\n",
      "INFO : 11/02/2021 12:33:46 PM : Done appending to the file ../data/spain_2017.csv!!!\n",
      "INFO : 11/02/2021 12:33:46 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:33:47 PM : Extracting top 200 weekly for 2017-01-27--2017-02-03 - es\n",
      "INFO : 11/02/2021 12:33:47 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:33:47 PM : Appending data to the file ../data/spain_2017.csv...\n",
      "INFO : 11/02/2021 12:33:47 PM : Done appending to the file ../data/spain_2017.csv!!!\n",
      "INFO : 11/02/2021 12:33:47 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:33:48 PM : Extracting top 200 weekly for 2017-02-03--2017-02-10 - es\n",
      "INFO : 11/02/2021 12:33:48 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:33:48 PM : Appending data to the file ../data/spain_2017.csv...\n",
      "INFO : 11/02/2021 12:33:48 PM : Done appending to the file ../data/spain_2017.csv!!!\n",
      "INFO : 11/02/2021 12:33:48 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:33:49 PM : Extracting top 200 weekly for 2017-02-10--2017-02-17 - es\n",
      "INFO : 11/02/2021 12:33:49 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:33:49 PM : Appending data to the file ../data/spain_2017.csv...\n",
      "INFO : 11/02/2021 12:33:49 PM : Done appending to the file ../data/spain_2017.csv!!!\n",
      "INFO : 11/02/2021 12:33:49 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:33:50 PM : Extracting top 200 weekly for 2017-02-17--2017-02-24 - es\n",
      "INFO : 11/02/2021 12:33:50 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:33:50 PM : Appending data to the file ../data/spain_2017.csv...\n",
      "INFO : 11/02/2021 12:33:50 PM : Done appending to the file ../data/spain_2017.csv!!!\n",
      "INFO : 11/02/2021 12:33:50 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:33:51 PM : Extracting top 200 weekly for 2017-02-24--2017-03-03 - es\n",
      "INFO : 11/02/2021 12:33:51 PM : Appending data to the file ../data/spain_2017.csv...\n",
      "INFO : 11/02/2021 12:33:51 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:33:51 PM : Done appending to the file ../data/spain_2017.csv!!!\n",
      "INFO : 11/02/2021 12:33:51 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:33:52 PM : Extracting top 200 weekly for 2017-03-03--2017-03-10 - es\n",
      "INFO : 11/02/2021 12:33:52 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:33:52 PM : Appending data to the file ../data/spain_2017.csv...\n",
      "INFO : 11/02/2021 12:33:52 PM : Done appending to the file ../data/spain_2017.csv!!!\n",
      "INFO : 11/02/2021 12:33:52 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:33:53 PM : Extracting top 200 weekly for 2017-03-10--2017-03-17 - es\n",
      "INFO : 11/02/2021 12:33:53 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:33:53 PM : Appending data to the file ../data/spain_2017.csv...\n",
      "INFO : 11/02/2021 12:33:53 PM : Done appending to the file ../data/spain_2017.csv!!!\n",
      "INFO : 11/02/2021 12:33:53 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:33:54 PM : Extracting top 200 weekly for 2017-03-17--2017-03-24 - es\n",
      "INFO : 11/02/2021 12:33:54 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:33:54 PM : Appending data to the file ../data/spain_2017.csv...\n",
      "INFO : 11/02/2021 12:33:54 PM : Done appending to the file ../data/spain_2017.csv!!!\n",
      "INFO : 11/02/2021 12:33:54 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:33:55 PM : Extracting top 200 weekly for 2017-03-24--2017-03-31 - es\n",
      "INFO : 11/02/2021 12:33:55 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:33:55 PM : Appending data to the file ../data/spain_2017.csv...\n",
      "INFO : 11/02/2021 12:33:55 PM : Done appending to the file ../data/spain_2017.csv!!!\n",
      "INFO : 11/02/2021 12:33:55 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:33:55 PM : Extracting top 200 weekly for 2017-03-31--2017-04-07 - es\n",
      "INFO : 11/02/2021 12:33:55 PM : Appending data to the file ../data/spain_2017.csv...\n",
      "INFO : 11/02/2021 12:33:55 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:33:55 PM : Done appending to the file ../data/spain_2017.csv!!!\n",
      "INFO : 11/02/2021 12:33:55 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:33:56 PM : Extracting top 200 weekly for 2017-04-07--2017-04-14 - es\n",
      "INFO : 11/02/2021 12:33:56 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:33:56 PM : Appending data to the file ../data/spain_2017.csv...\n",
      "INFO : 11/02/2021 12:33:56 PM : Done appending to the file ../data/spain_2017.csv!!!\n",
      "INFO : 11/02/2021 12:33:56 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:33:57 PM : Extracting top 200 weekly for 2017-04-14--2017-04-21 - es\n",
      "INFO : 11/02/2021 12:33:57 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:33:57 PM : Appending data to the file ../data/spain_2017.csv...\n",
      "INFO : 11/02/2021 12:33:57 PM : Done appending to the file ../data/spain_2017.csv!!!\n",
      "INFO : 11/02/2021 12:33:57 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:33:58 PM : Extracting top 200 weekly for 2017-04-21--2017-04-28 - es\n",
      "INFO : 11/02/2021 12:33:58 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:33:58 PM : Appending data to the file ../data/spain_2017.csv...\n",
      "INFO : 11/02/2021 12:33:58 PM : Done appending to the file ../data/spain_2017.csv!!!\n",
      "INFO : 11/02/2021 12:33:58 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:33:59 PM : Extracting top 200 weekly for 2017-04-28--2017-05-05 - es\n",
      "INFO : 11/02/2021 12:33:59 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:33:59 PM : Appending data to the file ../data/spain_2017.csv...\n",
      "INFO : 11/02/2021 12:33:59 PM : Done appending to the file ../data/spain_2017.csv!!!\n",
      "INFO : 11/02/2021 12:33:59 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:34:00 PM : Extracting top 200 weekly for 2017-05-05--2017-05-12 - es\n",
      "INFO : 11/02/2021 12:34:00 PM : Appending data to the file ../data/spain_2017.csv...\n",
      "INFO : 11/02/2021 12:34:00 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:34:00 PM : Done appending to the file ../data/spain_2017.csv!!!\n",
      "INFO : 11/02/2021 12:34:00 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:34:01 PM : Extracting top 200 weekly for 2017-05-12--2017-05-19 - es\n",
      "INFO : 11/02/2021 12:34:01 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:34:01 PM : Appending data to the file ../data/spain_2017.csv...\n",
      "INFO : 11/02/2021 12:34:01 PM : Done appending to the file ../data/spain_2017.csv!!!\n",
      "INFO : 11/02/2021 12:34:01 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:34:02 PM : Extracting top 200 weekly for 2017-05-19--2017-05-26 - es\n",
      "INFO : 11/02/2021 12:34:02 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:34:02 PM : Appending data to the file ../data/spain_2017.csv...\n",
      "INFO : 11/02/2021 12:34:02 PM : Done appending to the file ../data/spain_2017.csv!!!\n",
      "INFO : 11/02/2021 12:34:02 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:34:03 PM : Extracting top 200 weekly for 2017-05-26--2017-06-02 - es\n",
      "INFO : 11/02/2021 12:34:03 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:34:03 PM : Appending data to the file ../data/spain_2017.csv...\n",
      "INFO : 11/02/2021 12:34:03 PM : Done appending to the file ../data/spain_2017.csv!!!\n",
      "INFO : 11/02/2021 12:34:03 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:34:04 PM : Extracting top 200 weekly for 2017-06-02--2017-06-09 - es\n",
      "INFO : 11/02/2021 12:34:04 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:34:04 PM : Appending data to the file ../data/spain_2017.csv...\n",
      "INFO : 11/02/2021 12:34:04 PM : Done appending to the file ../data/spain_2017.csv!!!\n",
      "INFO : 11/02/2021 12:34:04 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:34:05 PM : Extracting top 200 weekly for 2017-06-09--2017-06-16 - es\n",
      "INFO : 11/02/2021 12:34:05 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:34:05 PM : Appending data to the file ../data/spain_2017.csv...\n",
      "INFO : 11/02/2021 12:34:05 PM : Done appending to the file ../data/spain_2017.csv!!!\n",
      "INFO : 11/02/2021 12:34:05 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:34:06 PM : Extracting top 200 weekly for 2017-06-16--2017-06-23 - es\n",
      "INFO : 11/02/2021 12:34:06 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:34:06 PM : Appending data to the file ../data/spain_2017.csv...\n",
      "INFO : 11/02/2021 12:34:06 PM : Done appending to the file ../data/spain_2017.csv!!!\n",
      "INFO : 11/02/2021 12:34:06 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:34:07 PM : Extracting top 200 weekly for 2017-06-23--2017-06-30 - es\n",
      "INFO : 11/02/2021 12:34:07 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:34:07 PM : Appending data to the file ../data/spain_2017.csv...\n",
      "INFO : 11/02/2021 12:34:07 PM : Done appending to the file ../data/spain_2017.csv!!!\n",
      "INFO : 11/02/2021 12:34:07 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:34:08 PM : Extracting top 200 weekly for 2017-06-30--2017-07-07 - es\n",
      "INFO : 11/02/2021 12:34:08 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:34:08 PM : Appending data to the file ../data/spain_2017.csv...\n",
      "INFO : 11/02/2021 12:34:08 PM : Done appending to the file ../data/spain_2017.csv!!!\n",
      "INFO : 11/02/2021 12:34:08 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:34:09 PM : Extracting top 200 weekly for 2017-07-07--2017-07-14 - es\n",
      "INFO : 11/02/2021 12:34:09 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:34:09 PM : Appending data to the file ../data/spain_2017.csv...\n",
      "INFO : 11/02/2021 12:34:09 PM : Done appending to the file ../data/spain_2017.csv!!!\n",
      "INFO : 11/02/2021 12:34:09 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:34:10 PM : Extracting top 200 weekly for 2017-07-14--2017-07-21 - es\n",
      "INFO : 11/02/2021 12:34:10 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:34:10 PM : Appending data to the file ../data/spain_2017.csv...\n",
      "INFO : 11/02/2021 12:34:10 PM : Done appending to the file ../data/spain_2017.csv!!!\n",
      "INFO : 11/02/2021 12:34:10 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:34:11 PM : Extracting top 200 weekly for 2017-07-21--2017-07-28 - es\n",
      "INFO : 11/02/2021 12:34:11 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:34:11 PM : Appending data to the file ../data/spain_2017.csv...\n",
      "INFO : 11/02/2021 12:34:11 PM : Done appending to the file ../data/spain_2017.csv!!!\n",
      "INFO : 11/02/2021 12:34:11 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:34:12 PM : Extracting top 200 weekly for 2017-07-28--2017-08-04 - es\n",
      "INFO : 11/02/2021 12:34:12 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:34:12 PM : Appending data to the file ../data/spain_2017.csv...\n",
      "INFO : 11/02/2021 12:34:12 PM : Done appending to the file ../data/spain_2017.csv!!!\n",
      "INFO : 11/02/2021 12:34:12 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:34:13 PM : Extracting top 200 weekly for 2017-08-04--2017-08-11 - es\n",
      "INFO : 11/02/2021 12:34:13 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:34:13 PM : Appending data to the file ../data/spain_2017.csv...\n",
      "INFO : 11/02/2021 12:34:13 PM : Done appending to the file ../data/spain_2017.csv!!!\n",
      "INFO : 11/02/2021 12:34:13 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:34:14 PM : Extracting top 200 weekly for 2017-08-11--2017-08-18 - es\n",
      "INFO : 11/02/2021 12:34:14 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:34:14 PM : Appending data to the file ../data/spain_2017.csv...\n",
      "INFO : 11/02/2021 12:34:14 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:34:14 PM : Done appending to the file ../data/spain_2017.csv!!!\n",
      "INFO : 11/02/2021 12:34:15 PM : Extracting top 200 weekly for 2017-08-18--2017-08-25 - es\n",
      "INFO : 11/02/2021 12:34:15 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:34:15 PM : Appending data to the file ../data/spain_2017.csv...\n",
      "INFO : 11/02/2021 12:34:15 PM : Done appending to the file ../data/spain_2017.csv!!!\n",
      "INFO : 11/02/2021 12:34:15 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:34:16 PM : Extracting top 200 weekly for 2017-08-25--2017-09-01 - es\n",
      "INFO : 11/02/2021 12:34:16 PM : Appending data to the file ../data/spain_2017.csv...\n",
      "INFO : 11/02/2021 12:34:16 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:34:16 PM : Done appending to the file ../data/spain_2017.csv!!!\n",
      "INFO : 11/02/2021 12:34:16 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:34:17 PM : Extracting top 200 weekly for 2017-09-01--2017-09-08 - es\n",
      "INFO : 11/02/2021 12:34:17 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:34:17 PM : Appending data to the file ../data/spain_2017.csv...\n",
      "INFO : 11/02/2021 12:34:17 PM : Done appending to the file ../data/spain_2017.csv!!!\n",
      "INFO : 11/02/2021 12:34:17 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:34:18 PM : Extracting top 200 weekly for 2017-09-08--2017-09-15 - es\n",
      "INFO : 11/02/2021 12:34:18 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:34:18 PM : Appending data to the file ../data/spain_2017.csv...\n",
      "INFO : 11/02/2021 12:34:18 PM : Done appending to the file ../data/spain_2017.csv!!!\n",
      "INFO : 11/02/2021 12:34:18 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:34:19 PM : Extracting top 200 weekly for 2017-09-15--2017-09-22 - es\n",
      "INFO : 11/02/2021 12:34:19 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:34:19 PM : Appending data to the file ../data/spain_2017.csv...\n",
      "INFO : 11/02/2021 12:34:19 PM : Done appending to the file ../data/spain_2017.csv!!!\n",
      "INFO : 11/02/2021 12:34:19 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:34:20 PM : Extracting top 200 weekly for 2017-09-22--2017-09-29 - es\n",
      "INFO : 11/02/2021 12:34:20 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:34:20 PM : Appending data to the file ../data/spain_2017.csv...\n",
      "INFO : 11/02/2021 12:34:20 PM : Done appending to the file ../data/spain_2017.csv!!!\n",
      "INFO : 11/02/2021 12:34:20 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:34:21 PM : Extracting top 200 weekly for 2017-09-29--2017-10-06 - es\n",
      "INFO : 11/02/2021 12:34:21 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:34:21 PM : Appending data to the file ../data/spain_2017.csv...\n",
      "INFO : 11/02/2021 12:34:21 PM : Done appending to the file ../data/spain_2017.csv!!!\n",
      "INFO : 11/02/2021 12:34:21 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:34:22 PM : Extracting top 200 weekly for 2017-10-06--2017-10-13 - es\n",
      "INFO : 11/02/2021 12:34:22 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:34:22 PM : Appending data to the file ../data/spain_2017.csv...\n",
      "INFO : 11/02/2021 12:34:22 PM : Done appending to the file ../data/spain_2017.csv!!!\n",
      "INFO : 11/02/2021 12:34:22 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:34:23 PM : Extracting top 200 weekly for 2017-10-13--2017-10-20 - es\n",
      "INFO : 11/02/2021 12:34:23 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:34:23 PM : Appending data to the file ../data/spain_2017.csv...\n",
      "INFO : 11/02/2021 12:34:23 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:34:23 PM : Done appending to the file ../data/spain_2017.csv!!!\n",
      "INFO : 11/02/2021 12:34:24 PM : Extracting top 200 weekly for 2017-10-20--2017-10-27 - es\n",
      "INFO : 11/02/2021 12:34:24 PM : Appending data to the file ../data/spain_2017.csv...\n",
      "INFO : 11/02/2021 12:34:24 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:34:24 PM : Done appending to the file ../data/spain_2017.csv!!!\n",
      "INFO : 11/02/2021 12:34:24 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:34:25 PM : Extracting top 200 weekly for 2017-10-27--2017-11-03 - es\n",
      "INFO : 11/02/2021 12:34:25 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:34:25 PM : Appending data to the file ../data/spain_2017.csv...\n",
      "INFO : 11/02/2021 12:34:25 PM : Done appending to the file ../data/spain_2017.csv!!!\n",
      "INFO : 11/02/2021 12:34:25 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:34:27 PM : Extracting top 200 weekly for 2017-11-03--2017-11-10 - es\n",
      "INFO : 11/02/2021 12:34:27 PM : Appending data to the file ../data/spain_2017.csv...\n",
      "INFO : 11/02/2021 12:34:27 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:34:27 PM : Done appending to the file ../data/spain_2017.csv!!!\n",
      "INFO : 11/02/2021 12:34:27 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:34:28 PM : Extracting top 200 weekly for 2017-11-10--2017-11-17 - es\n",
      "INFO : 11/02/2021 12:34:28 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:34:28 PM : Appending data to the file ../data/spain_2017.csv...\n",
      "INFO : 11/02/2021 12:34:28 PM : Done appending to the file ../data/spain_2017.csv!!!\n",
      "INFO : 11/02/2021 12:34:28 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:34:29 PM : Extracting top 200 weekly for 2017-11-17--2017-11-24 - es\n",
      "INFO : 11/02/2021 12:34:29 PM : Appending data to the file ../data/spain_2017.csv...\n",
      "INFO : 11/02/2021 12:34:29 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:34:29 PM : Done appending to the file ../data/spain_2017.csv!!!\n",
      "INFO : 11/02/2021 12:34:29 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:34:30 PM : Extracting top 200 weekly for 2017-11-24--2017-12-01 - es\n",
      "INFO : 11/02/2021 12:34:30 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:34:30 PM : Appending data to the file ../data/spain_2017.csv...\n",
      "INFO : 11/02/2021 12:34:30 PM : Done appending to the file ../data/spain_2017.csv!!!\n",
      "INFO : 11/02/2021 12:34:30 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:34:31 PM : Extracting top 200 weekly for 2017-12-01--2017-12-08 - es\n",
      "INFO : 11/02/2021 12:34:31 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:34:31 PM : Appending data to the file ../data/spain_2017.csv...\n",
      "INFO : 11/02/2021 12:34:31 PM : Done appending to the file ../data/spain_2017.csv!!!\n",
      "INFO : 11/02/2021 12:34:31 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:34:32 PM : Extracting top 200 weekly for 2017-12-08--2017-12-15 - es\n",
      "INFO : 11/02/2021 12:34:32 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:34:32 PM : Appending data to the file ../data/spain_2017.csv...\n",
      "INFO : 11/02/2021 12:34:32 PM : Done appending to the file ../data/spain_2017.csv!!!\n",
      "INFO : 11/02/2021 12:34:32 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:34:33 PM : Extracting top 200 weekly for 2017-12-15--2017-12-22 - es\n",
      "INFO : 11/02/2021 12:34:33 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:34:33 PM : Appending data to the file ../data/spain_2017.csv...\n",
      "INFO : 11/02/2021 12:34:33 PM : Done appending to the file ../data/spain_2017.csv!!!\n",
      "INFO : 11/02/2021 12:34:33 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:34:34 PM : Extracting top 200 weekly for 2017-12-22--2017-12-29 - es\n",
      "INFO : 11/02/2021 12:34:34 PM : Appending data to the file ../data/spain_2017.csv...\n",
      "INFO : 11/02/2021 12:34:34 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:34:34 PM : Done appending to the file ../data/spain_2017.csv!!!\n",
      "INFO : 11/02/2021 12:34:34 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:34:35 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:34:35 PM : Appending data to the file ../data/spain_2017.csv...\n",
      "INFO : 11/02/2021 12:34:35 PM : Done appending to the file ../data/spain_2017.csv!!!\n",
      "INFO : 11/02/2021 12:34:35 PM : Done appending to the table top_200_weekly!!!\n"
     ]
    }
   ],
   "source": [
    "api = SpotifyCharts()\n",
    "connector = sqlalchemy.create_engine(\"sqlite:///../data/spain_2017.db\", echo=False)\n",
    "api.top200Weekly(output_file = \"../data/spain_2017.csv\", output_db = connector, webhook = [\"https://mywebhookssite.com/post/\"], start = \"2017-01-01\", end = \"2017-12-31\", region = \"es\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Position</th>\n",
       "      <th>Track Name</th>\n",
       "      <th>Artist</th>\n",
       "      <th>Streams</th>\n",
       "      <th>date</th>\n",
       "      <th>region</th>\n",
       "      <th>spotify_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Chantaje (feat. Maluma)</td>\n",
       "      <td>Shakira</td>\n",
       "      <td>1423583</td>\n",
       "      <td>2017-01-06--2017-01-13</td>\n",
       "      <td>es</td>\n",
       "      <td>6mICuAdrwEjh6Y6lroV2Kg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Reggaetón Lento (Bailemos)</td>\n",
       "      <td>CNCO</td>\n",
       "      <td>1339834</td>\n",
       "      <td>2017-01-06--2017-01-13</td>\n",
       "      <td>es</td>\n",
       "      <td>3AEZUABDXNtecAOSC1qTfo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Shape of You</td>\n",
       "      <td>Ed Sheeran</td>\n",
       "      <td>1215829</td>\n",
       "      <td>2017-01-06--2017-01-13</td>\n",
       "      <td>es</td>\n",
       "      <td>7qiZfU4dY1lWllzX7mPBI3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Rockabye (feat. Sean Paul &amp; Anne-Marie)</td>\n",
       "      <td>Clean Bandit</td>\n",
       "      <td>1148813</td>\n",
       "      <td>2017-01-06--2017-01-13</td>\n",
       "      <td>es</td>\n",
       "      <td>5knuzwU65gJK7IF5yJsuaW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Safari</td>\n",
       "      <td>J Balvin</td>\n",
       "      <td>1044868</td>\n",
       "      <td>2017-01-06--2017-01-13</td>\n",
       "      <td>es</td>\n",
       "      <td>6rQSrBHf7HlZjtcMZ4S4bO</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Position                               Track Name        Artist  Streams  \\\n",
       "0         1                  Chantaje (feat. Maluma)       Shakira  1423583   \n",
       "1         2               Reggaetón Lento (Bailemos)          CNCO  1339834   \n",
       "2         3                             Shape of You    Ed Sheeran  1215829   \n",
       "3         4  Rockabye (feat. Sean Paul & Anne-Marie)  Clean Bandit  1148813   \n",
       "4         5                                   Safari      J Balvin  1044868   \n",
       "\n",
       "                     date region              spotify_id  \n",
       "0  2017-01-06--2017-01-13     es  6mICuAdrwEjh6Y6lroV2Kg  \n",
       "1  2017-01-06--2017-01-13     es  3AEZUABDXNtecAOSC1qTfo  \n",
       "2  2017-01-06--2017-01-13     es  7qiZfU4dY1lWllzX7mPBI3  \n",
       "3  2017-01-06--2017-01-13     es  5knuzwU65gJK7IF5yJsuaW  \n",
       "4  2017-01-06--2017-01-13     es  6rQSrBHf7HlZjtcMZ4S4bO  "
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reading in csv from output file  \n",
    "spain_2017 = pd.read_csv('../data/spain_2017.csv')\n",
    "spain_2017.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Position</th>\n",
       "      <th>Track Name</th>\n",
       "      <th>Artist</th>\n",
       "      <th>Streams</th>\n",
       "      <th>date</th>\n",
       "      <th>region</th>\n",
       "      <th>spotify_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10195</th>\n",
       "      <td>196</td>\n",
       "      <td>Es Mejor</td>\n",
       "      <td>Rels B</td>\n",
       "      <td>140242</td>\n",
       "      <td>2017-12-22--2017-12-29</td>\n",
       "      <td>es</td>\n",
       "      <td>2nmkKucRDKr5jIBEvg6X0z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10196</th>\n",
       "      <td>197</td>\n",
       "      <td>Boca de hule</td>\n",
       "      <td>Pablo Alborán</td>\n",
       "      <td>139891</td>\n",
       "      <td>2017-12-22--2017-12-29</td>\n",
       "      <td>es</td>\n",
       "      <td>5Wr3Ebe60UsEY6CCqYHeRj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10197</th>\n",
       "      <td>198</td>\n",
       "      <td>XO Tour Llif3</td>\n",
       "      <td>Lil Uzi Vert</td>\n",
       "      <td>139242</td>\n",
       "      <td>2017-12-22--2017-12-29</td>\n",
       "      <td>es</td>\n",
       "      <td>7GX5flRQZVHRAGd6B4TmDO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10198</th>\n",
       "      <td>199</td>\n",
       "      <td>What About Us</td>\n",
       "      <td>P!nk</td>\n",
       "      <td>137680</td>\n",
       "      <td>2017-12-22--2017-12-29</td>\n",
       "      <td>es</td>\n",
       "      <td>0Qh38w01QRXK6KHIv0e3hb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10199</th>\n",
       "      <td>200</td>\n",
       "      <td>Hábito de ti</td>\n",
       "      <td>Vanesa Martín</td>\n",
       "      <td>137456</td>\n",
       "      <td>2017-12-22--2017-12-29</td>\n",
       "      <td>es</td>\n",
       "      <td>0Th3kPhraGzyXllj1ZteMz</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Position     Track Name         Artist  Streams  \\\n",
       "10195       196       Es Mejor         Rels B   140242   \n",
       "10196       197   Boca de hule  Pablo Alborán   139891   \n",
       "10197       198  XO Tour Llif3   Lil Uzi Vert   139242   \n",
       "10198       199  What About Us           P!nk   137680   \n",
       "10199       200   Hábito de ti  Vanesa Martín   137456   \n",
       "\n",
       "                         date region              spotify_id  \n",
       "10195  2017-12-22--2017-12-29     es  2nmkKucRDKr5jIBEvg6X0z  \n",
       "10196  2017-12-22--2017-12-29     es  5Wr3Ebe60UsEY6CCqYHeRj  \n",
       "10197  2017-12-22--2017-12-29     es  7GX5flRQZVHRAGd6B4TmDO  \n",
       "10198  2017-12-22--2017-12-29     es  0Qh38w01QRXK6KHIv0e3hb  \n",
       "10199  2017-12-22--2017-12-29     es  0Th3kPhraGzyXllj1ZteMz  "
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spain_2017.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10200 entries, 0 to 10199\n",
      "Data columns (total 7 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   Position    10200 non-null  int64 \n",
      " 1   Track Name  10195 non-null  object\n",
      " 2   Artist      10195 non-null  object\n",
      " 3   Streams     10200 non-null  int64 \n",
      " 4   date        10200 non-null  object\n",
      " 5   region      10200 non-null  object\n",
      " 6   spotify_id  10200 non-null  object\n",
      "dtypes: int64(2), object(5)\n",
      "memory usage: 557.9+ KB\n"
     ]
    }
   ],
   "source": [
    "spain_2017.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spain Top 200 Weekly, 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : 11/02/2021 12:42:41 PM : The start date 2018-01-01 provided for top200Weekly is invalid. Wanna give one these a try? ['2018-01-05', '2018-01-12', '2018-01-19', '2018-01-26', '2018-02-02']\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter (1) to use the first suggestion, or (2) to quit and set yourself:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : 11/02/2021 12:42:43 PM : Extracting top 200 weekly for 2018-01-05--2018-01-12 - es\n",
      "INFO : 11/02/2021 12:42:44 PM : Extracting top 200 weekly for 2018-01-12--2018-01-19 - es\n",
      "INFO : 11/02/2021 12:42:44 PM : POSTing data to the endpoint https://mywebhookssite.com/post/\n",
      "INFO : 11/02/2021 12:42:44 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:42:44 PM : Appending data to the file ../data/spain_2018.csv...\n",
      "INFO : 11/02/2021 12:42:44 PM : Done appending to the file ../data/spain_2018.csv!!!\n",
      "Exception in thread Thread-22:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/urllib3/connection.py\", line 159, in _new_conn\n",
      "INFO : 11/02/2021 12:42:44 PM : Done appending to the table top_200_weekly!!!\n",
      "    conn = connection.create_connection(\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/urllib3/util/connection.py\", line 61, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/socket.py\", line 918, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 8] nodename nor servname provided, or not known\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 670, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 381, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 978, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/urllib3/connection.py\", line 309, in connect\n",
      "    conn = self._new_conn()\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/urllib3/connection.py\", line 171, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x7f986abfc580>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/requests/adapters.py\", line 439, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 726, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/urllib3/util/retry.py\", line 446, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='mywebhookssite.com', port=443): Max retries exceeded with url: /post/ (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f986abfc580>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/fycharts/SpotifyCharts.py\", line 88, in __post_to_endpoint_from_queue\n",
      "    postToRestEndpoint(df, url, what_data)\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/fycharts/write_to_outputs.py\", line 61, in postToRestEndpoint\n",
      "    raise(e)\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/fycharts/write_to_outputs.py\", line 58, in postToRestEndpoint\n",
      "    requests.post(url, json = dump)\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/requests/api.py\", line 119, in post\n",
      "    return request('post', url, data=data, json=json, **kwargs)\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/requests/api.py\", line 61, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/requests/sessions.py\", line 530, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/requests/sessions.py\", line 643, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/requests/adapters.py\", line 516, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='mywebhookssite.com', port=443): Max retries exceeded with url: /post/ (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f986abfc580>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/threading.py\", line 932, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/fycharts/SpotifyCharts.py\", line 90, in __post_to_endpoint_from_queue\n",
      "    raise RuntimeError(e)\n",
      "RuntimeError: HTTPSConnectionPool(host='mywebhookssite.com', port=443): Max retries exceeded with url: /post/ (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f986abfc580>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\n",
      "INFO : 11/02/2021 12:42:46 PM : Extracting top 200 weekly for 2018-01-19--2018-01-26 - es\n",
      "INFO : 11/02/2021 12:42:46 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:42:46 PM : Appending data to the file ../data/spain_2018.csv...\n",
      "INFO : 11/02/2021 12:42:46 PM : Done appending to the file ../data/spain_2018.csv!!!\n",
      "INFO : 11/02/2021 12:42:46 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:42:47 PM : Extracting top 200 weekly for 2018-01-26--2018-02-02 - es\n",
      "INFO : 11/02/2021 12:42:47 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:42:47 PM : Appending data to the file ../data/spain_2018.csv...\n",
      "INFO : 11/02/2021 12:42:47 PM : Done appending to the file ../data/spain_2018.csv!!!\n",
      "INFO : 11/02/2021 12:42:47 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:42:48 PM : Extracting top 200 weekly for 2018-02-02--2018-02-09 - es\n",
      "INFO : 11/02/2021 12:42:48 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:42:48 PM : Appending data to the file ../data/spain_2018.csv...\n",
      "INFO : 11/02/2021 12:42:48 PM : Done appending to the file ../data/spain_2018.csv!!!\n",
      "INFO : 11/02/2021 12:42:48 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:42:49 PM : Extracting top 200 weekly for 2018-02-09--2018-02-16 - es\n",
      "INFO : 11/02/2021 12:42:49 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:42:49 PM : Appending data to the file ../data/spain_2018.csv...\n",
      "INFO : 11/02/2021 12:42:49 PM : Done appending to the file ../data/spain_2018.csv!!!\n",
      "INFO : 11/02/2021 12:42:49 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:42:50 PM : Extracting top 200 weekly for 2018-02-16--2018-02-23 - es\n",
      "INFO : 11/02/2021 12:42:50 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:42:50 PM : Appending data to the file ../data/spain_2018.csv...\n",
      "INFO : 11/02/2021 12:42:50 PM : Done appending to the file ../data/spain_2018.csv!!!\n",
      "INFO : 11/02/2021 12:42:50 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:42:51 PM : Extracting top 200 weekly for 2018-02-23--2018-03-02 - es\n",
      "INFO : 11/02/2021 12:42:51 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:42:51 PM : Appending data to the file ../data/spain_2018.csv...\n",
      "INFO : 11/02/2021 12:42:51 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:42:51 PM : Done appending to the file ../data/spain_2018.csv!!!\n",
      "INFO : 11/02/2021 12:42:52 PM : Extracting top 200 weekly for 2018-03-02--2018-03-09 - es\n",
      "INFO : 11/02/2021 12:42:52 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:42:52 PM : Appending data to the file ../data/spain_2018.csv...\n",
      "INFO : 11/02/2021 12:42:52 PM : Done appending to the file ../data/spain_2018.csv!!!\n",
      "INFO : 11/02/2021 12:42:52 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:42:53 PM : Extracting top 200 weekly for 2018-03-09--2018-03-16 - es\n",
      "INFO : 11/02/2021 12:42:53 PM : Appending data to the file ../data/spain_2018.csv...\n",
      "INFO : 11/02/2021 12:42:53 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:42:53 PM : Done appending to the file ../data/spain_2018.csv!!!\n",
      "INFO : 11/02/2021 12:42:53 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:42:54 PM : Extracting top 200 weekly for 2018-03-16--2018-03-23 - es\n",
      "INFO : 11/02/2021 12:42:54 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:42:54 PM : Appending data to the file ../data/spain_2018.csv...\n",
      "INFO : 11/02/2021 12:42:54 PM : Done appending to the file ../data/spain_2018.csv!!!\n",
      "INFO : 11/02/2021 12:42:54 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:42:55 PM : Extracting top 200 weekly for 2018-03-23--2018-03-30 - es\n",
      "INFO : 11/02/2021 12:42:55 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:42:55 PM : Appending data to the file ../data/spain_2018.csv...\n",
      "INFO : 11/02/2021 12:42:55 PM : Done appending to the file ../data/spain_2018.csv!!!\n",
      "INFO : 11/02/2021 12:42:55 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:42:56 PM : Extracting top 200 weekly for 2018-03-30--2018-04-06 - es\n",
      "INFO : 11/02/2021 12:42:56 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:42:56 PM : Appending data to the file ../data/spain_2018.csv...\n",
      "INFO : 11/02/2021 12:42:56 PM : Done appending to the file ../data/spain_2018.csv!!!\n",
      "INFO : 11/02/2021 12:42:56 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:42:57 PM : Extracting top 200 weekly for 2018-04-06--2018-04-13 - es\n",
      "INFO : 11/02/2021 12:42:57 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:42:57 PM : Appending data to the file ../data/spain_2018.csv...\n",
      "INFO : 11/02/2021 12:42:57 PM : Done appending to the file ../data/spain_2018.csv!!!\n",
      "INFO : 11/02/2021 12:42:57 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:42:58 PM : Extracting top 200 weekly for 2018-04-13--2018-04-20 - es\n",
      "INFO : 11/02/2021 12:42:58 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:42:58 PM : Appending data to the file ../data/spain_2018.csv...\n",
      "INFO : 11/02/2021 12:42:58 PM : Done appending to the file ../data/spain_2018.csv!!!\n",
      "INFO : 11/02/2021 12:42:58 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:42:59 PM : Extracting top 200 weekly for 2018-04-20--2018-04-27 - es\n",
      "INFO : 11/02/2021 12:42:59 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:42:59 PM : Appending data to the file ../data/spain_2018.csv...\n",
      "INFO : 11/02/2021 12:42:59 PM : Done appending to the file ../data/spain_2018.csv!!!\n",
      "INFO : 11/02/2021 12:42:59 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:43:00 PM : Extracting top 200 weekly for 2018-04-27--2018-05-04 - es\n",
      "INFO : 11/02/2021 12:43:00 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:43:00 PM : Appending data to the file ../data/spain_2018.csv...\n",
      "INFO : 11/02/2021 12:43:00 PM : Done appending to the file ../data/spain_2018.csv!!!\n",
      "INFO : 11/02/2021 12:43:00 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:43:01 PM : Extracting top 200 weekly for 2018-05-04--2018-05-11 - es\n",
      "INFO : 11/02/2021 12:43:01 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:43:01 PM : Appending data to the file ../data/spain_2018.csv...\n",
      "INFO : 11/02/2021 12:43:01 PM : Done appending to the file ../data/spain_2018.csv!!!\n",
      "INFO : 11/02/2021 12:43:01 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:43:02 PM : Extracting top 200 weekly for 2018-05-11--2018-05-18 - es\n",
      "INFO : 11/02/2021 12:43:02 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:43:02 PM : Appending data to the file ../data/spain_2018.csv...\n",
      "INFO : 11/02/2021 12:43:02 PM : Done appending to the file ../data/spain_2018.csv!!!\n",
      "INFO : 11/02/2021 12:43:02 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:43:03 PM : Extracting top 200 weekly for 2018-05-18--2018-05-25 - es\n",
      "INFO : 11/02/2021 12:43:03 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:43:03 PM : Appending data to the file ../data/spain_2018.csv...\n",
      "INFO : 11/02/2021 12:43:03 PM : Done appending to the file ../data/spain_2018.csv!!!\n",
      "INFO : 11/02/2021 12:43:03 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:43:04 PM : Extracting top 200 weekly for 2018-05-25--2018-06-01 - es\n",
      "INFO : 11/02/2021 12:43:04 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:43:04 PM : Appending data to the file ../data/spain_2018.csv...\n",
      "INFO : 11/02/2021 12:43:04 PM : Done appending to the file ../data/spain_2018.csv!!!\n",
      "INFO : 11/02/2021 12:43:04 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:43:05 PM : Extracting top 200 weekly for 2018-06-01--2018-06-08 - es\n",
      "INFO : 11/02/2021 12:43:05 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:43:05 PM : Appending data to the file ../data/spain_2018.csv...\n",
      "INFO : 11/02/2021 12:43:05 PM : Done appending to the file ../data/spain_2018.csv!!!\n",
      "INFO : 11/02/2021 12:43:05 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:43:06 PM : Extracting top 200 weekly for 2018-06-08--2018-06-15 - es\n",
      "INFO : 11/02/2021 12:43:06 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:43:06 PM : Appending data to the file ../data/spain_2018.csv...\n",
      "INFO : 11/02/2021 12:43:06 PM : Done appending to the file ../data/spain_2018.csv!!!\n",
      "INFO : 11/02/2021 12:43:06 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:43:07 PM : Extracting top 200 weekly for 2018-06-15--2018-06-22 - es\n",
      "INFO : 11/02/2021 12:43:07 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:43:07 PM : Appending data to the file ../data/spain_2018.csv...\n",
      "INFO : 11/02/2021 12:43:07 PM : Done appending to the file ../data/spain_2018.csv!!!\n",
      "INFO : 11/02/2021 12:43:07 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:43:08 PM : Extracting top 200 weekly for 2018-06-22--2018-06-29 - es\n",
      "INFO : 11/02/2021 12:43:08 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:43:08 PM : Appending data to the file ../data/spain_2018.csv...\n",
      "INFO : 11/02/2021 12:43:08 PM : Done appending to the file ../data/spain_2018.csv!!!\n",
      "INFO : 11/02/2021 12:43:08 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:43:08 PM : Extracting top 200 weekly for 2018-06-29--2018-07-06 - es\n",
      "INFO : 11/02/2021 12:43:08 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:43:08 PM : Appending data to the file ../data/spain_2018.csv...\n",
      "INFO : 11/02/2021 12:43:08 PM : Done appending to the file ../data/spain_2018.csv!!!\n",
      "INFO : 11/02/2021 12:43:08 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:43:09 PM : Extracting top 200 weekly for 2018-07-06--2018-07-13 - es\n",
      "INFO : 11/02/2021 12:43:09 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:43:09 PM : Appending data to the file ../data/spain_2018.csv...\n",
      "INFO : 11/02/2021 12:43:09 PM : Done appending to the file ../data/spain_2018.csv!!!\n",
      "INFO : 11/02/2021 12:43:09 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:43:10 PM : Extracting top 200 weekly for 2018-07-13--2018-07-20 - es\n",
      "INFO : 11/02/2021 12:43:10 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:43:10 PM : Appending data to the file ../data/spain_2018.csv...\n",
      "INFO : 11/02/2021 12:43:10 PM : Done appending to the file ../data/spain_2018.csv!!!\n",
      "INFO : 11/02/2021 12:43:10 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:43:11 PM : Extracting top 200 weekly for 2018-07-20--2018-07-27 - es\n",
      "INFO : 11/02/2021 12:43:11 PM : Appending data to the file ../data/spain_2018.csv...\n",
      "INFO : 11/02/2021 12:43:11 PM : Done appending to the file ../data/spain_2018.csv!!!\n",
      "INFO : 11/02/2021 12:43:11 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:43:11 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:43:13 PM : Extracting top 200 weekly for 2018-07-27--2018-08-03 - es\n",
      "INFO : 11/02/2021 12:43:13 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:43:13 PM : Appending data to the file ../data/spain_2018.csv...\n",
      "INFO : 11/02/2021 12:43:13 PM : Done appending to the file ../data/spain_2018.csv!!!\n",
      "INFO : 11/02/2021 12:43:13 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:43:14 PM : Extracting top 200 weekly for 2018-08-03--2018-08-10 - es\n",
      "INFO : 11/02/2021 12:43:14 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:43:14 PM : Appending data to the file ../data/spain_2018.csv...\n",
      "INFO : 11/02/2021 12:43:14 PM : Done appending to the file ../data/spain_2018.csv!!!\n",
      "INFO : 11/02/2021 12:43:14 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:43:15 PM : Extracting top 200 weekly for 2018-08-10--2018-08-17 - es\n",
      "INFO : 11/02/2021 12:43:15 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:43:15 PM : Appending data to the file ../data/spain_2018.csv...\n",
      "INFO : 11/02/2021 12:43:15 PM : Done appending to the file ../data/spain_2018.csv!!!\n",
      "INFO : 11/02/2021 12:43:15 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:43:16 PM : Extracting top 200 weekly for 2018-08-17--2018-08-24 - es\n",
      "INFO : 11/02/2021 12:43:16 PM : Appending data to the file ../data/spain_2018.csv...\n",
      "INFO : 11/02/2021 12:43:16 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:43:16 PM : Done appending to the file ../data/spain_2018.csv!!!\n",
      "INFO : 11/02/2021 12:43:16 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:43:17 PM : Extracting top 200 weekly for 2018-08-24--2018-08-31 - es\n",
      "INFO : 11/02/2021 12:43:17 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:43:17 PM : Appending data to the file ../data/spain_2018.csv...\n",
      "INFO : 11/02/2021 12:43:17 PM : Done appending to the file ../data/spain_2018.csv!!!\n",
      "INFO : 11/02/2021 12:43:17 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:43:18 PM : Extracting top 200 weekly for 2018-08-31--2018-09-07 - es\n",
      "INFO : 11/02/2021 12:43:18 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:43:18 PM : Appending data to the file ../data/spain_2018.csv...\n",
      "INFO : 11/02/2021 12:43:18 PM : Done appending to the file ../data/spain_2018.csv!!!\n",
      "INFO : 11/02/2021 12:43:18 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:43:19 PM : Extracting top 200 weekly for 2018-09-07--2018-09-14 - es\n",
      "INFO : 11/02/2021 12:43:19 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:43:19 PM : Appending data to the file ../data/spain_2018.csv...\n",
      "INFO : 11/02/2021 12:43:19 PM : Done appending to the file ../data/spain_2018.csv!!!\n",
      "INFO : 11/02/2021 12:43:19 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:43:20 PM : Extracting top 200 weekly for 2018-09-14--2018-09-21 - es\n",
      "INFO : 11/02/2021 12:43:20 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:43:20 PM : Appending data to the file ../data/spain_2018.csv...\n",
      "INFO : 11/02/2021 12:43:20 PM : Done appending to the file ../data/spain_2018.csv!!!\n",
      "INFO : 11/02/2021 12:43:20 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:43:21 PM : Extracting top 200 weekly for 2018-09-21--2018-09-28 - es\n",
      "INFO : 11/02/2021 12:43:21 PM : Appending data to the file ../data/spain_2018.csv...\n",
      "INFO : 11/02/2021 12:43:21 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:43:21 PM : Done appending to the file ../data/spain_2018.csv!!!\n",
      "INFO : 11/02/2021 12:43:21 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:43:22 PM : Extracting top 200 weekly for 2018-09-28--2018-10-05 - es\n",
      "INFO : 11/02/2021 12:43:22 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:43:22 PM : Appending data to the file ../data/spain_2018.csv...\n",
      "INFO : 11/02/2021 12:43:22 PM : Done appending to the file ../data/spain_2018.csv!!!\n",
      "INFO : 11/02/2021 12:43:22 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:43:23 PM : Extracting top 200 weekly for 2018-10-05--2018-10-12 - es\n",
      "INFO : 11/02/2021 12:43:23 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:43:23 PM : Appending data to the file ../data/spain_2018.csv...\n",
      "INFO : 11/02/2021 12:43:23 PM : Done appending to the file ../data/spain_2018.csv!!!\n",
      "INFO : 11/02/2021 12:43:23 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:43:24 PM : Extracting top 200 weekly for 2018-10-12--2018-10-19 - es\n",
      "INFO : 11/02/2021 12:43:24 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:43:24 PM : Appending data to the file ../data/spain_2018.csv...\n",
      "INFO : 11/02/2021 12:43:24 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:43:24 PM : Done appending to the file ../data/spain_2018.csv!!!\n",
      "INFO : 11/02/2021 12:43:25 PM : Extracting top 200 weekly for 2018-10-19--2018-10-26 - es\n",
      "INFO : 11/02/2021 12:43:25 PM : Appending data to the file ../data/spain_2018.csv...\n",
      "INFO : 11/02/2021 12:43:25 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:43:25 PM : Done appending to the file ../data/spain_2018.csv!!!\n",
      "INFO : 11/02/2021 12:43:25 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:43:26 PM : Extracting top 200 weekly for 2018-10-26--2018-11-02 - es\n",
      "INFO : 11/02/2021 12:43:26 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:43:26 PM : Appending data to the file ../data/spain_2018.csv...\n",
      "INFO : 11/02/2021 12:43:26 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:43:26 PM : Done appending to the file ../data/spain_2018.csv!!!\n",
      "INFO : 11/02/2021 12:43:28 PM : Extracting top 200 weekly for 2018-11-02--2018-11-09 - es\n",
      "INFO : 11/02/2021 12:43:28 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:43:28 PM : Appending data to the file ../data/spain_2018.csv...\n",
      "INFO : 11/02/2021 12:43:28 PM : Done appending to the file ../data/spain_2018.csv!!!\n",
      "INFO : 11/02/2021 12:43:28 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:43:29 PM : Extracting top 200 weekly for 2018-11-09--2018-11-16 - es\n",
      "INFO : 11/02/2021 12:43:29 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:43:29 PM : Appending data to the file ../data/spain_2018.csv...\n",
      "INFO : 11/02/2021 12:43:29 PM : Done appending to the file ../data/spain_2018.csv!!!\n",
      "INFO : 11/02/2021 12:43:29 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:43:30 PM : Extracting top 200 weekly for 2018-11-16--2018-11-23 - es\n",
      "INFO : 11/02/2021 12:43:30 PM : Appending data to the file ../data/spain_2018.csv...\n",
      "INFO : 11/02/2021 12:43:30 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:43:30 PM : Done appending to the file ../data/spain_2018.csv!!!\n",
      "INFO : 11/02/2021 12:43:30 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:43:30 PM : Extracting top 200 weekly for 2018-11-23--2018-11-30 - es\n",
      "INFO : 11/02/2021 12:43:30 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:43:30 PM : Appending data to the file ../data/spain_2018.csv...\n",
      "INFO : 11/02/2021 12:43:30 PM : Done appending to the file ../data/spain_2018.csv!!!\n",
      "INFO : 11/02/2021 12:43:30 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:43:31 PM : Extracting top 200 weekly for 2018-11-30--2018-12-07 - es\n",
      "INFO : 11/02/2021 12:43:31 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:43:31 PM : Appending data to the file ../data/spain_2018.csv...\n",
      "INFO : 11/02/2021 12:43:32 PM : Done appending to the file ../data/spain_2018.csv!!!\n",
      "INFO : 11/02/2021 12:43:32 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:43:33 PM : Extracting top 200 weekly for 2018-12-07--2018-12-14 - es\n",
      "INFO : 11/02/2021 12:43:33 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:43:33 PM : Appending data to the file ../data/spain_2018.csv...\n",
      "INFO : 11/02/2021 12:43:33 PM : Done appending to the file ../data/spain_2018.csv!!!\n",
      "INFO : 11/02/2021 12:43:33 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:43:34 PM : Extracting top 200 weekly for 2018-12-14--2018-12-21 - es\n",
      "INFO : 11/02/2021 12:43:34 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:43:34 PM : Appending data to the file ../data/spain_2018.csv...\n",
      "INFO : 11/02/2021 12:43:34 PM : Done appending to the file ../data/spain_2018.csv!!!\n",
      "INFO : 11/02/2021 12:43:34 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:43:35 PM : Extracting top 200 weekly for 2018-12-21--2018-12-28 - es\n",
      "INFO : 11/02/2021 12:43:35 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:43:35 PM : Appending data to the file ../data/spain_2018.csv...\n",
      "INFO : 11/02/2021 12:43:35 PM : Done appending to the file ../data/spain_2018.csv!!!\n",
      "INFO : 11/02/2021 12:43:35 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:43:36 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:43:36 PM : Appending data to the file ../data/spain_2018.csv...\n",
      "INFO : 11/02/2021 12:43:36 PM : Done appending to the file ../data/spain_2018.csv!!!\n",
      "INFO : 11/02/2021 12:43:36 PM : Done appending to the table top_200_weekly!!!\n"
     ]
    }
   ],
   "source": [
    "api = SpotifyCharts()\n",
    "connector = sqlalchemy.create_engine(\"sqlite:///../data/spain_2018.db\", echo=False)\n",
    "api.top200Weekly(output_file = \"../data/spain_2018.csv\", output_db = connector, webhook = [\"https://mywebhookssite.com/post/\"], start = \"2018-01-01\", end = \"2018-12-31\", region = \"es\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Position</th>\n",
       "      <th>Track Name</th>\n",
       "      <th>Artist</th>\n",
       "      <th>Streams</th>\n",
       "      <th>date</th>\n",
       "      <th>region</th>\n",
       "      <th>spotify_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10195</th>\n",
       "      <td>196</td>\n",
       "      <td>Nena Maldición (feat. Lenny Tavárez)</td>\n",
       "      <td>Paulo Londra</td>\n",
       "      <td>200600</td>\n",
       "      <td>2018-12-21--2018-12-28</td>\n",
       "      <td>es</td>\n",
       "      <td>2aMVyJSuDFFL1sTjJnvk7X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10196</th>\n",
       "      <td>197</td>\n",
       "      <td>Winter Wonderland</td>\n",
       "      <td>Tony Bennett</td>\n",
       "      <td>200341</td>\n",
       "      <td>2018-12-21--2018-12-28</td>\n",
       "      <td>es</td>\n",
       "      <td>4ricyQVd20UQde1jpXCSuJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10197</th>\n",
       "      <td>198</td>\n",
       "      <td>Perfect</td>\n",
       "      <td>Ed Sheeran</td>\n",
       "      <td>200130</td>\n",
       "      <td>2018-12-21--2018-12-28</td>\n",
       "      <td>es</td>\n",
       "      <td>0tgVpDi06FyKpA1z0VMD4v</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10198</th>\n",
       "      <td>199</td>\n",
       "      <td>We Will Rock You - Remastered</td>\n",
       "      <td>Queen</td>\n",
       "      <td>197737</td>\n",
       "      <td>2018-12-21--2018-12-28</td>\n",
       "      <td>es</td>\n",
       "      <td>4pbJqGIASGPr0ZpGpnWkDn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10199</th>\n",
       "      <td>200</td>\n",
       "      <td>Natural</td>\n",
       "      <td>Imagine Dragons</td>\n",
       "      <td>197593</td>\n",
       "      <td>2018-12-21--2018-12-28</td>\n",
       "      <td>es</td>\n",
       "      <td>2FY7b99s15jUprqC0M5NCT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Position                            Track Name           Artist  \\\n",
       "10195       196  Nena Maldición (feat. Lenny Tavárez)     Paulo Londra   \n",
       "10196       197                     Winter Wonderland     Tony Bennett   \n",
       "10197       198                               Perfect       Ed Sheeran   \n",
       "10198       199         We Will Rock You - Remastered            Queen   \n",
       "10199       200                               Natural  Imagine Dragons   \n",
       "\n",
       "       Streams                    date region              spotify_id  \n",
       "10195   200600  2018-12-21--2018-12-28     es  2aMVyJSuDFFL1sTjJnvk7X  \n",
       "10196   200341  2018-12-21--2018-12-28     es  4ricyQVd20UQde1jpXCSuJ  \n",
       "10197   200130  2018-12-21--2018-12-28     es  0tgVpDi06FyKpA1z0VMD4v  \n",
       "10198   197737  2018-12-21--2018-12-28     es  4pbJqGIASGPr0ZpGpnWkDn  \n",
       "10199   197593  2018-12-21--2018-12-28     es  2FY7b99s15jUprqC0M5NCT  "
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reading in csv from output file  \n",
    "spain_2018 = pd.read_csv('../data/spain_2018.csv')\n",
    "spain_2018.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spain Top 200 Weekly, 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : 11/02/2021 12:43:45 PM : The start date 2019-01-01 provided for top200Weekly is invalid. Wanna give one these a try? ['2019-01-04', '2019-01-11', '2019-01-18', '2019-01-25', '2019-02-01']\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter (1) to use the first suggestion, or (2) to quit and set yourself:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : 11/02/2021 12:45:59 PM : Extracting top 200 weekly for 2019-01-04--2019-01-11 - es\n",
      "INFO : 11/02/2021 12:46:00 PM : Extracting top 200 weekly for 2019-01-11--2019-01-18 - es\n",
      "INFO : 11/02/2021 12:46:00 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:46:00 PM : POSTing data to the endpoint https://mywebhookssite.com/post/\n",
      "INFO : 11/02/2021 12:46:00 PM : Appending data to the file ../data/spain_2019.csv...\n",
      "INFO : 11/02/2021 12:46:00 PM : Done appending to the file ../data/spain_2019.csv!!!\n",
      "Exception in thread Thread-25:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/urllib3/connection.py\", line 159, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/urllib3/util/connection.py\", line 61, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/socket.py\", line 918, in getaddrinfo\n",
      "INFO : 11/02/2021 12:46:00 PM : Done appending to the table top_200_weekly!!!\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 8] nodename nor servname provided, or not known\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 670, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 381, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 978, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/urllib3/connection.py\", line 309, in connect\n",
      "    conn = self._new_conn()\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/urllib3/connection.py\", line 171, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x7f9869ed2370>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/requests/adapters.py\", line 439, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 726, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/urllib3/util/retry.py\", line 446, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='mywebhookssite.com', port=443): Max retries exceeded with url: /post/ (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f9869ed2370>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/fycharts/SpotifyCharts.py\", line 88, in __post_to_endpoint_from_queue\n",
      "    postToRestEndpoint(df, url, what_data)\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/fycharts/write_to_outputs.py\", line 61, in postToRestEndpoint\n",
      "    raise(e)\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/fycharts/write_to_outputs.py\", line 58, in postToRestEndpoint\n",
      "    requests.post(url, json = dump)\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/requests/api.py\", line 119, in post\n",
      "    return request('post', url, data=data, json=json, **kwargs)\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/requests/api.py\", line 61, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/requests/sessions.py\", line 530, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/requests/sessions.py\", line 643, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/requests/adapters.py\", line 516, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='mywebhookssite.com', port=443): Max retries exceeded with url: /post/ (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f9869ed2370>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/threading.py\", line 932, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/fycharts/SpotifyCharts.py\", line 90, in __post_to_endpoint_from_queue\n",
      "    raise RuntimeError(e)\n",
      "RuntimeError: HTTPSConnectionPool(host='mywebhookssite.com', port=443): Max retries exceeded with url: /post/ (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f9869ed2370>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\n",
      "INFO : 11/02/2021 12:46:01 PM : Extracting top 200 weekly for 2019-01-18--2019-01-25 - es\n",
      "INFO : 11/02/2021 12:46:01 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:46:01 PM : Appending data to the file ../data/spain_2019.csv...\n",
      "INFO : 11/02/2021 12:46:01 PM : Done appending to the file ../data/spain_2019.csv!!!\n",
      "INFO : 11/02/2021 12:46:01 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:46:02 PM : Extracting top 200 weekly for 2019-01-25--2019-02-01 - es\n",
      "INFO : 11/02/2021 12:46:02 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:46:02 PM : Appending data to the file ../data/spain_2019.csv...\n",
      "INFO : 11/02/2021 12:46:02 PM : Done appending to the file ../data/spain_2019.csv!!!\n",
      "INFO : 11/02/2021 12:46:02 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:46:03 PM : Extracting top 200 weekly for 2019-02-01--2019-02-08 - es\n",
      "INFO : 11/02/2021 12:46:03 PM : Appending data to the file ../data/spain_2019.csv...\n",
      "INFO : 11/02/2021 12:46:03 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:46:03 PM : Done appending to the file ../data/spain_2019.csv!!!\n",
      "INFO : 11/02/2021 12:46:03 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:46:04 PM : Extracting top 200 weekly for 2019-02-08--2019-02-15 - es\n",
      "INFO : 11/02/2021 12:46:04 PM : Appending data to the file ../data/spain_2019.csv...\n",
      "INFO : 11/02/2021 12:46:04 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:46:04 PM : Done appending to the file ../data/spain_2019.csv!!!\n",
      "INFO : 11/02/2021 12:46:04 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:46:05 PM : Extracting top 200 weekly for 2019-02-15--2019-02-22 - es\n",
      "INFO : 11/02/2021 12:46:05 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:46:05 PM : Appending data to the file ../data/spain_2019.csv...\n",
      "INFO : 11/02/2021 12:46:05 PM : Done appending to the file ../data/spain_2019.csv!!!\n",
      "INFO : 11/02/2021 12:46:05 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:46:06 PM : Extracting top 200 weekly for 2019-02-22--2019-03-01 - es\n",
      "INFO : 11/02/2021 12:46:06 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:46:06 PM : Appending data to the file ../data/spain_2019.csv...\n",
      "INFO : 11/02/2021 12:46:06 PM : Done appending to the file ../data/spain_2019.csv!!!\n",
      "INFO : 11/02/2021 12:46:06 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:46:07 PM : Extracting top 200 weekly for 2019-03-01--2019-03-08 - es\n",
      "INFO : 11/02/2021 12:46:07 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:46:07 PM : Appending data to the file ../data/spain_2019.csv...\n",
      "INFO : 11/02/2021 12:46:07 PM : Done appending to the file ../data/spain_2019.csv!!!\n",
      "INFO : 11/02/2021 12:46:07 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:46:08 PM : Extracting top 200 weekly for 2019-03-08--2019-03-15 - es\n",
      "INFO : 11/02/2021 12:46:08 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:46:08 PM : Appending data to the file ../data/spain_2019.csv...\n",
      "INFO : 11/02/2021 12:46:08 PM : Done appending to the file ../data/spain_2019.csv!!!\n",
      "INFO : 11/02/2021 12:46:08 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:46:09 PM : Extracting top 200 weekly for 2019-03-15--2019-03-22 - es\n",
      "INFO : 11/02/2021 12:46:09 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:46:09 PM : Appending data to the file ../data/spain_2019.csv...\n",
      "INFO : 11/02/2021 12:46:09 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:46:09 PM : Done appending to the file ../data/spain_2019.csv!!!\n",
      "INFO : 11/02/2021 12:46:10 PM : Extracting top 200 weekly for 2019-03-22--2019-03-29 - es\n",
      "INFO : 11/02/2021 12:46:10 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:46:10 PM : Appending data to the file ../data/spain_2019.csv...\n",
      "INFO : 11/02/2021 12:46:10 PM : Done appending to the file ../data/spain_2019.csv!!!\n",
      "INFO : 11/02/2021 12:46:10 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:46:11 PM : Extracting top 200 weekly for 2019-03-29--2019-04-05 - es\n",
      "INFO : 11/02/2021 12:46:11 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:46:11 PM : Appending data to the file ../data/spain_2019.csv...\n",
      "INFO : 11/02/2021 12:46:11 PM : Done appending to the file ../data/spain_2019.csv!!!\n",
      "INFO : 11/02/2021 12:46:11 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:46:12 PM : Extracting top 200 weekly for 2019-04-05--2019-04-12 - es\n",
      "INFO : 11/02/2021 12:46:12 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:46:12 PM : Appending data to the file ../data/spain_2019.csv...\n",
      "INFO : 11/02/2021 12:46:12 PM : Done appending to the file ../data/spain_2019.csv!!!\n",
      "INFO : 11/02/2021 12:46:12 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:46:13 PM : Extracting top 200 weekly for 2019-04-12--2019-04-19 - es\n",
      "INFO : 11/02/2021 12:46:13 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:46:13 PM : Appending data to the file ../data/spain_2019.csv...\n",
      "INFO : 11/02/2021 12:46:13 PM : Done appending to the file ../data/spain_2019.csv!!!\n",
      "INFO : 11/02/2021 12:46:13 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:46:14 PM : Extracting top 200 weekly for 2019-04-19--2019-04-26 - es\n",
      "INFO : 11/02/2021 12:46:14 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:46:14 PM : Appending data to the file ../data/spain_2019.csv...\n",
      "INFO : 11/02/2021 12:46:14 PM : Done appending to the file ../data/spain_2019.csv!!!\n",
      "INFO : 11/02/2021 12:46:15 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:46:16 PM : Extracting top 200 weekly for 2019-04-26--2019-05-03 - es\n",
      "INFO : 11/02/2021 12:46:16 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:46:16 PM : Appending data to the file ../data/spain_2019.csv...\n",
      "INFO : 11/02/2021 12:46:16 PM : Done appending to the file ../data/spain_2019.csv!!!\n",
      "INFO : 11/02/2021 12:46:16 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:46:16 PM : Extracting top 200 weekly for 2019-05-03--2019-05-10 - es\n",
      "INFO : 11/02/2021 12:46:16 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:46:16 PM : Appending data to the file ../data/spain_2019.csv...\n",
      "INFO : 11/02/2021 12:46:16 PM : Done appending to the file ../data/spain_2019.csv!!!\n",
      "INFO : 11/02/2021 12:46:16 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:46:17 PM : Extracting top 200 weekly for 2019-05-10--2019-05-17 - es\n",
      "INFO : 11/02/2021 12:46:17 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:46:17 PM : Appending data to the file ../data/spain_2019.csv...\n",
      "INFO : 11/02/2021 12:46:17 PM : Done appending to the file ../data/spain_2019.csv!!!\n",
      "INFO : 11/02/2021 12:46:17 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:46:18 PM : Extracting top 200 weekly for 2019-05-17--2019-05-24 - es\n",
      "INFO : 11/02/2021 12:46:18 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:46:18 PM : Appending data to the file ../data/spain_2019.csv...\n",
      "INFO : 11/02/2021 12:46:18 PM : Done appending to the file ../data/spain_2019.csv!!!\n",
      "INFO : 11/02/2021 12:46:18 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:46:19 PM : Extracting top 200 weekly for 2019-05-24--2019-05-31 - es\n",
      "INFO : 11/02/2021 12:46:19 PM : Appending data to the file ../data/spain_2019.csv...\n",
      "INFO : 11/02/2021 12:46:19 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:46:19 PM : Done appending to the file ../data/spain_2019.csv!!!\n",
      "INFO : 11/02/2021 12:46:19 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:46:20 PM : Extracting top 200 weekly for 2019-05-31--2019-06-07 - es\n",
      "INFO : 11/02/2021 12:46:20 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:46:20 PM : Appending data to the file ../data/spain_2019.csv...\n",
      "INFO : 11/02/2021 12:46:20 PM : Done appending to the file ../data/spain_2019.csv!!!\n",
      "INFO : 11/02/2021 12:46:20 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:46:21 PM : Extracting top 200 weekly for 2019-06-07--2019-06-14 - es\n",
      "INFO : 11/02/2021 12:46:21 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:46:21 PM : Appending data to the file ../data/spain_2019.csv...\n",
      "INFO : 11/02/2021 12:46:21 PM : Done appending to the file ../data/spain_2019.csv!!!\n",
      "INFO : 11/02/2021 12:46:21 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:46:22 PM : Extracting top 200 weekly for 2019-06-14--2019-06-21 - es\n",
      "INFO : 11/02/2021 12:46:22 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:46:22 PM : Appending data to the file ../data/spain_2019.csv...\n",
      "INFO : 11/02/2021 12:46:22 PM : Done appending to the file ../data/spain_2019.csv!!!\n",
      "INFO : 11/02/2021 12:46:22 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:46:23 PM : Extracting top 200 weekly for 2019-06-21--2019-06-28 - es\n",
      "INFO : 11/02/2021 12:46:23 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:46:23 PM : Appending data to the file ../data/spain_2019.csv...\n",
      "INFO : 11/02/2021 12:46:23 PM : Done appending to the file ../data/spain_2019.csv!!!\n",
      "INFO : 11/02/2021 12:46:23 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:46:24 PM : Extracting top 200 weekly for 2019-06-28--2019-07-05 - es\n",
      "INFO : 11/02/2021 12:46:24 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:46:24 PM : Appending data to the file ../data/spain_2019.csv...\n",
      "INFO : 11/02/2021 12:46:24 PM : Done appending to the file ../data/spain_2019.csv!!!\n",
      "INFO : 11/02/2021 12:46:24 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:46:25 PM : Extracting top 200 weekly for 2019-07-05--2019-07-12 - es\n",
      "INFO : 11/02/2021 12:46:25 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:46:25 PM : Appending data to the file ../data/spain_2019.csv...\n",
      "INFO : 11/02/2021 12:46:25 PM : Done appending to the file ../data/spain_2019.csv!!!\n",
      "INFO : 11/02/2021 12:46:25 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:46:26 PM : Extracting top 200 weekly for 2019-07-12--2019-07-19 - es\n",
      "INFO : 11/02/2021 12:46:26 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:46:26 PM : Appending data to the file ../data/spain_2019.csv...\n",
      "INFO : 11/02/2021 12:46:26 PM : Done appending to the file ../data/spain_2019.csv!!!\n",
      "INFO : 11/02/2021 12:46:26 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:46:27 PM : Extracting top 200 weekly for 2019-07-19--2019-07-26 - es\n",
      "INFO : 11/02/2021 12:46:27 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:46:27 PM : Appending data to the file ../data/spain_2019.csv...\n",
      "INFO : 11/02/2021 12:46:27 PM : Done appending to the file ../data/spain_2019.csv!!!\n",
      "INFO : 11/02/2021 12:46:27 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:46:28 PM : Extracting top 200 weekly for 2019-07-26--2019-08-02 - es\n",
      "INFO : 11/02/2021 12:46:28 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:46:28 PM : Appending data to the file ../data/spain_2019.csv...\n",
      "INFO : 11/02/2021 12:46:28 PM : Done appending to the file ../data/spain_2019.csv!!!\n",
      "INFO : 11/02/2021 12:46:28 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:46:29 PM : Extracting top 200 weekly for 2019-08-02--2019-08-09 - es\n",
      "INFO : 11/02/2021 12:46:29 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:46:29 PM : Appending data to the file ../data/spain_2019.csv...\n",
      "INFO : 11/02/2021 12:46:29 PM : Done appending to the file ../data/spain_2019.csv!!!\n",
      "INFO : 11/02/2021 12:46:29 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:46:30 PM : Extracting top 200 weekly for 2019-08-09--2019-08-16 - es\n",
      "INFO : 11/02/2021 12:46:30 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:46:30 PM : Appending data to the file ../data/spain_2019.csv...\n",
      "INFO : 11/02/2021 12:46:30 PM : Done appending to the file ../data/spain_2019.csv!!!\n",
      "INFO : 11/02/2021 12:46:30 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:46:31 PM : Extracting top 200 weekly for 2019-08-16--2019-08-23 - es\n",
      "INFO : 11/02/2021 12:46:31 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:46:31 PM : Appending data to the file ../data/spain_2019.csv...\n",
      "INFO : 11/02/2021 12:46:31 PM : Done appending to the file ../data/spain_2019.csv!!!\n",
      "INFO : 11/02/2021 12:46:31 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:46:32 PM : Extracting top 200 weekly for 2019-08-23--2019-08-30 - es\n",
      "INFO : 11/02/2021 12:46:32 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:46:32 PM : Appending data to the file ../data/spain_2019.csv...\n",
      "INFO : 11/02/2021 12:46:32 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:46:32 PM : Done appending to the file ../data/spain_2019.csv!!!\n",
      "INFO : 11/02/2021 12:46:33 PM : Extracting top 200 weekly for 2019-08-30--2019-09-06 - es\n",
      "INFO : 11/02/2021 12:46:33 PM : Appending data to the file ../data/spain_2019.csv...\n",
      "INFO : 11/02/2021 12:46:33 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:46:33 PM : Done appending to the file ../data/spain_2019.csv!!!\n",
      "INFO : 11/02/2021 12:46:33 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:46:33 PM : Extracting top 200 weekly for 2019-09-06--2019-09-13 - es\n",
      "INFO : 11/02/2021 12:46:33 PM : Appending data to the file ../data/spain_2019.csv...\n",
      "INFO : 11/02/2021 12:46:33 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:46:34 PM : Done appending to the file ../data/spain_2019.csv!!!\n",
      "INFO : 11/02/2021 12:46:34 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:46:34 PM : Extracting top 200 weekly for 2019-09-13--2019-09-20 - es\n",
      "INFO : 11/02/2021 12:46:34 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:46:34 PM : Appending data to the file ../data/spain_2019.csv...\n",
      "INFO : 11/02/2021 12:46:34 PM : Done appending to the file ../data/spain_2019.csv!!!\n",
      "INFO : 11/02/2021 12:46:34 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:46:35 PM : Extracting top 200 weekly for 2019-09-20--2019-09-27 - es\n",
      "INFO : 11/02/2021 12:46:35 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:46:35 PM : Appending data to the file ../data/spain_2019.csv...\n",
      "INFO : 11/02/2021 12:46:35 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:46:35 PM : Done appending to the file ../data/spain_2019.csv!!!\n",
      "INFO : 11/02/2021 12:46:36 PM : Extracting top 200 weekly for 2019-09-27--2019-10-04 - es\n",
      "INFO : 11/02/2021 12:46:36 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:46:36 PM : Appending data to the file ../data/spain_2019.csv...\n",
      "INFO : 11/02/2021 12:46:36 PM : Done appending to the file ../data/spain_2019.csv!!!\n",
      "INFO : 11/02/2021 12:46:36 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:46:37 PM : Extracting top 200 weekly for 2019-10-04--2019-10-11 - es\n",
      "INFO : 11/02/2021 12:46:37 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:46:37 PM : Appending data to the file ../data/spain_2019.csv...\n",
      "INFO : 11/02/2021 12:46:37 PM : Done appending to the file ../data/spain_2019.csv!!!\n",
      "INFO : 11/02/2021 12:46:37 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:46:38 PM : Extracting top 200 weekly for 2019-10-11--2019-10-18 - es\n",
      "INFO : 11/02/2021 12:46:38 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:46:38 PM : Appending data to the file ../data/spain_2019.csv...\n",
      "INFO : 11/02/2021 12:46:38 PM : Done appending to the file ../data/spain_2019.csv!!!\n",
      "INFO : 11/02/2021 12:46:38 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:46:39 PM : Extracting top 200 weekly for 2019-10-18--2019-10-25 - es\n",
      "INFO : 11/02/2021 12:46:39 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:46:39 PM : Appending data to the file ../data/spain_2019.csv...\n",
      "INFO : 11/02/2021 12:46:39 PM : Done appending to the file ../data/spain_2019.csv!!!\n",
      "INFO : 11/02/2021 12:46:39 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:46:40 PM : Extracting top 200 weekly for 2019-10-25--2019-11-01 - es\n",
      "INFO : 11/02/2021 12:46:40 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:46:40 PM : Appending data to the file ../data/spain_2019.csv...\n",
      "INFO : 11/02/2021 12:46:40 PM : Done appending to the file ../data/spain_2019.csv!!!\n",
      "INFO : 11/02/2021 12:46:40 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:46:41 PM : Extracting top 200 weekly for 2019-11-01--2019-11-08 - es\n",
      "INFO : 11/02/2021 12:46:41 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:46:41 PM : Appending data to the file ../data/spain_2019.csv...\n",
      "INFO : 11/02/2021 12:46:41 PM : Done appending to the file ../data/spain_2019.csv!!!\n",
      "INFO : 11/02/2021 12:46:41 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:46:42 PM : Extracting top 200 weekly for 2019-11-08--2019-11-15 - es\n",
      "INFO : 11/02/2021 12:46:42 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:46:42 PM : Appending data to the file ../data/spain_2019.csv...\n",
      "INFO : 11/02/2021 12:46:42 PM : Done appending to the file ../data/spain_2019.csv!!!\n",
      "INFO : 11/02/2021 12:46:42 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:46:43 PM : Extracting top 200 weekly for 2019-11-15--2019-11-22 - es\n",
      "INFO : 11/02/2021 12:46:43 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:46:43 PM : Appending data to the file ../data/spain_2019.csv...\n",
      "INFO : 11/02/2021 12:46:43 PM : Done appending to the file ../data/spain_2019.csv!!!\n",
      "INFO : 11/02/2021 12:46:43 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:46:44 PM : Extracting top 200 weekly for 2019-11-22--2019-11-29 - es\n",
      "INFO : 11/02/2021 12:46:44 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:46:44 PM : Appending data to the file ../data/spain_2019.csv...\n",
      "INFO : 11/02/2021 12:46:44 PM : Done appending to the file ../data/spain_2019.csv!!!\n",
      "INFO : 11/02/2021 12:46:44 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:46:45 PM : Extracting top 200 weekly for 2019-11-29--2019-12-06 - es\n",
      "INFO : 11/02/2021 12:46:45 PM : Appending data to the file ../data/spain_2019.csv...\n",
      "INFO : 11/02/2021 12:46:45 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:46:45 PM : Done appending to the file ../data/spain_2019.csv!!!\n",
      "INFO : 11/02/2021 12:46:45 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:46:46 PM : Extracting top 200 weekly for 2019-12-06--2019-12-13 - es\n",
      "INFO : 11/02/2021 12:46:46 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:46:46 PM : Appending data to the file ../data/spain_2019.csv...\n",
      "INFO : 11/02/2021 12:46:46 PM : Done appending to the file ../data/spain_2019.csv!!!\n",
      "INFO : 11/02/2021 12:46:46 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:46:47 PM : Extracting top 200 weekly for 2019-12-13--2019-12-20 - es\n",
      "INFO : 11/02/2021 12:46:47 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:46:47 PM : Appending data to the file ../data/spain_2019.csv...\n",
      "INFO : 11/02/2021 12:46:47 PM : Done appending to the file ../data/spain_2019.csv!!!\n",
      "INFO : 11/02/2021 12:46:47 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:46:48 PM : Extracting top 200 weekly for 2019-12-20--2019-12-27 - es\n",
      "INFO : 11/02/2021 12:46:48 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:46:48 PM : Appending data to the file ../data/spain_2019.csv...\n",
      "INFO : 11/02/2021 12:46:48 PM : Done appending to the file ../data/spain_2019.csv!!!\n",
      "INFO : 11/02/2021 12:46:48 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:46:49 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:46:49 PM : Appending data to the file ../data/spain_2019.csv...\n",
      "INFO : 11/02/2021 12:46:49 PM : Done appending to the file ../data/spain_2019.csv!!!\n"
     ]
    }
   ],
   "source": [
    "api = SpotifyCharts()\n",
    "connector = sqlalchemy.create_engine(\"sqlite:///../data/spain_2019.db\", echo=False)\n",
    "api.top200Weekly(output_file = \"../data/spain_2019.csv\", output_db = connector, webhook = [\"https://mywebhookssite.com/post/\"], start = \"2019-01-01\", end = \"2019-12-31\", region = \"es\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : 11/02/2021 12:46:49 PM : Done appending to the table top_200_weekly!!!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Position</th>\n",
       "      <th>Track Name</th>\n",
       "      <th>Artist</th>\n",
       "      <th>Streams</th>\n",
       "      <th>date</th>\n",
       "      <th>region</th>\n",
       "      <th>spotify_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10195</th>\n",
       "      <td>196</td>\n",
       "      <td>Baila Baila Baila - Remix</td>\n",
       "      <td>Ozuna</td>\n",
       "      <td>242776</td>\n",
       "      <td>2019-12-20--2019-12-27</td>\n",
       "      <td>es</td>\n",
       "      <td>7mWFF4gPADjTQjC97CgFVt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10196</th>\n",
       "      <td>197</td>\n",
       "      <td>Cristina</td>\n",
       "      <td>Sebastian Yatra</td>\n",
       "      <td>241315</td>\n",
       "      <td>2019-12-20--2019-12-27</td>\n",
       "      <td>es</td>\n",
       "      <td>703iVQrfbQsXt7Uzgy1h8k</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10197</th>\n",
       "      <td>198</td>\n",
       "      <td>Lalala</td>\n",
       "      <td>Y2K</td>\n",
       "      <td>241122</td>\n",
       "      <td>2019-12-20--2019-12-27</td>\n",
       "      <td>es</td>\n",
       "      <td>51Fjme0JiitpyXKuyQiCDo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10198</th>\n",
       "      <td>199</td>\n",
       "      <td>Let It Snow! Let It Snow! Let It Snow!</td>\n",
       "      <td>Dean Martin</td>\n",
       "      <td>240931</td>\n",
       "      <td>2019-12-20--2019-12-27</td>\n",
       "      <td>es</td>\n",
       "      <td>2uFaJJtFpPDc5Pa95XzTvg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10199</th>\n",
       "      <td>200</td>\n",
       "      <td>Loco</td>\n",
       "      <td>Beéle</td>\n",
       "      <td>238947</td>\n",
       "      <td>2019-12-20--2019-12-27</td>\n",
       "      <td>es</td>\n",
       "      <td>2J9B63FawlTaPdg4eH5X03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Position                              Track Name           Artist  \\\n",
       "10195       196               Baila Baila Baila - Remix            Ozuna   \n",
       "10196       197                                Cristina  Sebastian Yatra   \n",
       "10197       198                                  Lalala              Y2K   \n",
       "10198       199  Let It Snow! Let It Snow! Let It Snow!      Dean Martin   \n",
       "10199       200                                    Loco            Beéle   \n",
       "\n",
       "       Streams                    date region              spotify_id  \n",
       "10195   242776  2019-12-20--2019-12-27     es  7mWFF4gPADjTQjC97CgFVt  \n",
       "10196   241315  2019-12-20--2019-12-27     es  703iVQrfbQsXt7Uzgy1h8k  \n",
       "10197   241122  2019-12-20--2019-12-27     es  51Fjme0JiitpyXKuyQiCDo  \n",
       "10198   240931  2019-12-20--2019-12-27     es  2uFaJJtFpPDc5Pa95XzTvg  \n",
       "10199   238947  2019-12-20--2019-12-27     es  2J9B63FawlTaPdg4eH5X03  "
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reading in csv from output file  \n",
    "spain_2019 = pd.read_csv('../data/spain_2019.csv')\n",
    "spain_2019.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spain Top 200 Weekly, 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : 11/02/2021 12:48:39 PM : The start date 2020-01-01 provided for top200Weekly is invalid. Wanna give one these a try? ['2020-01-03', '2020-01-10', '2020-01-17', '2020-01-24', '2020-01-31']\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter (1) to use the first suggestion, or (2) to quit and set yourself:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : 11/02/2021 12:49:06 PM : Extracting top 200 weekly for 2020-01-03--2020-01-10 - es\n",
      "INFO : 11/02/2021 12:49:07 PM : Extracting top 200 weekly for 2020-01-10--2020-01-17 - es\n",
      "INFO : 11/02/2021 12:49:07 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:49:07 PM : POSTing data to the endpoint https://mywebhookssite.com/post/\n",
      "INFO : 11/02/2021 12:49:07 PM : Appending data to the file ../data/spain_2020.csv...\n",
      "INFO : 11/02/2021 12:49:07 PM : Done appending to the file ../data/spain_2020.csv!!!\n",
      "Exception in thread Thread-31:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/urllib3/connection.py\", line 159, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/urllib3/util/connection.py\", line 61, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/socket.py\", line 918, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 8] nodename nor servname provided, or not known\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 670, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 381, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 978, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/urllib3/connection.py\", line 309, in connect\n",
      "    conn = self._new_conn()\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/urllib3/connection.py\", line 171, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x7f986abf1e20>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/requests/adapters.py\", line 439, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 726, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/urllib3/util/retry.py\", line 446, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='mywebhookssite.com', port=443): Max retries exceeded with url: /post/ (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f986abf1e20>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/fycharts/SpotifyCharts.py\", line 88, in __post_to_endpoint_from_queue\n",
      "    postToRestEndpoint(df, url, what_data)\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/fycharts/write_to_outputs.py\", line 61, in postToRestEndpoint\n",
      "    raise(e)\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/fycharts/write_to_outputs.py\", line 58, in postToRestEndpoint\n",
      "    requests.post(url, json = dump)\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/requests/api.py\", line 119, in post\n",
      "    return request('post', url, data=data, json=json, **kwargs)\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/requests/api.py\", line 61, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/requests/sessions.py\", line 530, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/requests/sessions.py\", line 643, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/requests/adapters.py\", line 516, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='mywebhookssite.com', port=443): Max retries exceeded with url: /post/ (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f986abf1e20>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/threading.py\", line 932, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/fycharts/SpotifyCharts.py\", line 90, in __post_to_endpoint_from_queue\n",
      "    raise RuntimeError(e)\n",
      "RuntimeError: HTTPSConnectionPool(host='mywebhookssite.com', port=443): Max retries exceeded with url: /post/ (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f986abf1e20>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\n",
      "INFO : 11/02/2021 12:49:07 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:49:08 PM : Extracting top 200 weekly for 2020-01-17--2020-01-24 - es\n",
      "INFO : 11/02/2021 12:49:08 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:49:08 PM : Appending data to the file ../data/spain_2020.csv...\n",
      "INFO : 11/02/2021 12:49:08 PM : Done appending to the file ../data/spain_2020.csv!!!\n",
      "INFO : 11/02/2021 12:49:08 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:49:08 PM : Extracting top 200 weekly for 2020-01-24--2020-01-31 - es\n",
      "INFO : 11/02/2021 12:49:08 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:49:08 PM : Appending data to the file ../data/spain_2020.csv...\n",
      "INFO : 11/02/2021 12:49:08 PM : Done appending to the file ../data/spain_2020.csv!!!\n",
      "INFO : 11/02/2021 12:49:08 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:49:09 PM : Extracting top 200 weekly for 2020-01-31--2020-02-07 - es\n",
      "INFO : 11/02/2021 12:49:09 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:49:09 PM : Appending data to the file ../data/spain_2020.csv...\n",
      "INFO : 11/02/2021 12:49:09 PM : Done appending to the file ../data/spain_2020.csv!!!\n",
      "INFO : 11/02/2021 12:49:09 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:49:09 PM : Extracting top 200 weekly for 2020-02-07--2020-02-14 - es\n",
      "INFO : 11/02/2021 12:49:09 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:49:09 PM : Appending data to the file ../data/spain_2020.csv...\n",
      "INFO : 11/02/2021 12:49:09 PM : Done appending to the file ../data/spain_2020.csv!!!\n",
      "INFO : 11/02/2021 12:49:09 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:49:10 PM : Extracting top 200 weekly for 2020-02-14--2020-02-21 - es\n",
      "INFO : 11/02/2021 12:49:10 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:49:10 PM : Appending data to the file ../data/spain_2020.csv...\n",
      "INFO : 11/02/2021 12:49:10 PM : Done appending to the file ../data/spain_2020.csv!!!\n",
      "INFO : 11/02/2021 12:49:10 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:49:10 PM : Extracting top 200 weekly for 2020-02-21--2020-02-28 - es\n",
      "INFO : 11/02/2021 12:49:10 PM : Appending data to the file ../data/spain_2020.csv...\n",
      "INFO : 11/02/2021 12:49:10 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:49:10 PM : Done appending to the file ../data/spain_2020.csv!!!\n",
      "INFO : 11/02/2021 12:49:10 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:49:11 PM : Extracting top 200 weekly for 2020-02-28--2020-03-06 - es\n",
      "INFO : 11/02/2021 12:49:11 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:49:11 PM : Appending data to the file ../data/spain_2020.csv...\n",
      "INFO : 11/02/2021 12:49:11 PM : Done appending to the file ../data/spain_2020.csv!!!\n",
      "INFO : 11/02/2021 12:49:11 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:49:11 PM : Extracting top 200 weekly for 2020-03-06--2020-03-13 - es\n",
      "INFO : 11/02/2021 12:49:11 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:49:11 PM : Appending data to the file ../data/spain_2020.csv...\n",
      "INFO : 11/02/2021 12:49:11 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:49:11 PM : Done appending to the file ../data/spain_2020.csv!!!\n",
      "INFO : 11/02/2021 12:49:12 PM : Extracting top 200 weekly for 2020-03-13--2020-03-20 - es\n",
      "INFO : 11/02/2021 12:49:12 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:49:12 PM : Appending data to the file ../data/spain_2020.csv...\n",
      "INFO : 11/02/2021 12:49:12 PM : Done appending to the file ../data/spain_2020.csv!!!\n",
      "INFO : 11/02/2021 12:49:12 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:49:12 PM : Extracting top 200 weekly for 2020-03-20--2020-03-27 - es\n",
      "INFO : 11/02/2021 12:49:12 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:49:12 PM : Appending data to the file ../data/spain_2020.csv...\n",
      "INFO : 11/02/2021 12:49:12 PM : Done appending to the file ../data/spain_2020.csv!!!\n",
      "INFO : 11/02/2021 12:49:12 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:49:13 PM : Extracting top 200 weekly for 2020-03-27--2020-04-03 - es\n",
      "INFO : 11/02/2021 12:49:13 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:49:13 PM : Appending data to the file ../data/spain_2020.csv...\n",
      "INFO : 11/02/2021 12:49:13 PM : Done appending to the file ../data/spain_2020.csv!!!\n",
      "INFO : 11/02/2021 12:49:13 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:49:13 PM : Extracting top 200 weekly for 2020-04-03--2020-04-10 - es\n",
      "INFO : 11/02/2021 12:49:13 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:49:13 PM : Appending data to the file ../data/spain_2020.csv...\n",
      "INFO : 11/02/2021 12:49:13 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:49:13 PM : Done appending to the file ../data/spain_2020.csv!!!\n",
      "INFO : 11/02/2021 12:49:14 PM : Extracting top 200 weekly for 2020-04-10--2020-04-17 - es\n",
      "INFO : 11/02/2021 12:49:14 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:49:14 PM : Appending data to the file ../data/spain_2020.csv...\n",
      "INFO : 11/02/2021 12:49:14 PM : Done appending to the file ../data/spain_2020.csv!!!\n",
      "INFO : 11/02/2021 12:49:14 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:49:15 PM : Extracting top 200 weekly for 2020-04-17--2020-04-24 - es\n",
      "INFO : 11/02/2021 12:49:15 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:49:15 PM : Appending data to the file ../data/spain_2020.csv...\n",
      "INFO : 11/02/2021 12:49:15 PM : Done appending to the file ../data/spain_2020.csv!!!\n",
      "INFO : 11/02/2021 12:49:15 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:49:15 PM : Extracting top 200 weekly for 2020-04-24--2020-05-01 - es\n",
      "INFO : 11/02/2021 12:49:15 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:49:15 PM : Appending data to the file ../data/spain_2020.csv...\n",
      "INFO : 11/02/2021 12:49:15 PM : Done appending to the file ../data/spain_2020.csv!!!\n",
      "INFO : 11/02/2021 12:49:15 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:49:16 PM : Extracting top 200 weekly for 2020-05-01--2020-05-08 - es\n",
      "INFO : 11/02/2021 12:49:16 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:49:16 PM : Appending data to the file ../data/spain_2020.csv...\n",
      "INFO : 11/02/2021 12:49:16 PM : Done appending to the file ../data/spain_2020.csv!!!\n",
      "INFO : 11/02/2021 12:49:16 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:49:16 PM : Extracting top 200 weekly for 2020-05-08--2020-05-15 - es\n",
      "INFO : 11/02/2021 12:49:16 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:49:16 PM : Appending data to the file ../data/spain_2020.csv...\n",
      "INFO : 11/02/2021 12:49:16 PM : Done appending to the file ../data/spain_2020.csv!!!\n",
      "INFO : 11/02/2021 12:49:16 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:49:17 PM : Extracting top 200 weekly for 2020-05-15--2020-05-22 - es\n",
      "INFO : 11/02/2021 12:49:17 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:49:17 PM : Appending data to the file ../data/spain_2020.csv...\n",
      "INFO : 11/02/2021 12:49:17 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:49:17 PM : Done appending to the file ../data/spain_2020.csv!!!\n",
      "INFO : 11/02/2021 12:49:18 PM : Extracting top 200 weekly for 2020-05-22--2020-05-29 - es\n",
      "INFO : 11/02/2021 12:49:18 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:49:18 PM : Appending data to the file ../data/spain_2020.csv...\n",
      "INFO : 11/02/2021 12:49:18 PM : Done appending to the file ../data/spain_2020.csv!!!\n",
      "INFO : 11/02/2021 12:49:18 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:49:18 PM : Extracting top 200 weekly for 2020-05-29--2020-06-05 - es\n",
      "INFO : 11/02/2021 12:49:18 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:49:18 PM : Appending data to the file ../data/spain_2020.csv...\n",
      "INFO : 11/02/2021 12:49:18 PM : Done appending to the file ../data/spain_2020.csv!!!\n",
      "INFO : 11/02/2021 12:49:18 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:49:19 PM : Extracting top 200 weekly for 2020-06-05--2020-06-12 - es\n",
      "INFO : 11/02/2021 12:49:19 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:49:19 PM : Appending data to the file ../data/spain_2020.csv...\n",
      "INFO : 11/02/2021 12:49:19 PM : Done appending to the file ../data/spain_2020.csv!!!\n",
      "INFO : 11/02/2021 12:49:19 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:49:19 PM : Extracting top 200 weekly for 2020-06-12--2020-06-19 - es\n",
      "INFO : 11/02/2021 12:49:19 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:49:19 PM : Appending data to the file ../data/spain_2020.csv...\n",
      "INFO : 11/02/2021 12:49:19 PM : Done appending to the file ../data/spain_2020.csv!!!\n",
      "INFO : 11/02/2021 12:49:19 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:49:20 PM : Extracting top 200 weekly for 2020-06-19--2020-06-26 - es\n",
      "INFO : 11/02/2021 12:49:20 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:49:20 PM : Appending data to the file ../data/spain_2020.csv...\n",
      "INFO : 11/02/2021 12:49:20 PM : Done appending to the file ../data/spain_2020.csv!!!\n",
      "INFO : 11/02/2021 12:49:20 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:49:20 PM : Extracting top 200 weekly for 2020-06-26--2020-07-03 - es\n",
      "INFO : 11/02/2021 12:49:20 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:49:20 PM : Appending data to the file ../data/spain_2020.csv...\n",
      "INFO : 11/02/2021 12:49:20 PM : Done appending to the file ../data/spain_2020.csv!!!\n",
      "INFO : 11/02/2021 12:49:20 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:49:22 PM : Extracting top 200 weekly for 2020-07-03--2020-07-10 - es\n",
      "INFO : 11/02/2021 12:49:22 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:49:22 PM : Appending data to the file ../data/spain_2020.csv...\n",
      "INFO : 11/02/2021 12:49:22 PM : Done appending to the file ../data/spain_2020.csv!!!\n",
      "INFO : 11/02/2021 12:49:22 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:49:22 PM : Extracting top 200 weekly for 2020-07-10--2020-07-17 - es\n",
      "INFO : 11/02/2021 12:49:22 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:49:22 PM : Appending data to the file ../data/spain_2020.csv...\n",
      "INFO : 11/02/2021 12:49:22 PM : Done appending to the file ../data/spain_2020.csv!!!\n",
      "INFO : 11/02/2021 12:49:22 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:49:23 PM : Extracting top 200 weekly for 2020-07-17--2020-07-24 - es\n",
      "INFO : 11/02/2021 12:49:23 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:49:23 PM : Appending data to the file ../data/spain_2020.csv...\n",
      "INFO : 11/02/2021 12:49:23 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:49:23 PM : Done appending to the file ../data/spain_2020.csv!!!\n",
      "INFO : 11/02/2021 12:49:23 PM : Extracting top 200 weekly for 2020-07-24--2020-07-31 - es\n",
      "INFO : 11/02/2021 12:49:23 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:49:23 PM : Appending data to the file ../data/spain_2020.csv...\n",
      "INFO : 11/02/2021 12:49:23 PM : Done appending to the file ../data/spain_2020.csv!!!\n",
      "INFO : 11/02/2021 12:49:23 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:49:24 PM : Extracting top 200 weekly for 2020-07-31--2020-08-07 - es\n",
      "INFO : 11/02/2021 12:49:24 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:49:24 PM : Appending data to the file ../data/spain_2020.csv...\n",
      "INFO : 11/02/2021 12:49:24 PM : Done appending to the file ../data/spain_2020.csv!!!\n",
      "INFO : 11/02/2021 12:49:24 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:49:24 PM : Extracting top 200 weekly for 2020-08-07--2020-08-14 - es\n",
      "INFO : 11/02/2021 12:49:24 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:49:24 PM : Appending data to the file ../data/spain_2020.csv...\n",
      "INFO : 11/02/2021 12:49:25 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:49:25 PM : Done appending to the file ../data/spain_2020.csv!!!\n",
      "INFO : 11/02/2021 12:49:25 PM : Extracting top 200 weekly for 2020-08-14--2020-08-21 - es\n",
      "INFO : 11/02/2021 12:49:25 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:49:25 PM : Appending data to the file ../data/spain_2020.csv...\n",
      "INFO : 11/02/2021 12:49:25 PM : Done appending to the file ../data/spain_2020.csv!!!\n",
      "INFO : 11/02/2021 12:49:25 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:49:26 PM : Extracting top 200 weekly for 2020-08-21--2020-08-28 - es\n",
      "INFO : 11/02/2021 12:49:26 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:49:26 PM : Appending data to the file ../data/spain_2020.csv...\n",
      "INFO : 11/02/2021 12:49:26 PM : Done appending to the file ../data/spain_2020.csv!!!\n",
      "INFO : 11/02/2021 12:49:26 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:49:26 PM : Extracting top 200 weekly for 2020-08-28--2020-09-04 - es\n",
      "INFO : 11/02/2021 12:49:26 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:49:26 PM : Appending data to the file ../data/spain_2020.csv...\n",
      "INFO : 11/02/2021 12:49:26 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:49:26 PM : Done appending to the file ../data/spain_2020.csv!!!\n",
      "INFO : 11/02/2021 12:49:27 PM : Extracting top 200 weekly for 2020-09-04--2020-09-11 - es\n",
      "INFO : 11/02/2021 12:49:27 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:49:27 PM : Appending data to the file ../data/spain_2020.csv...\n",
      "INFO : 11/02/2021 12:49:27 PM : Done appending to the file ../data/spain_2020.csv!!!\n",
      "INFO : 11/02/2021 12:49:27 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:49:27 PM : Extracting top 200 weekly for 2020-09-11--2020-09-18 - es\n",
      "INFO : 11/02/2021 12:49:27 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:49:27 PM : Appending data to the file ../data/spain_2020.csv...\n",
      "INFO : 11/02/2021 12:49:27 PM : Done appending to the file ../data/spain_2020.csv!!!\n",
      "INFO : 11/02/2021 12:49:27 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:49:28 PM : Extracting top 200 weekly for 2020-09-18--2020-09-25 - es\n",
      "INFO : 11/02/2021 12:49:28 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:49:28 PM : Appending data to the file ../data/spain_2020.csv...\n",
      "INFO : 11/02/2021 12:49:28 PM : Done appending to the file ../data/spain_2020.csv!!!\n",
      "INFO : 11/02/2021 12:49:28 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:49:28 PM : Extracting top 200 weekly for 2020-09-25--2020-10-02 - es\n",
      "INFO : 11/02/2021 12:49:28 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:49:28 PM : Appending data to the file ../data/spain_2020.csv...\n",
      "INFO : 11/02/2021 12:49:28 PM : Done appending to the file ../data/spain_2020.csv!!!\n",
      "INFO : 11/02/2021 12:49:28 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:49:29 PM : Extracting top 200 weekly for 2020-10-02--2020-10-09 - es\n",
      "INFO : 11/02/2021 12:49:29 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:49:29 PM : Appending data to the file ../data/spain_2020.csv...\n",
      "INFO : 11/02/2021 12:49:29 PM : Done appending to the file ../data/spain_2020.csv!!!\n",
      "INFO : 11/02/2021 12:49:29 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:49:30 PM : Extracting top 200 weekly for 2020-10-09--2020-10-16 - es\n",
      "INFO : 11/02/2021 12:49:30 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:49:30 PM : Appending data to the file ../data/spain_2020.csv...\n",
      "INFO : 11/02/2021 12:49:30 PM : Done appending to the file ../data/spain_2020.csv!!!\n",
      "INFO : 11/02/2021 12:49:30 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:49:30 PM : Extracting top 200 weekly for 2020-10-16--2020-10-23 - es\n",
      "INFO : 11/02/2021 12:49:30 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:49:30 PM : Appending data to the file ../data/spain_2020.csv...\n",
      "INFO : 11/02/2021 12:49:30 PM : Done appending to the file ../data/spain_2020.csv!!!\n",
      "INFO : 11/02/2021 12:49:30 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:49:31 PM : Extracting top 200 weekly for 2020-10-23--2020-10-30 - es\n",
      "INFO : 11/02/2021 12:49:31 PM : Appending data to the file ../data/spain_2020.csv...\n",
      "INFO : 11/02/2021 12:49:31 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:49:31 PM : Done appending to the file ../data/spain_2020.csv!!!\n",
      "INFO : 11/02/2021 12:49:31 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:49:31 PM : Extracting top 200 weekly for 2020-10-30--2020-11-06 - es\n",
      "INFO : 11/02/2021 12:49:31 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:49:31 PM : Appending data to the file ../data/spain_2020.csv...\n",
      "INFO : 11/02/2021 12:49:31 PM : Done appending to the file ../data/spain_2020.csv!!!\n",
      "INFO : 11/02/2021 12:49:31 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:49:32 PM : Extracting top 200 weekly for 2020-11-06--2020-11-13 - es\n",
      "INFO : 11/02/2021 12:49:32 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:49:32 PM : Appending data to the file ../data/spain_2020.csv...\n",
      "INFO : 11/02/2021 12:49:32 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:49:32 PM : Done appending to the file ../data/spain_2020.csv!!!\n",
      "INFO : 11/02/2021 12:49:32 PM : Extracting top 200 weekly for 2020-11-13--2020-11-20 - es\n",
      "INFO : 11/02/2021 12:49:32 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:49:32 PM : Appending data to the file ../data/spain_2020.csv...\n",
      "INFO : 11/02/2021 12:49:32 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:49:32 PM : Done appending to the file ../data/spain_2020.csv!!!\n",
      "INFO : 11/02/2021 12:49:33 PM : Extracting top 200 weekly for 2020-11-20--2020-11-27 - es\n",
      "INFO : 11/02/2021 12:49:33 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:49:33 PM : Appending data to the file ../data/spain_2020.csv...\n",
      "INFO : 11/02/2021 12:49:33 PM : Done appending to the file ../data/spain_2020.csv!!!\n",
      "INFO : 11/02/2021 12:49:33 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:49:33 PM : Extracting top 200 weekly for 2020-11-27--2020-12-04 - es\n",
      "INFO : 11/02/2021 12:49:33 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:49:33 PM : Appending data to the file ../data/spain_2020.csv...\n",
      "INFO : 11/02/2021 12:49:33 PM : Done appending to the file ../data/spain_2020.csv!!!\n",
      "INFO : 11/02/2021 12:49:33 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:49:34 PM : Extracting top 200 weekly for 2020-12-04--2020-12-11 - es\n",
      "INFO : 11/02/2021 12:49:34 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:49:34 PM : Appending data to the file ../data/spain_2020.csv...\n",
      "INFO : 11/02/2021 12:49:34 PM : Done appending to the file ../data/spain_2020.csv!!!\n",
      "INFO : 11/02/2021 12:49:34 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:49:35 PM : Extracting top 200 weekly for 2020-12-11--2020-12-18 - es\n",
      "INFO : 11/02/2021 12:49:35 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:49:35 PM : Appending data to the file ../data/spain_2020.csv...\n",
      "INFO : 11/02/2021 12:49:35 PM : Done appending to the file ../data/spain_2020.csv!!!\n",
      "INFO : 11/02/2021 12:49:35 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:49:35 PM : Extracting top 200 weekly for 2020-12-18--2020-12-25 - es\n",
      "INFO : 11/02/2021 12:49:35 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:49:35 PM : Appending data to the file ../data/spain_2020.csv...\n",
      "INFO : 11/02/2021 12:49:35 PM : Done appending to the file ../data/spain_2020.csv!!!\n",
      "INFO : 11/02/2021 12:49:35 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 12:49:36 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 12:49:36 PM : Appending data to the file ../data/spain_2020.csv...\n",
      "INFO : 11/02/2021 12:49:36 PM : Done appending to the file ../data/spain_2020.csv!!!\n"
     ]
    }
   ],
   "source": [
    "api = SpotifyCharts()\n",
    "connector = sqlalchemy.create_engine(\"sqlite:///../data/spain_2020.db\", echo=False)\n",
    "api.top200Weekly(output_file = \"../data/spain_2020.csv\", output_db = connector, webhook = [\"https://mywebhookssite.com/post/\"], start = \"2020-01-01\", end = \"2020-12-31\", region = \"es\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : 11/02/2021 12:49:36 PM : Done appending to the table top_200_weekly!!!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Position</th>\n",
       "      <th>Track Name</th>\n",
       "      <th>Artist</th>\n",
       "      <th>Streams</th>\n",
       "      <th>date</th>\n",
       "      <th>region</th>\n",
       "      <th>spotify_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Tusa</td>\n",
       "      <td>KAROL G</td>\n",
       "      <td>3041607</td>\n",
       "      <td>2020-01-03--2020-01-10</td>\n",
       "      <td>es</td>\n",
       "      <td>7k4t7uLgtOxPwTpFmtJNTY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Alocao (With Bad Gyal)</td>\n",
       "      <td>Omar Montes</td>\n",
       "      <td>1960463</td>\n",
       "      <td>2020-01-03--2020-01-10</td>\n",
       "      <td>es</td>\n",
       "      <td>6RyuoOJXNzlVWpfC5xQyeI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Dance Monkey</td>\n",
       "      <td>Tones And I</td>\n",
       "      <td>1914971</td>\n",
       "      <td>2020-01-03--2020-01-10</td>\n",
       "      <td>es</td>\n",
       "      <td>1rgnBhdG2JDFTbYkYRZAku</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>+</td>\n",
       "      <td>Aitana</td>\n",
       "      <td>1829598</td>\n",
       "      <td>2020-01-03--2020-01-10</td>\n",
       "      <td>es</td>\n",
       "      <td>5Cbo7oz78gqkzV3EAM63VA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Zorra</td>\n",
       "      <td>Bad Gyal</td>\n",
       "      <td>1592756</td>\n",
       "      <td>2020-01-03--2020-01-10</td>\n",
       "      <td>es</td>\n",
       "      <td>0OP1RzrglC008kj79Httv3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Position              Track Name       Artist  Streams  \\\n",
       "0        1                    Tusa      KAROL G  3041607   \n",
       "1        2  Alocao (With Bad Gyal)  Omar Montes  1960463   \n",
       "2        3            Dance Monkey  Tones And I  1914971   \n",
       "3        4                       +       Aitana  1829598   \n",
       "4        5                   Zorra     Bad Gyal  1592756   \n",
       "\n",
       "                     date region              spotify_id  \n",
       "0  2020-01-03--2020-01-10     es  7k4t7uLgtOxPwTpFmtJNTY  \n",
       "1  2020-01-03--2020-01-10     es  6RyuoOJXNzlVWpfC5xQyeI  \n",
       "2  2020-01-03--2020-01-10     es  1rgnBhdG2JDFTbYkYRZAku  \n",
       "3  2020-01-03--2020-01-10     es  5Cbo7oz78gqkzV3EAM63VA  \n",
       "4  2020-01-03--2020-01-10     es  0OP1RzrglC008kj79Httv3  "
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reading in csv from output file  \n",
    "spain_2020 = pd.read_csv('../data/spain_2020.csv')\n",
    "spain_2020.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10200, 7)"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spain_2017.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10200, 7)"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spain_2018.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10200, 7)"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spain_2019.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20401, 7)"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spain_2020.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_I scraped the charts for each separately in case I wanted to separate DataFrames, but now I will now merge into one DataFrame years 2017-2019 because this is more useful for EDA and the start of time series modeling._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Position</th>\n",
       "      <th>Track Name</th>\n",
       "      <th>Artist</th>\n",
       "      <th>Streams</th>\n",
       "      <th>date</th>\n",
       "      <th>region</th>\n",
       "      <th>spotify_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Chantaje (feat. Maluma)</td>\n",
       "      <td>Shakira</td>\n",
       "      <td>1423583</td>\n",
       "      <td>2017-01-06--2017-01-13</td>\n",
       "      <td>es</td>\n",
       "      <td>6mICuAdrwEjh6Y6lroV2Kg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Reggaetón Lento (Bailemos)</td>\n",
       "      <td>CNCO</td>\n",
       "      <td>1339834</td>\n",
       "      <td>2017-01-06--2017-01-13</td>\n",
       "      <td>es</td>\n",
       "      <td>3AEZUABDXNtecAOSC1qTfo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Shape of You</td>\n",
       "      <td>Ed Sheeran</td>\n",
       "      <td>1215829</td>\n",
       "      <td>2017-01-06--2017-01-13</td>\n",
       "      <td>es</td>\n",
       "      <td>7qiZfU4dY1lWllzX7mPBI3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Rockabye (feat. Sean Paul &amp; Anne-Marie)</td>\n",
       "      <td>Clean Bandit</td>\n",
       "      <td>1148813</td>\n",
       "      <td>2017-01-06--2017-01-13</td>\n",
       "      <td>es</td>\n",
       "      <td>5knuzwU65gJK7IF5yJsuaW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Safari</td>\n",
       "      <td>J Balvin</td>\n",
       "      <td>1044868</td>\n",
       "      <td>2017-01-06--2017-01-13</td>\n",
       "      <td>es</td>\n",
       "      <td>6rQSrBHf7HlZjtcMZ4S4bO</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Position                               Track Name        Artist  Streams  \\\n",
       "0         1                  Chantaje (feat. Maluma)       Shakira  1423583   \n",
       "1         2               Reggaetón Lento (Bailemos)          CNCO  1339834   \n",
       "2         3                             Shape of You    Ed Sheeran  1215829   \n",
       "3         4  Rockabye (feat. Sean Paul & Anne-Marie)  Clean Bandit  1148813   \n",
       "4         5                                   Safari      J Balvin  1044868   \n",
       "\n",
       "                     date region              spotify_id  \n",
       "0  2017-01-06--2017-01-13     es  6mICuAdrwEjh6Y6lroV2Kg  \n",
       "1  2017-01-06--2017-01-13     es  3AEZUABDXNtecAOSC1qTfo  \n",
       "2  2017-01-06--2017-01-13     es  7qiZfU4dY1lWllzX7mPBI3  \n",
       "3  2017-01-06--2017-01-13     es  5knuzwU65gJK7IF5yJsuaW  \n",
       "4  2017-01-06--2017-01-13     es  6rQSrBHf7HlZjtcMZ4S4bO  "
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spain_17_19 = pd.concat([spain_2017, spain_2018, spain_2019])\n",
    "spain_17_19.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Position</th>\n",
       "      <th>Track Name</th>\n",
       "      <th>Artist</th>\n",
       "      <th>Streams</th>\n",
       "      <th>date</th>\n",
       "      <th>region</th>\n",
       "      <th>spotify_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30595</th>\n",
       "      <td>196</td>\n",
       "      <td>Baila Baila Baila - Remix</td>\n",
       "      <td>Ozuna</td>\n",
       "      <td>242776</td>\n",
       "      <td>2019-12-20--2019-12-27</td>\n",
       "      <td>es</td>\n",
       "      <td>7mWFF4gPADjTQjC97CgFVt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30596</th>\n",
       "      <td>197</td>\n",
       "      <td>Cristina</td>\n",
       "      <td>Sebastian Yatra</td>\n",
       "      <td>241315</td>\n",
       "      <td>2019-12-20--2019-12-27</td>\n",
       "      <td>es</td>\n",
       "      <td>703iVQrfbQsXt7Uzgy1h8k</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30597</th>\n",
       "      <td>198</td>\n",
       "      <td>Lalala</td>\n",
       "      <td>Y2K</td>\n",
       "      <td>241122</td>\n",
       "      <td>2019-12-20--2019-12-27</td>\n",
       "      <td>es</td>\n",
       "      <td>51Fjme0JiitpyXKuyQiCDo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30598</th>\n",
       "      <td>199</td>\n",
       "      <td>Let It Snow! Let It Snow! Let It Snow!</td>\n",
       "      <td>Dean Martin</td>\n",
       "      <td>240931</td>\n",
       "      <td>2019-12-20--2019-12-27</td>\n",
       "      <td>es</td>\n",
       "      <td>2uFaJJtFpPDc5Pa95XzTvg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30599</th>\n",
       "      <td>200</td>\n",
       "      <td>Loco</td>\n",
       "      <td>Beéle</td>\n",
       "      <td>238947</td>\n",
       "      <td>2019-12-20--2019-12-27</td>\n",
       "      <td>es</td>\n",
       "      <td>2J9B63FawlTaPdg4eH5X03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Position                              Track Name           Artist  \\\n",
       "30595       196               Baila Baila Baila - Remix            Ozuna   \n",
       "30596       197                                Cristina  Sebastian Yatra   \n",
       "30597       198                                  Lalala              Y2K   \n",
       "30598       199  Let It Snow! Let It Snow! Let It Snow!      Dean Martin   \n",
       "30599       200                                    Loco            Beéle   \n",
       "\n",
       "       Streams                    date region              spotify_id  \n",
       "30595   242776  2019-12-20--2019-12-27     es  7mWFF4gPADjTQjC97CgFVt  \n",
       "30596   241315  2019-12-20--2019-12-27     es  703iVQrfbQsXt7Uzgy1h8k  \n",
       "30597   241122  2019-12-20--2019-12-27     es  51Fjme0JiitpyXKuyQiCDo  \n",
       "30598   240931  2019-12-20--2019-12-27     es  2uFaJJtFpPDc5Pa95XzTvg  \n",
       "30599   238947  2019-12-20--2019-12-27     es  2J9B63FawlTaPdg4eH5X03  "
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# resettting index so it can be concatenated with features dataframe later on\n",
    "spain_17_19.reset_index(drop=True, inplace=True)\n",
    "spain_17_19.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30600, 7)"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spain_17_19.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 30600 entries, 0 to 30599\n",
      "Data columns (total 7 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   Position    30600 non-null  int64 \n",
      " 1   Track Name  30592 non-null  object\n",
      " 2   Artist      30592 non-null  object\n",
      " 3   Streams     30600 non-null  int64 \n",
      " 4   date        30600 non-null  object\n",
      " 5   region      30600 non-null  object\n",
      " 6   spotify_id  30600 non-null  object\n",
      "dtypes: int64(2), object(5)\n",
      "memory usage: 1.6+ MB\n"
     ]
    }
   ],
   "source": [
    "spain_17_19.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audio Features "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtaining access to audio features for each track using Spotify's Client Credentials Flow (see above) and a wrapper library [Spotipy](https://spotipy.readthedocs.io/en/2.16.1/). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_**Citation: Code for scraping audio features borrowed from [CNN_for_Dance_Music_Classification](https://github.com/amytaylor330/CNN_for_Dance_Music_Classification)._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tracks where no audio features were available: 0\n",
      "Number of usable tracks: 30600\n"
     ]
    }
   ],
   "source": [
    "features_list_spain_17_19 = []          # will create a list of each track's audio features, each appended as a separate dictionary \n",
    "batchsize = 100    # max number of track ids we're allowed to submit per query\n",
    "None_counter = 0   # count if there are any songs without any audio features\n",
    "\n",
    "for i in range(0,len(spain_17_19['spotify_id']), batchsize):    \n",
    "    batch = spain_17_19['spotify_id'][i:i+batchsize]           # offsetting batchsize to acquire more tracks \n",
    "    \n",
    "    feature_results = sp.audio_features(batch)              #begins querying the audio features endpoint\n",
    "\n",
    "    for i, t in enumerate(feature_results):\n",
    "        if t == None:                               #if the audio features for a song are missing, count 1        \n",
    "            None_counter += 1          \n",
    "        else:\n",
    "            features_list_spain_17_19.append(t)  \n",
    "            \n",
    "print('Number of tracks where no audio features were available:', None_counter)\n",
    "print('Number of usable tracks:', len(features_list_spain_17_19))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>key</th>\n",
       "      <th>loudness</th>\n",
       "      <th>mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>valence</th>\n",
       "      <th>tempo</th>\n",
       "      <th>type</th>\n",
       "      <th>id</th>\n",
       "      <th>uri</th>\n",
       "      <th>track_href</th>\n",
       "      <th>analysis_url</th>\n",
       "      <th>duration_ms</th>\n",
       "      <th>time_signature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.852</td>\n",
       "      <td>0.773</td>\n",
       "      <td>8</td>\n",
       "      <td>-2.921</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0776</td>\n",
       "      <td>0.187</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.1590</td>\n",
       "      <td>0.907</td>\n",
       "      <td>102.034</td>\n",
       "      <td>audio_features</td>\n",
       "      <td>6mICuAdrwEjh6Y6lroV2Kg</td>\n",
       "      <td>spotify:track:6mICuAdrwEjh6Y6lroV2Kg</td>\n",
       "      <td>https://api.spotify.com/v1/tracks/6mICuAdrwEjh...</td>\n",
       "      <td>https://api.spotify.com/v1/audio-analysis/6mIC...</td>\n",
       "      <td>195840</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.761</td>\n",
       "      <td>0.838</td>\n",
       "      <td>4</td>\n",
       "      <td>-3.073</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0502</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1760</td>\n",
       "      <td>0.710</td>\n",
       "      <td>93.974</td>\n",
       "      <td>audio_features</td>\n",
       "      <td>3AEZUABDXNtecAOSC1qTfo</td>\n",
       "      <td>spotify:track:3AEZUABDXNtecAOSC1qTfo</td>\n",
       "      <td>https://api.spotify.com/v1/tracks/3AEZUABDXNte...</td>\n",
       "      <td>https://api.spotify.com/v1/audio-analysis/3AEZ...</td>\n",
       "      <td>222560</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.825</td>\n",
       "      <td>0.652</td>\n",
       "      <td>1</td>\n",
       "      <td>-3.183</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0802</td>\n",
       "      <td>0.581</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0931</td>\n",
       "      <td>0.931</td>\n",
       "      <td>95.977</td>\n",
       "      <td>audio_features</td>\n",
       "      <td>7qiZfU4dY1lWllzX7mPBI3</td>\n",
       "      <td>spotify:track:7qiZfU4dY1lWllzX7mPBI3</td>\n",
       "      <td>https://api.spotify.com/v1/tracks/7qiZfU4dY1lW...</td>\n",
       "      <td>https://api.spotify.com/v1/audio-analysis/7qiZ...</td>\n",
       "      <td>233713</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.720</td>\n",
       "      <td>0.763</td>\n",
       "      <td>9</td>\n",
       "      <td>-4.068</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0523</td>\n",
       "      <td>0.406</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1800</td>\n",
       "      <td>0.742</td>\n",
       "      <td>101.965</td>\n",
       "      <td>audio_features</td>\n",
       "      <td>5knuzwU65gJK7IF5yJsuaW</td>\n",
       "      <td>spotify:track:5knuzwU65gJK7IF5yJsuaW</td>\n",
       "      <td>https://api.spotify.com/v1/tracks/5knuzwU65gJK...</td>\n",
       "      <td>https://api.spotify.com/v1/audio-analysis/5knu...</td>\n",
       "      <td>251088</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.508</td>\n",
       "      <td>0.687</td>\n",
       "      <td>0</td>\n",
       "      <td>-4.361</td>\n",
       "      <td>1</td>\n",
       "      <td>0.3260</td>\n",
       "      <td>0.551</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.1260</td>\n",
       "      <td>0.555</td>\n",
       "      <td>180.044</td>\n",
       "      <td>audio_features</td>\n",
       "      <td>6rQSrBHf7HlZjtcMZ4S4bO</td>\n",
       "      <td>spotify:track:6rQSrBHf7HlZjtcMZ4S4bO</td>\n",
       "      <td>https://api.spotify.com/v1/tracks/6rQSrBHf7HlZ...</td>\n",
       "      <td>https://api.spotify.com/v1/audio-analysis/6rQS...</td>\n",
       "      <td>205600</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   danceability  energy  key  loudness  mode  speechiness  acousticness  \\\n",
       "0         0.852   0.773    8    -2.921     0       0.0776         0.187   \n",
       "1         0.761   0.838    4    -3.073     0       0.0502         0.400   \n",
       "2         0.825   0.652    1    -3.183     0       0.0802         0.581   \n",
       "3         0.720   0.763    9    -4.068     0       0.0523         0.406   \n",
       "4         0.508   0.687    0    -4.361     1       0.3260         0.551   \n",
       "\n",
       "   instrumentalness  liveness  valence    tempo            type  \\\n",
       "0          0.000030    0.1590    0.907  102.034  audio_features   \n",
       "1          0.000000    0.1760    0.710   93.974  audio_features   \n",
       "2          0.000000    0.0931    0.931   95.977  audio_features   \n",
       "3          0.000000    0.1800    0.742  101.965  audio_features   \n",
       "4          0.000003    0.1260    0.555  180.044  audio_features   \n",
       "\n",
       "                       id                                   uri  \\\n",
       "0  6mICuAdrwEjh6Y6lroV2Kg  spotify:track:6mICuAdrwEjh6Y6lroV2Kg   \n",
       "1  3AEZUABDXNtecAOSC1qTfo  spotify:track:3AEZUABDXNtecAOSC1qTfo   \n",
       "2  7qiZfU4dY1lWllzX7mPBI3  spotify:track:7qiZfU4dY1lWllzX7mPBI3   \n",
       "3  5knuzwU65gJK7IF5yJsuaW  spotify:track:5knuzwU65gJK7IF5yJsuaW   \n",
       "4  6rQSrBHf7HlZjtcMZ4S4bO  spotify:track:6rQSrBHf7HlZjtcMZ4S4bO   \n",
       "\n",
       "                                          track_href  \\\n",
       "0  https://api.spotify.com/v1/tracks/6mICuAdrwEjh...   \n",
       "1  https://api.spotify.com/v1/tracks/3AEZUABDXNte...   \n",
       "2  https://api.spotify.com/v1/tracks/7qiZfU4dY1lW...   \n",
       "3  https://api.spotify.com/v1/tracks/5knuzwU65gJK...   \n",
       "4  https://api.spotify.com/v1/tracks/6rQSrBHf7HlZ...   \n",
       "\n",
       "                                        analysis_url  duration_ms  \\\n",
       "0  https://api.spotify.com/v1/audio-analysis/6mIC...       195840   \n",
       "1  https://api.spotify.com/v1/audio-analysis/3AEZ...       222560   \n",
       "2  https://api.spotify.com/v1/audio-analysis/7qiZ...       233713   \n",
       "3  https://api.spotify.com/v1/audio-analysis/5knu...       251088   \n",
       "4  https://api.spotify.com/v1/audio-analysis/6rQS...       205600   \n",
       "\n",
       "   time_signature  \n",
       "0               4  \n",
       "1               4  \n",
       "2               4  \n",
       "3               4  \n",
       "4               4  "
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# building dataframe from features_list, which is a list of dictionaries (each dictionary represents the audio features from one song)\n",
    "spain_17_19_features = pd.DataFrame(features_list_spain_17_19)\n",
    "spain_17_19_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merging audio features into song charts DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30600, 7)"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spain_17_19.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30600, 18)"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spain_17_19_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Position</th>\n",
       "      <th>Track Name</th>\n",
       "      <th>Artist</th>\n",
       "      <th>Streams</th>\n",
       "      <th>date</th>\n",
       "      <th>region</th>\n",
       "      <th>spotify_id</th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>key</th>\n",
       "      <th>...</th>\n",
       "      <th>liveness</th>\n",
       "      <th>valence</th>\n",
       "      <th>tempo</th>\n",
       "      <th>type</th>\n",
       "      <th>id</th>\n",
       "      <th>uri</th>\n",
       "      <th>track_href</th>\n",
       "      <th>analysis_url</th>\n",
       "      <th>duration_ms</th>\n",
       "      <th>time_signature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Chantaje (feat. Maluma)</td>\n",
       "      <td>Shakira</td>\n",
       "      <td>1423583</td>\n",
       "      <td>2017-01-06--2017-01-13</td>\n",
       "      <td>es</td>\n",
       "      <td>6mICuAdrwEjh6Y6lroV2Kg</td>\n",
       "      <td>0.852</td>\n",
       "      <td>0.773</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1590</td>\n",
       "      <td>0.907</td>\n",
       "      <td>102.034</td>\n",
       "      <td>audio_features</td>\n",
       "      <td>6mICuAdrwEjh6Y6lroV2Kg</td>\n",
       "      <td>spotify:track:6mICuAdrwEjh6Y6lroV2Kg</td>\n",
       "      <td>https://api.spotify.com/v1/tracks/6mICuAdrwEjh...</td>\n",
       "      <td>https://api.spotify.com/v1/audio-analysis/6mIC...</td>\n",
       "      <td>195840</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Reggaetón Lento (Bailemos)</td>\n",
       "      <td>CNCO</td>\n",
       "      <td>1339834</td>\n",
       "      <td>2017-01-06--2017-01-13</td>\n",
       "      <td>es</td>\n",
       "      <td>3AEZUABDXNtecAOSC1qTfo</td>\n",
       "      <td>0.761</td>\n",
       "      <td>0.838</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1760</td>\n",
       "      <td>0.710</td>\n",
       "      <td>93.974</td>\n",
       "      <td>audio_features</td>\n",
       "      <td>3AEZUABDXNtecAOSC1qTfo</td>\n",
       "      <td>spotify:track:3AEZUABDXNtecAOSC1qTfo</td>\n",
       "      <td>https://api.spotify.com/v1/tracks/3AEZUABDXNte...</td>\n",
       "      <td>https://api.spotify.com/v1/audio-analysis/3AEZ...</td>\n",
       "      <td>222560</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Shape of You</td>\n",
       "      <td>Ed Sheeran</td>\n",
       "      <td>1215829</td>\n",
       "      <td>2017-01-06--2017-01-13</td>\n",
       "      <td>es</td>\n",
       "      <td>7qiZfU4dY1lWllzX7mPBI3</td>\n",
       "      <td>0.825</td>\n",
       "      <td>0.652</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0931</td>\n",
       "      <td>0.931</td>\n",
       "      <td>95.977</td>\n",
       "      <td>audio_features</td>\n",
       "      <td>7qiZfU4dY1lWllzX7mPBI3</td>\n",
       "      <td>spotify:track:7qiZfU4dY1lWllzX7mPBI3</td>\n",
       "      <td>https://api.spotify.com/v1/tracks/7qiZfU4dY1lW...</td>\n",
       "      <td>https://api.spotify.com/v1/audio-analysis/7qiZ...</td>\n",
       "      <td>233713</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Rockabye (feat. Sean Paul &amp; Anne-Marie)</td>\n",
       "      <td>Clean Bandit</td>\n",
       "      <td>1148813</td>\n",
       "      <td>2017-01-06--2017-01-13</td>\n",
       "      <td>es</td>\n",
       "      <td>5knuzwU65gJK7IF5yJsuaW</td>\n",
       "      <td>0.720</td>\n",
       "      <td>0.763</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1800</td>\n",
       "      <td>0.742</td>\n",
       "      <td>101.965</td>\n",
       "      <td>audio_features</td>\n",
       "      <td>5knuzwU65gJK7IF5yJsuaW</td>\n",
       "      <td>spotify:track:5knuzwU65gJK7IF5yJsuaW</td>\n",
       "      <td>https://api.spotify.com/v1/tracks/5knuzwU65gJK...</td>\n",
       "      <td>https://api.spotify.com/v1/audio-analysis/5knu...</td>\n",
       "      <td>251088</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Safari</td>\n",
       "      <td>J Balvin</td>\n",
       "      <td>1044868</td>\n",
       "      <td>2017-01-06--2017-01-13</td>\n",
       "      <td>es</td>\n",
       "      <td>6rQSrBHf7HlZjtcMZ4S4bO</td>\n",
       "      <td>0.508</td>\n",
       "      <td>0.687</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1260</td>\n",
       "      <td>0.555</td>\n",
       "      <td>180.044</td>\n",
       "      <td>audio_features</td>\n",
       "      <td>6rQSrBHf7HlZjtcMZ4S4bO</td>\n",
       "      <td>spotify:track:6rQSrBHf7HlZjtcMZ4S4bO</td>\n",
       "      <td>https://api.spotify.com/v1/tracks/6rQSrBHf7HlZ...</td>\n",
       "      <td>https://api.spotify.com/v1/audio-analysis/6rQS...</td>\n",
       "      <td>205600</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Position                               Track Name        Artist  Streams  \\\n",
       "0         1                  Chantaje (feat. Maluma)       Shakira  1423583   \n",
       "1         2               Reggaetón Lento (Bailemos)          CNCO  1339834   \n",
       "2         3                             Shape of You    Ed Sheeran  1215829   \n",
       "3         4  Rockabye (feat. Sean Paul & Anne-Marie)  Clean Bandit  1148813   \n",
       "4         5                                   Safari      J Balvin  1044868   \n",
       "\n",
       "                     date region              spotify_id  danceability  \\\n",
       "0  2017-01-06--2017-01-13     es  6mICuAdrwEjh6Y6lroV2Kg         0.852   \n",
       "1  2017-01-06--2017-01-13     es  3AEZUABDXNtecAOSC1qTfo         0.761   \n",
       "2  2017-01-06--2017-01-13     es  7qiZfU4dY1lWllzX7mPBI3         0.825   \n",
       "3  2017-01-06--2017-01-13     es  5knuzwU65gJK7IF5yJsuaW         0.720   \n",
       "4  2017-01-06--2017-01-13     es  6rQSrBHf7HlZjtcMZ4S4bO         0.508   \n",
       "\n",
       "   energy  key  ...  liveness  valence    tempo            type  \\\n",
       "0   0.773    8  ...    0.1590    0.907  102.034  audio_features   \n",
       "1   0.838    4  ...    0.1760    0.710   93.974  audio_features   \n",
       "2   0.652    1  ...    0.0931    0.931   95.977  audio_features   \n",
       "3   0.763    9  ...    0.1800    0.742  101.965  audio_features   \n",
       "4   0.687    0  ...    0.1260    0.555  180.044  audio_features   \n",
       "\n",
       "                       id                                   uri  \\\n",
       "0  6mICuAdrwEjh6Y6lroV2Kg  spotify:track:6mICuAdrwEjh6Y6lroV2Kg   \n",
       "1  3AEZUABDXNtecAOSC1qTfo  spotify:track:3AEZUABDXNtecAOSC1qTfo   \n",
       "2  7qiZfU4dY1lWllzX7mPBI3  spotify:track:7qiZfU4dY1lWllzX7mPBI3   \n",
       "3  5knuzwU65gJK7IF5yJsuaW  spotify:track:5knuzwU65gJK7IF5yJsuaW   \n",
       "4  6rQSrBHf7HlZjtcMZ4S4bO  spotify:track:6rQSrBHf7HlZjtcMZ4S4bO   \n",
       "\n",
       "                                          track_href  \\\n",
       "0  https://api.spotify.com/v1/tracks/6mICuAdrwEjh...   \n",
       "1  https://api.spotify.com/v1/tracks/3AEZUABDXNte...   \n",
       "2  https://api.spotify.com/v1/tracks/7qiZfU4dY1lW...   \n",
       "3  https://api.spotify.com/v1/tracks/5knuzwU65gJK...   \n",
       "4  https://api.spotify.com/v1/tracks/6rQSrBHf7HlZ...   \n",
       "\n",
       "                                        analysis_url duration_ms  \\\n",
       "0  https://api.spotify.com/v1/audio-analysis/6mIC...      195840   \n",
       "1  https://api.spotify.com/v1/audio-analysis/3AEZ...      222560   \n",
       "2  https://api.spotify.com/v1/audio-analysis/7qiZ...      233713   \n",
       "3  https://api.spotify.com/v1/audio-analysis/5knu...      251088   \n",
       "4  https://api.spotify.com/v1/audio-analysis/6rQS...      205600   \n",
       "\n",
       "  time_signature  \n",
       "0              4  \n",
       "1              4  \n",
       "2              4  \n",
       "3              4  \n",
       "4              4  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spain_17_19_df = pd.concat([spain_17_19, spain_17_19_features], axis=1)\n",
    "spain_17_19_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Position</th>\n",
       "      <th>Track Name</th>\n",
       "      <th>Artist</th>\n",
       "      <th>Streams</th>\n",
       "      <th>date</th>\n",
       "      <th>region</th>\n",
       "      <th>spotify_id</th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>key</th>\n",
       "      <th>...</th>\n",
       "      <th>liveness</th>\n",
       "      <th>valence</th>\n",
       "      <th>tempo</th>\n",
       "      <th>type</th>\n",
       "      <th>id</th>\n",
       "      <th>uri</th>\n",
       "      <th>track_href</th>\n",
       "      <th>analysis_url</th>\n",
       "      <th>duration_ms</th>\n",
       "      <th>time_signature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30595</th>\n",
       "      <td>196</td>\n",
       "      <td>Baila Baila Baila - Remix</td>\n",
       "      <td>Ozuna</td>\n",
       "      <td>242776</td>\n",
       "      <td>2019-12-20--2019-12-27</td>\n",
       "      <td>es</td>\n",
       "      <td>7mWFF4gPADjTQjC97CgFVt</td>\n",
       "      <td>0.785</td>\n",
       "      <td>0.572</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.2180</td>\n",
       "      <td>0.490</td>\n",
       "      <td>100.016</td>\n",
       "      <td>audio_features</td>\n",
       "      <td>7mWFF4gPADjTQjC97CgFVt</td>\n",
       "      <td>spotify:track:7mWFF4gPADjTQjC97CgFVt</td>\n",
       "      <td>https://api.spotify.com/v1/tracks/7mWFF4gPADjT...</td>\n",
       "      <td>https://api.spotify.com/v1/audio-analysis/7mWF...</td>\n",
       "      <td>235284</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30596</th>\n",
       "      <td>197</td>\n",
       "      <td>Cristina</td>\n",
       "      <td>Sebastian Yatra</td>\n",
       "      <td>241315</td>\n",
       "      <td>2019-12-20--2019-12-27</td>\n",
       "      <td>es</td>\n",
       "      <td>703iVQrfbQsXt7Uzgy1h8k</td>\n",
       "      <td>0.411</td>\n",
       "      <td>0.508</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1230</td>\n",
       "      <td>0.668</td>\n",
       "      <td>81.960</td>\n",
       "      <td>audio_features</td>\n",
       "      <td>703iVQrfbQsXt7Uzgy1h8k</td>\n",
       "      <td>spotify:track:703iVQrfbQsXt7Uzgy1h8k</td>\n",
       "      <td>https://api.spotify.com/v1/tracks/703iVQrfbQsX...</td>\n",
       "      <td>https://api.spotify.com/v1/audio-analysis/703i...</td>\n",
       "      <td>201627</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30597</th>\n",
       "      <td>198</td>\n",
       "      <td>Lalala</td>\n",
       "      <td>Y2K</td>\n",
       "      <td>241122</td>\n",
       "      <td>2019-12-20--2019-12-27</td>\n",
       "      <td>es</td>\n",
       "      <td>51Fjme0JiitpyXKuyQiCDo</td>\n",
       "      <td>0.843</td>\n",
       "      <td>0.391</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1370</td>\n",
       "      <td>0.496</td>\n",
       "      <td>129.972</td>\n",
       "      <td>audio_features</td>\n",
       "      <td>51Fjme0JiitpyXKuyQiCDo</td>\n",
       "      <td>spotify:track:51Fjme0JiitpyXKuyQiCDo</td>\n",
       "      <td>https://api.spotify.com/v1/tracks/51Fjme0Jiitp...</td>\n",
       "      <td>https://api.spotify.com/v1/audio-analysis/51Fj...</td>\n",
       "      <td>160627</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30598</th>\n",
       "      <td>199</td>\n",
       "      <td>Let It Snow! Let It Snow! Let It Snow!</td>\n",
       "      <td>Dean Martin</td>\n",
       "      <td>240931</td>\n",
       "      <td>2019-12-20--2019-12-27</td>\n",
       "      <td>es</td>\n",
       "      <td>2uFaJJtFpPDc5Pa95XzTvg</td>\n",
       "      <td>0.451</td>\n",
       "      <td>0.240</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1750</td>\n",
       "      <td>0.701</td>\n",
       "      <td>134.009</td>\n",
       "      <td>audio_features</td>\n",
       "      <td>2uFaJJtFpPDc5Pa95XzTvg</td>\n",
       "      <td>spotify:track:2uFaJJtFpPDc5Pa95XzTvg</td>\n",
       "      <td>https://api.spotify.com/v1/tracks/2uFaJJtFpPDc...</td>\n",
       "      <td>https://api.spotify.com/v1/audio-analysis/2uFa...</td>\n",
       "      <td>117147</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30599</th>\n",
       "      <td>200</td>\n",
       "      <td>Loco</td>\n",
       "      <td>Beéle</td>\n",
       "      <td>238947</td>\n",
       "      <td>2019-12-20--2019-12-27</td>\n",
       "      <td>es</td>\n",
       "      <td>2J9B63FawlTaPdg4eH5X03</td>\n",
       "      <td>0.820</td>\n",
       "      <td>0.565</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0469</td>\n",
       "      <td>0.773</td>\n",
       "      <td>105.110</td>\n",
       "      <td>audio_features</td>\n",
       "      <td>2J9B63FawlTaPdg4eH5X03</td>\n",
       "      <td>spotify:track:2J9B63FawlTaPdg4eH5X03</td>\n",
       "      <td>https://api.spotify.com/v1/tracks/2J9B63FawlTa...</td>\n",
       "      <td>https://api.spotify.com/v1/audio-analysis/2J9B...</td>\n",
       "      <td>204000</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Position                              Track Name           Artist  \\\n",
       "30595       196               Baila Baila Baila - Remix            Ozuna   \n",
       "30596       197                                Cristina  Sebastian Yatra   \n",
       "30597       198                                  Lalala              Y2K   \n",
       "30598       199  Let It Snow! Let It Snow! Let It Snow!      Dean Martin   \n",
       "30599       200                                    Loco            Beéle   \n",
       "\n",
       "       Streams                    date region              spotify_id  \\\n",
       "30595   242776  2019-12-20--2019-12-27     es  7mWFF4gPADjTQjC97CgFVt   \n",
       "30596   241315  2019-12-20--2019-12-27     es  703iVQrfbQsXt7Uzgy1h8k   \n",
       "30597   241122  2019-12-20--2019-12-27     es  51Fjme0JiitpyXKuyQiCDo   \n",
       "30598   240931  2019-12-20--2019-12-27     es  2uFaJJtFpPDc5Pa95XzTvg   \n",
       "30599   238947  2019-12-20--2019-12-27     es  2J9B63FawlTaPdg4eH5X03   \n",
       "\n",
       "       danceability  energy  key  ...  liveness  valence    tempo  \\\n",
       "30595         0.785   0.572    2  ...    0.2180    0.490  100.016   \n",
       "30596         0.411   0.508    5  ...    0.1230    0.668   81.960   \n",
       "30597         0.843   0.391    2  ...    0.1370    0.496  129.972   \n",
       "30598         0.451   0.240    1  ...    0.1750    0.701  134.009   \n",
       "30599         0.820   0.565    6  ...    0.0469    0.773  105.110   \n",
       "\n",
       "                 type                      id  \\\n",
       "30595  audio_features  7mWFF4gPADjTQjC97CgFVt   \n",
       "30596  audio_features  703iVQrfbQsXt7Uzgy1h8k   \n",
       "30597  audio_features  51Fjme0JiitpyXKuyQiCDo   \n",
       "30598  audio_features  2uFaJJtFpPDc5Pa95XzTvg   \n",
       "30599  audio_features  2J9B63FawlTaPdg4eH5X03   \n",
       "\n",
       "                                        uri  \\\n",
       "30595  spotify:track:7mWFF4gPADjTQjC97CgFVt   \n",
       "30596  spotify:track:703iVQrfbQsXt7Uzgy1h8k   \n",
       "30597  spotify:track:51Fjme0JiitpyXKuyQiCDo   \n",
       "30598  spotify:track:2uFaJJtFpPDc5Pa95XzTvg   \n",
       "30599  spotify:track:2J9B63FawlTaPdg4eH5X03   \n",
       "\n",
       "                                              track_href  \\\n",
       "30595  https://api.spotify.com/v1/tracks/7mWFF4gPADjT...   \n",
       "30596  https://api.spotify.com/v1/tracks/703iVQrfbQsX...   \n",
       "30597  https://api.spotify.com/v1/tracks/51Fjme0Jiitp...   \n",
       "30598  https://api.spotify.com/v1/tracks/2uFaJJtFpPDc...   \n",
       "30599  https://api.spotify.com/v1/tracks/2J9B63FawlTa...   \n",
       "\n",
       "                                            analysis_url duration_ms  \\\n",
       "30595  https://api.spotify.com/v1/audio-analysis/7mWF...      235284   \n",
       "30596  https://api.spotify.com/v1/audio-analysis/703i...      201627   \n",
       "30597  https://api.spotify.com/v1/audio-analysis/51Fj...      160627   \n",
       "30598  https://api.spotify.com/v1/audio-analysis/2uFa...      117147   \n",
       "30599  https://api.spotify.com/v1/audio-analysis/2J9B...      204000   \n",
       "\n",
       "      time_signature  \n",
       "30595              4  \n",
       "30596              3  \n",
       "30597              4  \n",
       "30598              4  \n",
       "30599              4  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spain_17_19_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 30600 entries, 0 to 30599\n",
      "Data columns (total 25 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   Position          30600 non-null  int64  \n",
      " 1   Track Name        30592 non-null  object \n",
      " 2   Artist            30592 non-null  object \n",
      " 3   Streams           30600 non-null  int64  \n",
      " 4   date              30600 non-null  object \n",
      " 5   region            30600 non-null  object \n",
      " 6   spotify_id        30600 non-null  object \n",
      " 7   danceability      30600 non-null  float64\n",
      " 8   energy            30600 non-null  float64\n",
      " 9   key               30600 non-null  int64  \n",
      " 10  loudness          30600 non-null  float64\n",
      " 11  mode              30600 non-null  int64  \n",
      " 12  speechiness       30600 non-null  float64\n",
      " 13  acousticness      30600 non-null  float64\n",
      " 14  instrumentalness  30600 non-null  float64\n",
      " 15  liveness          30600 non-null  float64\n",
      " 16  valence           30600 non-null  float64\n",
      " 17  tempo             30600 non-null  float64\n",
      " 18  type              30600 non-null  object \n",
      " 19  id                30600 non-null  object \n",
      " 20  uri               30600 non-null  object \n",
      " 21  track_href        30600 non-null  object \n",
      " 22  analysis_url      30600 non-null  object \n",
      " 23  duration_ms       30600 non-null  int64  \n",
      " 24  time_signature    30600 non-null  int64  \n",
      "dtypes: float64(9), int64(6), object(10)\n",
      "memory usage: 5.8+ MB\n"
     ]
    }
   ],
   "source": [
    "spain_17_19_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dropping extraneous columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Position</th>\n",
       "      <th>Track Name</th>\n",
       "      <th>Artist</th>\n",
       "      <th>Streams</th>\n",
       "      <th>date</th>\n",
       "      <th>region</th>\n",
       "      <th>spotify_id</th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>key</th>\n",
       "      <th>loudness</th>\n",
       "      <th>mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>valence</th>\n",
       "      <th>tempo</th>\n",
       "      <th>duration_ms</th>\n",
       "      <th>time_signature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Chantaje (feat. Maluma)</td>\n",
       "      <td>Shakira</td>\n",
       "      <td>1423583</td>\n",
       "      <td>2017-01-06--2017-01-13</td>\n",
       "      <td>es</td>\n",
       "      <td>6mICuAdrwEjh6Y6lroV2Kg</td>\n",
       "      <td>0.852</td>\n",
       "      <td>0.773</td>\n",
       "      <td>8</td>\n",
       "      <td>-2.921</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0776</td>\n",
       "      <td>0.187</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.1590</td>\n",
       "      <td>0.907</td>\n",
       "      <td>102.034</td>\n",
       "      <td>195840</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Reggaetón Lento (Bailemos)</td>\n",
       "      <td>CNCO</td>\n",
       "      <td>1339834</td>\n",
       "      <td>2017-01-06--2017-01-13</td>\n",
       "      <td>es</td>\n",
       "      <td>3AEZUABDXNtecAOSC1qTfo</td>\n",
       "      <td>0.761</td>\n",
       "      <td>0.838</td>\n",
       "      <td>4</td>\n",
       "      <td>-3.073</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0502</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1760</td>\n",
       "      <td>0.710</td>\n",
       "      <td>93.974</td>\n",
       "      <td>222560</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Shape of You</td>\n",
       "      <td>Ed Sheeran</td>\n",
       "      <td>1215829</td>\n",
       "      <td>2017-01-06--2017-01-13</td>\n",
       "      <td>es</td>\n",
       "      <td>7qiZfU4dY1lWllzX7mPBI3</td>\n",
       "      <td>0.825</td>\n",
       "      <td>0.652</td>\n",
       "      <td>1</td>\n",
       "      <td>-3.183</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0802</td>\n",
       "      <td>0.581</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0931</td>\n",
       "      <td>0.931</td>\n",
       "      <td>95.977</td>\n",
       "      <td>233713</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Rockabye (feat. Sean Paul &amp; Anne-Marie)</td>\n",
       "      <td>Clean Bandit</td>\n",
       "      <td>1148813</td>\n",
       "      <td>2017-01-06--2017-01-13</td>\n",
       "      <td>es</td>\n",
       "      <td>5knuzwU65gJK7IF5yJsuaW</td>\n",
       "      <td>0.720</td>\n",
       "      <td>0.763</td>\n",
       "      <td>9</td>\n",
       "      <td>-4.068</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0523</td>\n",
       "      <td>0.406</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1800</td>\n",
       "      <td>0.742</td>\n",
       "      <td>101.965</td>\n",
       "      <td>251088</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Safari</td>\n",
       "      <td>J Balvin</td>\n",
       "      <td>1044868</td>\n",
       "      <td>2017-01-06--2017-01-13</td>\n",
       "      <td>es</td>\n",
       "      <td>6rQSrBHf7HlZjtcMZ4S4bO</td>\n",
       "      <td>0.508</td>\n",
       "      <td>0.687</td>\n",
       "      <td>0</td>\n",
       "      <td>-4.361</td>\n",
       "      <td>1</td>\n",
       "      <td>0.3260</td>\n",
       "      <td>0.551</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.1260</td>\n",
       "      <td>0.555</td>\n",
       "      <td>180.044</td>\n",
       "      <td>205600</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Position                               Track Name        Artist  Streams  \\\n",
       "0         1                  Chantaje (feat. Maluma)       Shakira  1423583   \n",
       "1         2               Reggaetón Lento (Bailemos)          CNCO  1339834   \n",
       "2         3                             Shape of You    Ed Sheeran  1215829   \n",
       "3         4  Rockabye (feat. Sean Paul & Anne-Marie)  Clean Bandit  1148813   \n",
       "4         5                                   Safari      J Balvin  1044868   \n",
       "\n",
       "                     date region              spotify_id  danceability  \\\n",
       "0  2017-01-06--2017-01-13     es  6mICuAdrwEjh6Y6lroV2Kg         0.852   \n",
       "1  2017-01-06--2017-01-13     es  3AEZUABDXNtecAOSC1qTfo         0.761   \n",
       "2  2017-01-06--2017-01-13     es  7qiZfU4dY1lWllzX7mPBI3         0.825   \n",
       "3  2017-01-06--2017-01-13     es  5knuzwU65gJK7IF5yJsuaW         0.720   \n",
       "4  2017-01-06--2017-01-13     es  6rQSrBHf7HlZjtcMZ4S4bO         0.508   \n",
       "\n",
       "   energy  key  loudness  mode  speechiness  acousticness  instrumentalness  \\\n",
       "0   0.773    8    -2.921     0       0.0776         0.187          0.000030   \n",
       "1   0.838    4    -3.073     0       0.0502         0.400          0.000000   \n",
       "2   0.652    1    -3.183     0       0.0802         0.581          0.000000   \n",
       "3   0.763    9    -4.068     0       0.0523         0.406          0.000000   \n",
       "4   0.687    0    -4.361     1       0.3260         0.551          0.000003   \n",
       "\n",
       "   liveness  valence    tempo  duration_ms  time_signature  \n",
       "0    0.1590    0.907  102.034       195840               4  \n",
       "1    0.1760    0.710   93.974       222560               4  \n",
       "2    0.0931    0.931   95.977       233713               4  \n",
       "3    0.1800    0.742  101.965       251088               4  \n",
       "4    0.1260    0.555  180.044       205600               4  "
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols_to_drop = ['type', 'id', 'uri', 'track_href', 'analysis_url']\n",
    "\n",
    "spain_17_19_df.drop(columns=cols_to_drop, inplace=True)\n",
    "\n",
    "spain_17_19_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converting `date` column to DateTime and setting as index for use in later time series analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `date` column is currently a range for the week, so I need to convert this to just being a single date. I will make it the first day of the week, which is a Sunday because that's how Spotify charts set up their system. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for date column\n",
    "\n",
    "spain_17_19_df['date'] = spain_17_19_df['date'].apply(lambda x: x[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "spain_17_19_df['date'] = pd.to_datetime(spain_17_19_df['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Position</th>\n",
       "      <th>Track Name</th>\n",
       "      <th>Artist</th>\n",
       "      <th>Streams</th>\n",
       "      <th>region</th>\n",
       "      <th>spotify_id</th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>key</th>\n",
       "      <th>loudness</th>\n",
       "      <th>mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>valence</th>\n",
       "      <th>tempo</th>\n",
       "      <th>duration_ms</th>\n",
       "      <th>time_signature</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-12-20</th>\n",
       "      <td>196</td>\n",
       "      <td>Baila Baila Baila - Remix</td>\n",
       "      <td>Ozuna</td>\n",
       "      <td>242776</td>\n",
       "      <td>es</td>\n",
       "      <td>7mWFF4gPADjTQjC97CgFVt</td>\n",
       "      <td>0.785</td>\n",
       "      <td>0.572</td>\n",
       "      <td>2</td>\n",
       "      <td>-7.504</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1230</td>\n",
       "      <td>0.0598</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.2180</td>\n",
       "      <td>0.490</td>\n",
       "      <td>100.016</td>\n",
       "      <td>235284</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-20</th>\n",
       "      <td>197</td>\n",
       "      <td>Cristina</td>\n",
       "      <td>Sebastian Yatra</td>\n",
       "      <td>241315</td>\n",
       "      <td>es</td>\n",
       "      <td>703iVQrfbQsXt7Uzgy1h8k</td>\n",
       "      <td>0.411</td>\n",
       "      <td>0.508</td>\n",
       "      <td>5</td>\n",
       "      <td>-4.342</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0557</td>\n",
       "      <td>0.2900</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1230</td>\n",
       "      <td>0.668</td>\n",
       "      <td>81.960</td>\n",
       "      <td>201627</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-20</th>\n",
       "      <td>198</td>\n",
       "      <td>Lalala</td>\n",
       "      <td>Y2K</td>\n",
       "      <td>241122</td>\n",
       "      <td>es</td>\n",
       "      <td>51Fjme0JiitpyXKuyQiCDo</td>\n",
       "      <td>0.843</td>\n",
       "      <td>0.391</td>\n",
       "      <td>2</td>\n",
       "      <td>-7.899</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0845</td>\n",
       "      <td>0.1810</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1370</td>\n",
       "      <td>0.496</td>\n",
       "      <td>129.972</td>\n",
       "      <td>160627</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-20</th>\n",
       "      <td>199</td>\n",
       "      <td>Let It Snow! Let It Snow! Let It Snow!</td>\n",
       "      <td>Dean Martin</td>\n",
       "      <td>240931</td>\n",
       "      <td>es</td>\n",
       "      <td>2uFaJJtFpPDc5Pa95XzTvg</td>\n",
       "      <td>0.451</td>\n",
       "      <td>0.240</td>\n",
       "      <td>1</td>\n",
       "      <td>-14.014</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0351</td>\n",
       "      <td>0.9120</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.1750</td>\n",
       "      <td>0.701</td>\n",
       "      <td>134.009</td>\n",
       "      <td>117147</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-20</th>\n",
       "      <td>200</td>\n",
       "      <td>Loco</td>\n",
       "      <td>Beéle</td>\n",
       "      <td>238947</td>\n",
       "      <td>es</td>\n",
       "      <td>2J9B63FawlTaPdg4eH5X03</td>\n",
       "      <td>0.820</td>\n",
       "      <td>0.565</td>\n",
       "      <td>6</td>\n",
       "      <td>-5.965</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1700</td>\n",
       "      <td>0.1580</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.0469</td>\n",
       "      <td>0.773</td>\n",
       "      <td>105.110</td>\n",
       "      <td>204000</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Position                              Track Name           Artist  \\\n",
       "date                                                                            \n",
       "2019-12-20       196               Baila Baila Baila - Remix            Ozuna   \n",
       "2019-12-20       197                                Cristina  Sebastian Yatra   \n",
       "2019-12-20       198                                  Lalala              Y2K   \n",
       "2019-12-20       199  Let It Snow! Let It Snow! Let It Snow!      Dean Martin   \n",
       "2019-12-20       200                                    Loco            Beéle   \n",
       "\n",
       "            Streams region              spotify_id  danceability  energy  key  \\\n",
       "date                                                                            \n",
       "2019-12-20   242776     es  7mWFF4gPADjTQjC97CgFVt         0.785   0.572    2   \n",
       "2019-12-20   241315     es  703iVQrfbQsXt7Uzgy1h8k         0.411   0.508    5   \n",
       "2019-12-20   241122     es  51Fjme0JiitpyXKuyQiCDo         0.843   0.391    2   \n",
       "2019-12-20   240931     es  2uFaJJtFpPDc5Pa95XzTvg         0.451   0.240    1   \n",
       "2019-12-20   238947     es  2J9B63FawlTaPdg4eH5X03         0.820   0.565    6   \n",
       "\n",
       "            loudness  mode  speechiness  acousticness  instrumentalness  \\\n",
       "date                                                                      \n",
       "2019-12-20    -7.504     1       0.1230        0.0598          0.000000   \n",
       "2019-12-20    -4.342     1       0.0557        0.2900          0.000000   \n",
       "2019-12-20    -7.899     1       0.0845        0.1810          0.000000   \n",
       "2019-12-20   -14.014     1       0.0351        0.9120          0.000002   \n",
       "2019-12-20    -5.965     1       0.1700        0.1580          0.000024   \n",
       "\n",
       "            liveness  valence    tempo  duration_ms  time_signature  \n",
       "date                                                                 \n",
       "2019-12-20    0.2180    0.490  100.016       235284               4  \n",
       "2019-12-20    0.1230    0.668   81.960       201627               3  \n",
       "2019-12-20    0.1370    0.496  129.972       160627               4  \n",
       "2019-12-20    0.1750    0.701  134.009       117147               4  \n",
       "2019-12-20    0.0469    0.773  105.110       204000               4  "
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spain_17_19_df.set_index('date', inplace=True)\n",
    "\n",
    "spain_17_19_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 30600 entries, 2017-01-06 to 2019-12-20\n",
      "Data columns (total 19 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   Position          30600 non-null  int64  \n",
      " 1   Track Name        30592 non-null  object \n",
      " 2   Artist            30592 non-null  object \n",
      " 3   Streams           30600 non-null  int64  \n",
      " 4   region            30600 non-null  object \n",
      " 5   spotify_id        30600 non-null  object \n",
      " 6   danceability      30600 non-null  float64\n",
      " 7   energy            30600 non-null  float64\n",
      " 8   key               30600 non-null  int64  \n",
      " 9   loudness          30600 non-null  float64\n",
      " 10  mode              30600 non-null  int64  \n",
      " 11  speechiness       30600 non-null  float64\n",
      " 12  acousticness      30600 non-null  float64\n",
      " 13  instrumentalness  30600 non-null  float64\n",
      " 14  liveness          30600 non-null  float64\n",
      " 15  valence           30600 non-null  float64\n",
      " 16  tempo             30600 non-null  float64\n",
      " 17  duration_ms       30600 non-null  int64  \n",
      " 18  time_signature    30600 non-null  int64  \n",
      "dtypes: float64(9), int64(6), object(4)\n",
      "memory usage: 4.7+ MB\n"
     ]
    }
   ],
   "source": [
    "spain_17_19_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving clean DataFrame to csv and pickle for use in another notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "spain_17_19_df.to_csv('../data/sp_17_19_feat.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "spain_17_19_df.to_pickle('../data/sp_17_19_feat.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Greece's top songs 2017-2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ _All lists of top 200 / viral 50 song gathered from [Spotify Charts Regional](https://spotifycharts.com/regional/)_\n",
    "\n",
    "+ _Data are written as both a csv file and a SQLLite db._ \n",
    "\n",
    "+ _**Citation:** Code for how to scrape [Spotify Charts Regional](https://spotifycharts.com/regional/) is inspired by the excellent documentation for the [Unofficial Spotify Charts API](https://github.com/kelvingakuo/fycharts) called `fycharts`._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Greece Top 200 Weekly, 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : 11/02/2021 01:53:23 PM : The start date 2017-01-01 provided for top200Weekly is invalid. Wanna give one these a try? ['2017-01-06', '2017-01-13', '2017-01-20', '2017-01-27', '2017-02-03']\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter (1) to use the first suggestion, or (2) to quit and set yourself:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : 11/02/2021 01:53:37 PM : Extracting top 200 weekly for 2017-01-06--2017-01-13 - gr\n",
      "INFO : 11/02/2021 01:53:38 PM : Extracting top 200 weekly for 2017-01-13--2017-01-20 - gr\n",
      "INFO : 11/02/2021 01:53:38 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:53:38 PM : POSTing data to the endpoint https://mywebhookssite.com/post/\n",
      "INFO : 11/02/2021 01:53:38 PM : Appending data to the file ../data/greece_2017.csv...\n",
      "INFO : 11/02/2021 01:53:38 PM : Done appending to the file ../data/greece_2017.csv!!!\n",
      "Exception in thread Thread-37INFO : 11/02/2021 01:53:38 PM : Done appending to the table top_200_weekly!!!\n",
      ":\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/urllib3/connection.py\", line 159, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/urllib3/util/connection.py\", line 61, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/socket.py\", line 918, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 8] nodename nor servname provided, or not known\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 670, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 381, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 978, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/urllib3/connection.py\", line 309, in connect\n",
      "    conn = self._new_conn()\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/urllib3/connection.py\", line 171, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x7f98779b5730>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/requests/adapters.py\", line 439, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 726, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/urllib3/util/retry.py\", line 446, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='mywebhookssite.com', port=443): Max retries exceeded with url: /post/ (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f98779b5730>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/fycharts/SpotifyCharts.py\", line 88, in __post_to_endpoint_from_queue\n",
      "    postToRestEndpoint(df, url, what_data)\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/fycharts/write_to_outputs.py\", line 61, in postToRestEndpoint\n",
      "    raise(e)\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/fycharts/write_to_outputs.py\", line 58, in postToRestEndpoint\n",
      "    requests.post(url, json = dump)\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/requests/api.py\", line 119, in post\n",
      "    return request('post', url, data=data, json=json, **kwargs)\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/requests/api.py\", line 61, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/requests/sessions.py\", line 530, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/requests/sessions.py\", line 643, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/requests/adapters.py\", line 516, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='mywebhookssite.com', port=443): Max retries exceeded with url: /post/ (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f98779b5730>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/threading.py\", line 932, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/fycharts/SpotifyCharts.py\", line 90, in __post_to_endpoint_from_queue\n",
      "    raise RuntimeError(e)\n",
      "RuntimeError: HTTPSConnectionPool(host='mywebhookssite.com', port=443): Max retries exceeded with url: /post/ (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f98779b5730>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\n",
      "INFO : 11/02/2021 01:53:40 PM : Extracting top 200 weekly for 2017-01-20--2017-01-27 - gr\n",
      "INFO : 11/02/2021 01:53:40 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:53:40 PM : Appending data to the file ../data/greece_2017.csv...\n",
      "INFO : 11/02/2021 01:53:40 PM : Done appending to the file ../data/greece_2017.csv!!!\n",
      "INFO : 11/02/2021 01:53:40 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:53:40 PM : Extracting top 200 weekly for 2017-01-27--2017-02-03 - gr\n",
      "INFO : 11/02/2021 01:53:40 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:53:40 PM : Appending data to the file ../data/greece_2017.csv...\n",
      "INFO : 11/02/2021 01:53:40 PM : Done appending to the file ../data/greece_2017.csv!!!\n",
      "INFO : 11/02/2021 01:53:40 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:53:41 PM : Extracting top 200 weekly for 2017-02-03--2017-02-10 - gr\n",
      "INFO : 11/02/2021 01:53:41 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:53:41 PM : Appending data to the file ../data/greece_2017.csv...\n",
      "INFO : 11/02/2021 01:53:41 PM : Done appending to the file ../data/greece_2017.csv!!!\n",
      "INFO : 11/02/2021 01:53:41 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:53:42 PM : Extracting top 200 weekly for 2017-02-10--2017-02-17 - gr\n",
      "INFO : 11/02/2021 01:53:42 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:53:42 PM : Appending data to the file ../data/greece_2017.csv...\n",
      "INFO : 11/02/2021 01:53:42 PM : Done appending to the file ../data/greece_2017.csv!!!\n",
      "INFO : 11/02/2021 01:53:42 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:53:43 PM : Extracting top 200 weekly for 2017-02-17--2017-02-24 - gr\n",
      "INFO : 11/02/2021 01:53:43 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:53:43 PM : Appending data to the file ../data/greece_2017.csv...\n",
      "INFO : 11/02/2021 01:53:43 PM : Done appending to the file ../data/greece_2017.csv!!!\n",
      "INFO : 11/02/2021 01:53:43 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:53:44 PM : Extracting top 200 weekly for 2017-02-24--2017-03-03 - gr\n",
      "INFO : 11/02/2021 01:53:44 PM : Appending data to the file ../data/greece_2017.csv...\n",
      "INFO : 11/02/2021 01:53:44 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:53:44 PM : Done appending to the file ../data/greece_2017.csv!!!\n",
      "INFO : 11/02/2021 01:53:44 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:53:45 PM : Extracting top 200 weekly for 2017-03-03--2017-03-10 - gr\n",
      "INFO : 11/02/2021 01:53:45 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:53:45 PM : Appending data to the file ../data/greece_2017.csv...\n",
      "INFO : 11/02/2021 01:53:45 PM : Done appending to the file ../data/greece_2017.csv!!!\n",
      "INFO : 11/02/2021 01:53:45 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:53:46 PM : Extracting top 200 weekly for 2017-03-10--2017-03-17 - gr\n",
      "INFO : 11/02/2021 01:53:46 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:53:46 PM : Appending data to the file ../data/greece_2017.csv...\n",
      "INFO : 11/02/2021 01:53:46 PM : Done appending to the file ../data/greece_2017.csv!!!\n",
      "INFO : 11/02/2021 01:53:46 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:53:47 PM : Extracting top 200 weekly for 2017-03-17--2017-03-24 - gr\n",
      "INFO : 11/02/2021 01:53:47 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:53:47 PM : Appending data to the file ../data/greece_2017.csv...\n",
      "INFO : 11/02/2021 01:53:47 PM : Done appending to the file ../data/greece_2017.csv!!!\n",
      "INFO : 11/02/2021 01:53:47 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:53:48 PM : Extracting top 200 weekly for 2017-03-24--2017-03-31 - gr\n",
      "INFO : 11/02/2021 01:53:48 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:53:48 PM : Appending data to the file ../data/greece_2017.csv...\n",
      "INFO : 11/02/2021 01:53:48 PM : Done appending to the file ../data/greece_2017.csv!!!\n",
      "INFO : 11/02/2021 01:53:48 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:53:48 PM : Extracting top 200 weekly for 2017-03-31--2017-04-07 - gr\n",
      "INFO : 11/02/2021 01:53:48 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:53:48 PM : Appending data to the file ../data/greece_2017.csv...\n",
      "INFO : 11/02/2021 01:53:48 PM : Done appending to the file ../data/greece_2017.csv!!!\n",
      "INFO : 11/02/2021 01:53:48 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:53:49 PM : Extracting top 200 weekly for 2017-04-07--2017-04-14 - gr\n",
      "INFO : 11/02/2021 01:53:49 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:53:49 PM : Appending data to the file ../data/greece_2017.csv...\n",
      "INFO : 11/02/2021 01:53:49 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:53:49 PM : Done appending to the file ../data/greece_2017.csv!!!\n",
      "INFO : 11/02/2021 01:53:50 PM : Extracting top 200 weekly for 2017-04-14--2017-04-21 - gr\n",
      "INFO : 11/02/2021 01:53:50 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:53:50 PM : Appending data to the file ../data/greece_2017.csv...\n",
      "INFO : 11/02/2021 01:53:50 PM : Done appending to the file ../data/greece_2017.csv!!!\n",
      "INFO : 11/02/2021 01:53:50 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:53:51 PM : Extracting top 200 weekly for 2017-04-21--2017-04-28 - gr\n",
      "INFO : 11/02/2021 01:53:51 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:53:51 PM : Appending data to the file ../data/greece_2017.csv...\n",
      "INFO : 11/02/2021 01:53:51 PM : Done appending to the file ../data/greece_2017.csv!!!\n",
      "INFO : 11/02/2021 01:53:51 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:53:52 PM : Extracting top 200 weekly for 2017-04-28--2017-05-05 - gr\n",
      "INFO : 11/02/2021 01:53:52 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:53:52 PM : Appending data to the file ../data/greece_2017.csv...\n",
      "INFO : 11/02/2021 01:53:52 PM : Done appending to the file ../data/greece_2017.csv!!!\n",
      "INFO : 11/02/2021 01:53:52 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:53:53 PM : Extracting top 200 weekly for 2017-05-05--2017-05-12 - gr\n",
      "INFO : 11/02/2021 01:53:53 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:53:53 PM : Appending data to the file ../data/greece_2017.csv...\n",
      "INFO : 11/02/2021 01:53:53 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:53:53 PM : Done appending to the file ../data/greece_2017.csv!!!\n",
      "INFO : 11/02/2021 01:53:54 PM : Extracting top 200 weekly for 2017-05-12--2017-05-19 - gr\n",
      "INFO : 11/02/2021 01:53:54 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:53:54 PM : Appending data to the file ../data/greece_2017.csv...\n",
      "INFO : 11/02/2021 01:53:54 PM : Done appending to the file ../data/greece_2017.csv!!!\n",
      "INFO : 11/02/2021 01:53:54 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:53:55 PM : Extracting top 200 weekly for 2017-05-19--2017-05-26 - gr\n",
      "INFO : 11/02/2021 01:53:55 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:53:55 PM : Appending data to the file ../data/greece_2017.csv...\n",
      "INFO : 11/02/2021 01:53:55 PM : Done appending to the file ../data/greece_2017.csv!!!\n",
      "INFO : 11/02/2021 01:53:55 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:53:56 PM : Extracting top 200 weekly for 2017-05-26--2017-06-02 - gr\n",
      "INFO : 11/02/2021 01:53:56 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:53:56 PM : Appending data to the file ../data/greece_2017.csv...\n",
      "INFO : 11/02/2021 01:53:56 PM : Done appending to the file ../data/greece_2017.csv!!!\n",
      "INFO : 11/02/2021 01:53:56 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:53:57 PM : Extracting top 200 weekly for 2017-06-02--2017-06-09 - gr\n",
      "INFO : 11/02/2021 01:53:57 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:53:57 PM : Appending data to the file ../data/greece_2017.csv...\n",
      "INFO : 11/02/2021 01:53:57 PM : Done appending to the file ../data/greece_2017.csv!!!\n",
      "INFO : 11/02/2021 01:53:57 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:53:57 PM : Extracting top 200 weekly for 2017-06-09--2017-06-16 - gr\n",
      "INFO : 11/02/2021 01:53:57 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:53:57 PM : Appending data to the file ../data/greece_2017.csv...\n",
      "INFO : 11/02/2021 01:53:57 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:53:57 PM : Done appending to the file ../data/greece_2017.csv!!!\n",
      "INFO : 11/02/2021 01:53:58 PM : Extracting top 200 weekly for 2017-06-16--2017-06-23 - gr\n",
      "INFO : 11/02/2021 01:53:58 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:53:58 PM : Appending data to the file ../data/greece_2017.csv...\n",
      "INFO : 11/02/2021 01:53:58 PM : Done appending to the file ../data/greece_2017.csv!!!\n",
      "INFO : 11/02/2021 01:53:58 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:53:59 PM : Extracting top 200 weekly for 2017-06-23--2017-06-30 - gr\n",
      "INFO : 11/02/2021 01:53:59 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:53:59 PM : Appending data to the file ../data/greece_2017.csv...\n",
      "INFO : 11/02/2021 01:53:59 PM : Done appending to the file ../data/greece_2017.csv!!!\n",
      "INFO : 11/02/2021 01:53:59 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:54:01 PM : Extracting top 200 weekly for 2017-06-30--2017-07-07 - gr\n",
      "INFO : 11/02/2021 01:54:01 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:54:01 PM : Appending data to the file ../data/greece_2017.csv...\n",
      "INFO : 11/02/2021 01:54:01 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:54:01 PM : Done appending to the file ../data/greece_2017.csv!!!\n",
      "INFO : 11/02/2021 01:54:02 PM : Extracting top 200 weekly for 2017-07-07--2017-07-14 - gr\n",
      "INFO : 11/02/2021 01:54:02 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:54:02 PM : Appending data to the file ../data/greece_2017.csv...\n",
      "INFO : 11/02/2021 01:54:02 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:54:02 PM : Done appending to the file ../data/greece_2017.csv!!!\n",
      "INFO : 11/02/2021 01:54:03 PM : Extracting top 200 weekly for 2017-07-14--2017-07-21 - gr\n",
      "INFO : 11/02/2021 01:54:03 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:54:03 PM : Appending data to the file ../data/greece_2017.csv...\n",
      "INFO : 11/02/2021 01:54:03 PM : Done appending to the file ../data/greece_2017.csv!!!\n",
      "INFO : 11/02/2021 01:54:03 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:54:04 PM : Extracting top 200 weekly for 2017-07-21--2017-07-28 - gr\n",
      "INFO : 11/02/2021 01:54:04 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:54:04 PM : Appending data to the file ../data/greece_2017.csv...\n",
      "INFO : 11/02/2021 01:54:04 PM : Done appending to the file ../data/greece_2017.csv!!!\n",
      "INFO : 11/02/2021 01:54:04 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:54:04 PM : Extracting top 200 weekly for 2017-07-28--2017-08-04 - gr\n",
      "INFO : 11/02/2021 01:54:04 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:54:04 PM : Appending data to the file ../data/greece_2017.csv...\n",
      "INFO : 11/02/2021 01:54:05 PM : Done appending to the file ../data/greece_2017.csv!!!\n",
      "INFO : 11/02/2021 01:54:05 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:54:05 PM : Extracting top 200 weekly for 2017-08-04--2017-08-11 - gr\n",
      "INFO : 11/02/2021 01:54:05 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:54:05 PM : Appending data to the file ../data/greece_2017.csv...\n",
      "INFO : 11/02/2021 01:54:05 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:54:05 PM : Done appending to the file ../data/greece_2017.csv!!!\n",
      "INFO : 11/02/2021 01:54:06 PM : Extracting top 200 weekly for 2017-08-11--2017-08-18 - gr\n",
      "INFO : 11/02/2021 01:54:06 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:54:06 PM : Appending data to the file ../data/greece_2017.csv...\n",
      "INFO : 11/02/2021 01:54:06 PM : Done appending to the file ../data/greece_2017.csv!!!\n",
      "INFO : 11/02/2021 01:54:06 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:54:07 PM : Extracting top 200 weekly for 2017-08-18--2017-08-25 - gr\n",
      "INFO : 11/02/2021 01:54:07 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:54:07 PM : Appending data to the file ../data/greece_2017.csv...\n",
      "INFO : 11/02/2021 01:54:07 PM : Done appending to the file ../data/greece_2017.csv!!!\n",
      "INFO : 11/02/2021 01:54:07 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:54:08 PM : Extracting top 200 weekly for 2017-08-25--2017-09-01 - gr\n",
      "INFO : 11/02/2021 01:54:08 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:54:08 PM : Appending data to the file ../data/greece_2017.csv...\n",
      "INFO : 11/02/2021 01:54:08 PM : Done appending to the file ../data/greece_2017.csv!!!\n",
      "INFO : 11/02/2021 01:54:08 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:54:09 PM : Extracting top 200 weekly for 2017-09-01--2017-09-08 - gr\n",
      "INFO : 11/02/2021 01:54:09 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:54:09 PM : Appending data to the file ../data/greece_2017.csv...\n",
      "INFO : 11/02/2021 01:54:09 PM : Done appending to the file ../data/greece_2017.csv!!!\n",
      "INFO : 11/02/2021 01:54:09 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:54:10 PM : Extracting top 200 weekly for 2017-09-08--2017-09-15 - gr\n",
      "INFO : 11/02/2021 01:54:10 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:54:10 PM : Appending data to the file ../data/greece_2017.csv...\n",
      "INFO : 11/02/2021 01:54:10 PM : Done appending to the file ../data/greece_2017.csv!!!\n",
      "INFO : 11/02/2021 01:54:10 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:54:11 PM : Extracting top 200 weekly for 2017-09-15--2017-09-22 - gr\n",
      "INFO : 11/02/2021 01:54:11 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:54:11 PM : Appending data to the file ../data/greece_2017.csv...\n",
      "INFO : 11/02/2021 01:54:11 PM : Done appending to the file ../data/greece_2017.csv!!!\n",
      "INFO : 11/02/2021 01:54:11 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:54:12 PM : Extracting top 200 weekly for 2017-09-22--2017-09-29 - gr\n",
      "INFO : 11/02/2021 01:54:12 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:54:12 PM : Appending data to the file ../data/greece_2017.csv...\n",
      "INFO : 11/02/2021 01:54:12 PM : Done appending to the file ../data/greece_2017.csv!!!\n",
      "INFO : 11/02/2021 01:54:12 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:54:13 PM : Extracting top 200 weekly for 2017-09-29--2017-10-06 - gr\n",
      "INFO : 11/02/2021 01:54:13 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:54:13 PM : Appending data to the file ../data/greece_2017.csv...\n",
      "INFO : 11/02/2021 01:54:13 PM : Done appending to the file ../data/greece_2017.csv!!!\n",
      "INFO : 11/02/2021 01:54:13 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:54:14 PM : Extracting top 200 weekly for 2017-10-06--2017-10-13 - gr\n",
      "INFO : 11/02/2021 01:54:14 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:54:14 PM : Appending data to the file ../data/greece_2017.csv...\n",
      "INFO : 11/02/2021 01:54:14 PM : Done appending to the file ../data/greece_2017.csv!!!\n",
      "INFO : 11/02/2021 01:54:14 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:54:15 PM : Extracting top 200 weekly for 2017-10-13--2017-10-20 - gr\n",
      "INFO : 11/02/2021 01:54:15 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:54:15 PM : Appending data to the file ../data/greece_2017.csv...\n",
      "INFO : 11/02/2021 01:54:15 PM : Done appending to the file ../data/greece_2017.csv!!!\n",
      "INFO : 11/02/2021 01:54:15 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:54:15 PM : Extracting top 200 weekly for 2017-10-20--2017-10-27 - gr\n",
      "INFO : 11/02/2021 01:54:15 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:54:15 PM : Appending data to the file ../data/greece_2017.csv...\n",
      "INFO : 11/02/2021 01:54:15 PM : Done appending to the file ../data/greece_2017.csv!!!\n",
      "INFO : 11/02/2021 01:54:15 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:54:16 PM : Extracting top 200 weekly for 2017-10-27--2017-11-03 - gr\n",
      "INFO : 11/02/2021 01:54:16 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:54:16 PM : Appending data to the file ../data/greece_2017.csv...\n",
      "INFO : 11/02/2021 01:54:16 PM : Done appending to the file ../data/greece_2017.csv!!!\n",
      "INFO : 11/02/2021 01:54:16 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:54:18 PM : Extracting top 200 weekly for 2017-11-03--2017-11-10 - gr\n",
      "INFO : 11/02/2021 01:54:18 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:54:18 PM : Appending data to the file ../data/greece_2017.csv...\n",
      "INFO : 11/02/2021 01:54:18 PM : Done appending to the file ../data/greece_2017.csv!!!\n",
      "INFO : 11/02/2021 01:54:18 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:54:19 PM : Extracting top 200 weekly for 2017-11-10--2017-11-17 - gr\n",
      "INFO : 11/02/2021 01:54:19 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:54:19 PM : Appending data to the file ../data/greece_2017.csv...\n",
      "INFO : 11/02/2021 01:54:19 PM : Done appending to the file ../data/greece_2017.csv!!!\n",
      "INFO : 11/02/2021 01:54:19 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:54:19 PM : Extracting top 200 weekly for 2017-11-17--2017-11-24 - gr\n",
      "INFO : 11/02/2021 01:54:19 PM : Appending data to the file ../data/greece_2017.csv...\n",
      "INFO : 11/02/2021 01:54:19 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:54:19 PM : Done appending to the file ../data/greece_2017.csv!!!\n",
      "INFO : 11/02/2021 01:54:19 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:54:20 PM : Extracting top 200 weekly for 2017-11-24--2017-12-01 - gr\n",
      "INFO : 11/02/2021 01:54:20 PM : Appending data to the file ../data/greece_2017.csv...\n",
      "INFO : 11/02/2021 01:54:20 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:54:20 PM : Done appending to the file ../data/greece_2017.csv!!!\n",
      "INFO : 11/02/2021 01:54:20 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:54:21 PM : Extracting top 200 weekly for 2017-12-01--2017-12-08 - gr\n",
      "INFO : 11/02/2021 01:54:21 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:54:21 PM : Appending data to the file ../data/greece_2017.csv...\n",
      "INFO : 11/02/2021 01:54:21 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:54:21 PM : Done appending to the file ../data/greece_2017.csv!!!\n",
      "INFO : 11/02/2021 01:54:22 PM : Extracting top 200 weekly for 2017-12-08--2017-12-15 - gr\n",
      "INFO : 11/02/2021 01:54:22 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:54:22 PM : Appending data to the file ../data/greece_2017.csv...\n",
      "INFO : 11/02/2021 01:54:22 PM : Done appending to the file ../data/greece_2017.csv!!!\n",
      "INFO : 11/02/2021 01:54:22 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:54:23 PM : Extracting top 200 weekly for 2017-12-15--2017-12-22 - gr\n",
      "INFO : 11/02/2021 01:54:23 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:54:23 PM : Appending data to the file ../data/greece_2017.csv...\n",
      "INFO : 11/02/2021 01:54:23 PM : Done appending to the file ../data/greece_2017.csv!!!\n",
      "INFO : 11/02/2021 01:54:23 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:54:24 PM : Extracting top 200 weekly for 2017-12-22--2017-12-29 - gr\n",
      "INFO : 11/02/2021 01:54:24 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:54:24 PM : Appending data to the file ../data/greece_2017.csv...\n",
      "INFO : 11/02/2021 01:54:24 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:54:24 PM : Done appending to the file ../data/greece_2017.csv!!!\n",
      "INFO : 11/02/2021 01:54:25 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:54:25 PM : Appending data to the file ../data/greece_2017.csv...\n",
      "INFO : 11/02/2021 01:54:25 PM : Done appending to the file ../data/greece_2017.csv!!!\n",
      "INFO : 11/02/2021 01:54:25 PM : Done appending to the table top_200_weekly!!!\n"
     ]
    }
   ],
   "source": [
    "api = SpotifyCharts()\n",
    "connector = sqlalchemy.create_engine(\"sqlite:///../data/greece_2017.db\", echo=False)\n",
    "api.top200Weekly(output_file = \"../data/greece_2017.csv\", output_db = connector, webhook = [\"https://mywebhookssite.com/post/\"], start = \"2017-01-01\", end = \"2017-12-31\", region = \"gr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Position</th>\n",
       "      <th>Track Name</th>\n",
       "      <th>Artist</th>\n",
       "      <th>Streams</th>\n",
       "      <th>date</th>\n",
       "      <th>region</th>\n",
       "      <th>spotify_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Shape of You</td>\n",
       "      <td>Ed Sheeran</td>\n",
       "      <td>76164</td>\n",
       "      <td>2017-01-06--2017-01-13</td>\n",
       "      <td>gr</td>\n",
       "      <td>7qiZfU4dY1lWllzX7mPBI3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Castle on the Hill</td>\n",
       "      <td>Ed Sheeran</td>\n",
       "      <td>37779</td>\n",
       "      <td>2017-01-06--2017-01-13</td>\n",
       "      <td>gr</td>\n",
       "      <td>6PCUP3dWmTjcTtXY02oFdT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Starboy</td>\n",
       "      <td>The Weeknd</td>\n",
       "      <td>34824</td>\n",
       "      <td>2017-01-06--2017-01-13</td>\n",
       "      <td>gr</td>\n",
       "      <td>5aAx2yezTd8zXrkmtKl66Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>I Don’t Wanna Live Forever (Fifty Shades Darke...</td>\n",
       "      <td>ZAYN</td>\n",
       "      <td>24781</td>\n",
       "      <td>2017-01-06--2017-01-13</td>\n",
       "      <td>gr</td>\n",
       "      <td>3NdDpSvN911VPGivFlV5d0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Rockabye (feat. Sean Paul &amp; Anne-Marie)</td>\n",
       "      <td>Clean Bandit</td>\n",
       "      <td>22380</td>\n",
       "      <td>2017-01-06--2017-01-13</td>\n",
       "      <td>gr</td>\n",
       "      <td>5knuzwU65gJK7IF5yJsuaW</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Position                                         Track Name        Artist  \\\n",
       "0         1                                       Shape of You    Ed Sheeran   \n",
       "1         2                                 Castle on the Hill    Ed Sheeran   \n",
       "2         3                                            Starboy    The Weeknd   \n",
       "3         4  I Don’t Wanna Live Forever (Fifty Shades Darke...          ZAYN   \n",
       "4         5            Rockabye (feat. Sean Paul & Anne-Marie)  Clean Bandit   \n",
       "\n",
       "   Streams                    date region              spotify_id  \n",
       "0    76164  2017-01-06--2017-01-13     gr  7qiZfU4dY1lWllzX7mPBI3  \n",
       "1    37779  2017-01-06--2017-01-13     gr  6PCUP3dWmTjcTtXY02oFdT  \n",
       "2    34824  2017-01-06--2017-01-13     gr  5aAx2yezTd8zXrkmtKl66Z  \n",
       "3    24781  2017-01-06--2017-01-13     gr  3NdDpSvN911VPGivFlV5d0  \n",
       "4    22380  2017-01-06--2017-01-13     gr  5knuzwU65gJK7IF5yJsuaW  "
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reading in csv from output file  \n",
    "greece_2017 = pd.read_csv('../data/greece_2017.csv')\n",
    "greece_2017.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Position</th>\n",
       "      <th>Track Name</th>\n",
       "      <th>Artist</th>\n",
       "      <th>Streams</th>\n",
       "      <th>date</th>\n",
       "      <th>region</th>\n",
       "      <th>spotify_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10195</th>\n",
       "      <td>196</td>\n",
       "      <td>Say You Won't Let Go</td>\n",
       "      <td>James Arthur</td>\n",
       "      <td>4639</td>\n",
       "      <td>2017-12-22--2017-12-29</td>\n",
       "      <td>gr</td>\n",
       "      <td>5uCax9HTNlzGybIStD3vDh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10196</th>\n",
       "      <td>197</td>\n",
       "      <td>Jingle Bell Rock - Daryl's Version</td>\n",
       "      <td>Daryl Hall &amp; John Oates</td>\n",
       "      <td>4638</td>\n",
       "      <td>2017-12-22--2017-12-29</td>\n",
       "      <td>gr</td>\n",
       "      <td>6pVW5LRWgeLaHudxauOTJU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10197</th>\n",
       "      <td>198</td>\n",
       "      <td>Swalla (feat. Nicki Minaj &amp; Ty Dolla $ign)</td>\n",
       "      <td>Jason Derulo</td>\n",
       "      <td>4623</td>\n",
       "      <td>2017-12-22--2017-12-29</td>\n",
       "      <td>gr</td>\n",
       "      <td>6kex4EBAj0WHXDKZMEJaaF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10198</th>\n",
       "      <td>199</td>\n",
       "      <td>Despacito - Remix</td>\n",
       "      <td>Luis Fonsi</td>\n",
       "      <td>4612</td>\n",
       "      <td>2017-12-22--2017-12-29</td>\n",
       "      <td>gr</td>\n",
       "      <td>5CtI0qwDJkDQGwXD1H1cLb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10199</th>\n",
       "      <td>200</td>\n",
       "      <td>Jocelyn Flores</td>\n",
       "      <td>XXXTENTACION</td>\n",
       "      <td>4587</td>\n",
       "      <td>2017-12-22--2017-12-29</td>\n",
       "      <td>gr</td>\n",
       "      <td>7m9OqQk4RVRkw9JJdeAw96</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Position                                  Track Name  \\\n",
       "10195       196                        Say You Won't Let Go   \n",
       "10196       197          Jingle Bell Rock - Daryl's Version   \n",
       "10197       198  Swalla (feat. Nicki Minaj & Ty Dolla $ign)   \n",
       "10198       199                           Despacito - Remix   \n",
       "10199       200                              Jocelyn Flores   \n",
       "\n",
       "                        Artist  Streams                    date region  \\\n",
       "10195             James Arthur     4639  2017-12-22--2017-12-29     gr   \n",
       "10196  Daryl Hall & John Oates     4638  2017-12-22--2017-12-29     gr   \n",
       "10197             Jason Derulo     4623  2017-12-22--2017-12-29     gr   \n",
       "10198               Luis Fonsi     4612  2017-12-22--2017-12-29     gr   \n",
       "10199             XXXTENTACION     4587  2017-12-22--2017-12-29     gr   \n",
       "\n",
       "                   spotify_id  \n",
       "10195  5uCax9HTNlzGybIStD3vDh  \n",
       "10196  6pVW5LRWgeLaHudxauOTJU  \n",
       "10197  6kex4EBAj0WHXDKZMEJaaF  \n",
       "10198  5CtI0qwDJkDQGwXD1H1cLb  \n",
       "10199  7m9OqQk4RVRkw9JJdeAw96  "
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "greece_2017.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10200 entries, 0 to 10199\n",
      "Data columns (total 7 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   Position    10200 non-null  int64 \n",
      " 1   Track Name  10196 non-null  object\n",
      " 2   Artist      10196 non-null  object\n",
      " 3   Streams     10200 non-null  int64 \n",
      " 4   date        10200 non-null  object\n",
      " 5   region      10200 non-null  object\n",
      " 6   spotify_id  10200 non-null  object\n",
      "dtypes: int64(2), object(5)\n",
      "memory usage: 557.9+ KB\n"
     ]
    }
   ],
   "source": [
    "greece_2017.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Greece Top 200 Weekly, 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : 11/02/2021 01:52:23 PM : The start date 2018-01-01 provided for top200Weekly is invalid. Wanna give one these a try? ['2018-01-05', '2018-01-12', '2018-01-19', '2018-01-26', '2018-02-02']\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter (1) to use the first suggestion, or (2) to quit and set yourself:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : 11/02/2021 01:52:29 PM : Extracting top 200 weekly for 2018-01-05--2018-01-12 - gr\n",
      "INFO : 11/02/2021 01:52:30 PM : Extracting top 200 weekly for 2018-01-12--2018-01-19 - gr\n",
      "INFO : 11/02/2021 01:52:30 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:52:30 PM : Appending data to the file ../data/greece_2018.csv...\n",
      "INFO : 11/02/2021 01:52:30 PM : POSTing data to the endpoint https://mywebhookssite.com/post/\n",
      "INFO : 11/02/2021 01:52:30 PM : Done appending to the file ../data/greece_2018.csv!!!\n",
      "INFO : 11/02/2021 01:52:30 PM : Done appending to the table top_200_weekly!!!\n",
      "Exception in thread Thread-34:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/urllib3/connection.py\", line 159, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/urllib3/util/connection.py\", line 61, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/socket.py\", line 918, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 8] nodename nor servname provided, or not known\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 670, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 381, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 978, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/urllib3/connection.py\", line 309, in connect\n",
      "    conn = self._new_conn()\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/urllib3/connection.py\", line 171, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x7f9869e7a640>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/requests/adapters.py\", line 439, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 726, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/urllib3/util/retry.py\", line 446, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='mywebhookssite.com', port=443): Max retries exceeded with url: /post/ (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f9869e7a640>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/fycharts/SpotifyCharts.py\", line 88, in __post_to_endpoint_from_queue\n",
      "    postToRestEndpoint(df, url, what_data)\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/fycharts/write_to_outputs.py\", line 61, in postToRestEndpoint\n",
      "    raise(e)\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/fycharts/write_to_outputs.py\", line 58, in postToRestEndpoint\n",
      "    requests.post(url, json = dump)\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/requests/api.py\", line 119, in post\n",
      "    return request('post', url, data=data, json=json, **kwargs)\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/requests/api.py\", line 61, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/requests/sessions.py\", line 530, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/requests/sessions.py\", line 643, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/requests/adapters.py\", line 516, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='mywebhookssite.com', port=443): Max retries exceeded with url: /post/ (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f9869e7a640>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/threading.py\", line 932, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/fycharts/SpotifyCharts.py\", line 90, in __post_to_endpoint_from_queue\n",
      "    raise RuntimeError(e)\n",
      "RuntimeError: HTTPSConnectionPool(host='mywebhookssite.com', port=443): Max retries exceeded with url: /post/ (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f9869e7a640>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\n",
      "INFO : 11/02/2021 01:52:31 PM : Extracting top 200 weekly for 2018-01-19--2018-01-26 - gr\n",
      "INFO : 11/02/2021 01:52:31 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:52:31 PM : Appending data to the file ../data/greece_2018.csv...\n",
      "INFO : 11/02/2021 01:52:31 PM : Done appending to the file ../data/greece_2018.csv!!!\n",
      "INFO : 11/02/2021 01:52:31 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:52:32 PM : Extracting top 200 weekly for 2018-01-26--2018-02-02 - gr\n",
      "INFO : 11/02/2021 01:52:32 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:52:32 PM : Appending data to the file ../data/greece_2018.csv...\n",
      "INFO : 11/02/2021 01:52:32 PM : Done appending to the file ../data/greece_2018.csv!!!\n",
      "INFO : 11/02/2021 01:52:32 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:52:33 PM : Extracting top 200 weekly for 2018-02-02--2018-02-09 - gr\n",
      "INFO : 11/02/2021 01:52:33 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:52:33 PM : Appending data to the file ../data/greece_2018.csv...\n",
      "INFO : 11/02/2021 01:52:33 PM : Done appending to the file ../data/greece_2018.csv!!!\n",
      "INFO : 11/02/2021 01:52:33 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:52:34 PM : Extracting top 200 weekly for 2018-02-09--2018-02-16 - gr\n",
      "INFO : 11/02/2021 01:52:34 PM : Appending data to the file ../data/greece_2018.csv...\n",
      "INFO : 11/02/2021 01:52:34 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:52:34 PM : Done appending to the file ../data/greece_2018.csv!!!\n",
      "INFO : 11/02/2021 01:52:34 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:52:35 PM : Extracting top 200 weekly for 2018-02-16--2018-02-23 - gr\n",
      "INFO : 11/02/2021 01:52:35 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:52:35 PM : Appending data to the file ../data/greece_2018.csv...\n",
      "INFO : 11/02/2021 01:52:35 PM : Done appending to the file ../data/greece_2018.csv!!!\n",
      "INFO : 11/02/2021 01:52:35 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:52:36 PM : Extracting top 200 weekly for 2018-02-23--2018-03-02 - gr\n",
      "INFO : 11/02/2021 01:52:36 PM : Appending data to the file ../data/greece_2018.csv...\n",
      "INFO : 11/02/2021 01:52:36 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:52:36 PM : Done appending to the file ../data/greece_2018.csv!!!\n",
      "INFO : 11/02/2021 01:52:36 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:52:37 PM : Extracting top 200 weekly for 2018-03-02--2018-03-09 - gr\n",
      "INFO : 11/02/2021 01:52:37 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:52:37 PM : Appending data to the file ../data/greece_2018.csv...\n",
      "INFO : 11/02/2021 01:52:37 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:52:37 PM : Done appending to the file ../data/greece_2018.csv!!!\n",
      "INFO : 11/02/2021 01:52:38 PM : Extracting top 200 weekly for 2018-03-09--2018-03-16 - gr\n",
      "INFO : 11/02/2021 01:52:38 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:52:38 PM : Appending data to the file ../data/greece_2018.csv...\n",
      "INFO : 11/02/2021 01:52:38 PM : Done appending to the file ../data/greece_2018.csv!!!\n",
      "INFO : 11/02/2021 01:52:38 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:52:39 PM : Extracting top 200 weekly for 2018-03-16--2018-03-23 - gr\n",
      "INFO : 11/02/2021 01:52:39 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:52:39 PM : Appending data to the file ../data/greece_2018.csv...\n",
      "INFO : 11/02/2021 01:52:39 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:52:39 PM : Done appending to the file ../data/greece_2018.csv!!!\n",
      "INFO : 11/02/2021 01:52:40 PM : Extracting top 200 weekly for 2018-03-23--2018-03-30 - gr\n",
      "INFO : 11/02/2021 01:52:40 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:52:40 PM : Appending data to the file ../data/greece_2018.csv...\n",
      "INFO : 11/02/2021 01:52:40 PM : Done appending to the file ../data/greece_2018.csv!!!\n",
      "INFO : 11/02/2021 01:52:40 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:52:41 PM : Extracting top 200 weekly for 2018-03-30--2018-04-06 - gr\n",
      "INFO : 11/02/2021 01:52:41 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:52:41 PM : Appending data to the file ../data/greece_2018.csv...\n",
      "INFO : 11/02/2021 01:52:41 PM : Done appending to the file ../data/greece_2018.csv!!!\n",
      "INFO : 11/02/2021 01:52:41 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:52:42 PM : Extracting top 200 weekly for 2018-04-06--2018-04-13 - gr\n",
      "INFO : 11/02/2021 01:52:42 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:52:42 PM : Appending data to the file ../data/greece_2018.csv...\n",
      "INFO : 11/02/2021 01:52:42 PM : Done appending to the file ../data/greece_2018.csv!!!\n",
      "INFO : 11/02/2021 01:52:42 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:52:43 PM : Extracting top 200 weekly for 2018-04-13--2018-04-20 - gr\n",
      "INFO : 11/02/2021 01:52:43 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:52:43 PM : Appending data to the file ../data/greece_2018.csv...\n",
      "INFO : 11/02/2021 01:52:43 PM : Done appending to the file ../data/greece_2018.csv!!!\n",
      "INFO : 11/02/2021 01:52:43 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:52:44 PM : Extracting top 200 weekly for 2018-04-20--2018-04-27 - gr\n",
      "INFO : 11/02/2021 01:52:44 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:52:44 PM : Appending data to the file ../data/greece_2018.csv...\n",
      "INFO : 11/02/2021 01:52:44 PM : Done appending to the file ../data/greece_2018.csv!!!\n",
      "INFO : 11/02/2021 01:52:44 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:52:45 PM : Extracting top 200 weekly for 2018-04-27--2018-05-04 - gr\n",
      "INFO : 11/02/2021 01:52:45 PM : Appending data to the file ../data/greece_2018.csv...\n",
      "INFO : 11/02/2021 01:52:45 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:52:45 PM : Done appending to the file ../data/greece_2018.csv!!!\n",
      "INFO : 11/02/2021 01:52:45 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:52:46 PM : Extracting top 200 weekly for 2018-05-04--2018-05-11 - gr\n",
      "INFO : 11/02/2021 01:52:46 PM : Appending data to the file ../data/greece_2018.csv...\n",
      "INFO : 11/02/2021 01:52:46 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:52:46 PM : Done appending to the file ../data/greece_2018.csv!!!\n",
      "INFO : 11/02/2021 01:52:46 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:52:47 PM : Extracting top 200 weekly for 2018-05-11--2018-05-18 - gr\n",
      "INFO : 11/02/2021 01:52:47 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:52:47 PM : Appending data to the file ../data/greece_2018.csv...\n",
      "INFO : 11/02/2021 01:52:47 PM : Done appending to the file ../data/greece_2018.csv!!!\n",
      "INFO : 11/02/2021 01:52:47 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:52:48 PM : Extracting top 200 weekly for 2018-05-18--2018-05-25 - gr\n",
      "INFO : 11/02/2021 01:52:48 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:52:48 PM : Appending data to the file ../data/greece_2018.csv...\n",
      "INFO : 11/02/2021 01:52:48 PM : Done appending to the file ../data/greece_2018.csv!!!\n",
      "INFO : 11/02/2021 01:52:48 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:52:50 PM : Extracting top 200 weekly for 2018-05-25--2018-06-01 - gr\n",
      "INFO : 11/02/2021 01:52:50 PM : Appending data to the file ../data/greece_2018.csv...\n",
      "INFO : 11/02/2021 01:52:50 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:52:50 PM : Done appending to the file ../data/greece_2018.csv!!!\n",
      "INFO : 11/02/2021 01:52:50 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:52:51 PM : Extracting top 200 weekly for 2018-06-01--2018-06-08 - gr\n",
      "INFO : 11/02/2021 01:52:51 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:52:51 PM : Appending data to the file ../data/greece_2018.csv...\n",
      "INFO : 11/02/2021 01:52:51 PM : Done appending to the file ../data/greece_2018.csv!!!\n",
      "INFO : 11/02/2021 01:52:51 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:52:52 PM : Extracting top 200 weekly for 2018-06-08--2018-06-15 - gr\n",
      "INFO : 11/02/2021 01:52:52 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:52:52 PM : Appending data to the file ../data/greece_2018.csv...\n",
      "INFO : 11/02/2021 01:52:52 PM : Done appending to the file ../data/greece_2018.csv!!!\n",
      "INFO : 11/02/2021 01:52:52 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:52:53 PM : Extracting top 200 weekly for 2018-06-15--2018-06-22 - gr\n",
      "INFO : 11/02/2021 01:52:53 PM : Appending data to the file ../data/greece_2018.csv...\n",
      "INFO : 11/02/2021 01:52:53 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:52:53 PM : Done appending to the file ../data/greece_2018.csv!!!\n",
      "INFO : 11/02/2021 01:52:53 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:52:54 PM : Extracting top 200 weekly for 2018-06-22--2018-06-29 - gr\n",
      "INFO : 11/02/2021 01:52:54 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:52:54 PM : Appending data to the file ../data/greece_2018.csv...\n",
      "INFO : 11/02/2021 01:52:54 PM : Done appending to the file ../data/greece_2018.csv!!!\n",
      "INFO : 11/02/2021 01:52:54 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:52:55 PM : Extracting top 200 weekly for 2018-06-29--2018-07-06 - gr\n",
      "INFO : 11/02/2021 01:52:55 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:52:55 PM : Appending data to the file ../data/greece_2018.csv...\n",
      "INFO : 11/02/2021 01:52:55 PM : Done appending to the file ../data/greece_2018.csv!!!\n",
      "INFO : 11/02/2021 01:52:55 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:52:56 PM : Extracting top 200 weekly for 2018-07-06--2018-07-13 - gr\n",
      "INFO : 11/02/2021 01:52:56 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:52:56 PM : Appending data to the file ../data/greece_2018.csv...\n",
      "INFO : 11/02/2021 01:52:56 PM : Done appending to the file ../data/greece_2018.csv!!!\n",
      "INFO : 11/02/2021 01:52:56 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:52:57 PM : Extracting top 200 weekly for 2018-07-13--2018-07-20 - gr\n",
      "INFO : 11/02/2021 01:52:57 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:52:57 PM : Appending data to the file ../data/greece_2018.csv...\n",
      "INFO : 11/02/2021 01:52:57 PM : Done appending to the file ../data/greece_2018.csv!!!\n",
      "INFO : 11/02/2021 01:52:57 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:52:58 PM : Extracting top 200 weekly for 2018-07-20--2018-07-27 - gr\n",
      "INFO : 11/02/2021 01:52:58 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:52:58 PM : Appending data to the file ../data/greece_2018.csv...\n",
      "INFO : 11/02/2021 01:52:58 PM : Done appending to the file ../data/greece_2018.csv!!!\n",
      "INFO : 11/02/2021 01:52:58 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:52:59 PM : Extracting top 200 weekly for 2018-07-27--2018-08-03 - gr\n",
      "INFO : 11/02/2021 01:52:59 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:52:59 PM : Appending data to the file ../data/greece_2018.csv...\n",
      "INFO : 11/02/2021 01:52:59 PM : Done appending to the file ../data/greece_2018.csv!!!\n",
      "INFO : 11/02/2021 01:52:59 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:53:00 PM : Extracting top 200 weekly for 2018-08-03--2018-08-10 - gr\n",
      "INFO : 11/02/2021 01:53:00 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:53:00 PM : Appending data to the file ../data/greece_2018.csv...\n",
      "INFO : 11/02/2021 01:53:00 PM : Done appending to the file ../data/greece_2018.csv!!!\n",
      "INFO : 11/02/2021 01:53:00 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:53:01 PM : Extracting top 200 weekly for 2018-08-10--2018-08-17 - gr\n",
      "INFO : 11/02/2021 01:53:01 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:53:01 PM : Appending data to the file ../data/greece_2018.csv...\n",
      "INFO : 11/02/2021 01:53:01 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:53:01 PM : Done appending to the file ../data/greece_2018.csv!!!\n",
      "INFO : 11/02/2021 01:53:02 PM : Extracting top 200 weekly for 2018-08-17--2018-08-24 - gr\n",
      "INFO : 11/02/2021 01:53:02 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:53:02 PM : Appending data to the file ../data/greece_2018.csv...\n",
      "INFO : 11/02/2021 01:53:02 PM : Done appending to the file ../data/greece_2018.csv!!!\n",
      "INFO : 11/02/2021 01:53:02 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:53:03 PM : Extracting top 200 weekly for 2018-08-24--2018-08-31 - gr\n",
      "INFO : 11/02/2021 01:53:03 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:53:03 PM : Appending data to the file ../data/greece_2018.csv...\n",
      "INFO : 11/02/2021 01:53:03 PM : Done appending to the file ../data/greece_2018.csv!!!\n",
      "INFO : 11/02/2021 01:53:03 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:53:04 PM : Extracting top 200 weekly for 2018-08-31--2018-09-07 - gr\n",
      "INFO : 11/02/2021 01:53:04 PM : Appending data to the file ../data/greece_2018.csv...\n",
      "INFO : 11/02/2021 01:53:04 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:53:04 PM : Done appending to the file ../data/greece_2018.csv!!!\n",
      "INFO : 11/02/2021 01:53:04 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:53:05 PM : Extracting top 200 weekly for 2018-09-07--2018-09-14 - gr\n",
      "INFO : 11/02/2021 01:53:05 PM : Appending data to the file ../data/greece_2018.csv...\n",
      "INFO : 11/02/2021 01:53:05 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:53:05 PM : Done appending to the file ../data/greece_2018.csv!!!\n",
      "INFO : 11/02/2021 01:53:05 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:53:06 PM : Extracting top 200 weekly for 2018-09-14--2018-09-21 - gr\n",
      "INFO : 11/02/2021 01:53:06 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:53:06 PM : Appending data to the file ../data/greece_2018.csv...\n",
      "INFO : 11/02/2021 01:53:06 PM : Done appending to the file ../data/greece_2018.csv!!!\n",
      "INFO : 11/02/2021 01:53:06 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:53:08 PM : Extracting top 200 weekly for 2018-09-21--2018-09-28 - gr\n",
      "INFO : 11/02/2021 01:53:08 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:53:08 PM : Appending data to the file ../data/greece_2018.csv...\n",
      "INFO : 11/02/2021 01:53:08 PM : Done appending to the file ../data/greece_2018.csv!!!\n",
      "INFO : 11/02/2021 01:53:08 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:53:09 PM : Extracting top 200 weekly for 2018-09-28--2018-10-05 - gr\n",
      "INFO : 11/02/2021 01:53:09 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:53:09 PM : Appending data to the file ../data/greece_2018.csv...\n",
      "INFO : 11/02/2021 01:53:09 PM : Done appending to the file ../data/greece_2018.csv!!!\n",
      "INFO : 11/02/2021 01:53:09 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:53:10 PM : Extracting top 200 weekly for 2018-10-05--2018-10-12 - gr\n",
      "INFO : 11/02/2021 01:53:10 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:53:10 PM : Appending data to the file ../data/greece_2018.csv...\n",
      "INFO : 11/02/2021 01:53:10 PM : Done appending to the file ../data/greece_2018.csv!!!\n",
      "INFO : 11/02/2021 01:53:10 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:53:11 PM : Extracting top 200 weekly for 2018-10-12--2018-10-19 - gr\n",
      "INFO : 11/02/2021 01:53:11 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:53:11 PM : Appending data to the file ../data/greece_2018.csv...\n",
      "INFO : 11/02/2021 01:53:11 PM : Done appending to the file ../data/greece_2018.csv!!!\n",
      "INFO : 11/02/2021 01:53:11 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:53:12 PM : Extracting top 200 weekly for 2018-10-19--2018-10-26 - gr\n",
      "INFO : 11/02/2021 01:53:12 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:53:12 PM : Appending data to the file ../data/greece_2018.csv...\n",
      "INFO : 11/02/2021 01:53:12 PM : Done appending to the file ../data/greece_2018.csv!!!\n",
      "INFO : 11/02/2021 01:53:12 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:53:13 PM : Extracting top 200 weekly for 2018-10-26--2018-11-02 - gr\n",
      "INFO : 11/02/2021 01:53:13 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:53:13 PM : Appending data to the file ../data/greece_2018.csv...\n",
      "INFO : 11/02/2021 01:53:13 PM : Done appending to the file ../data/greece_2018.csv!!!\n",
      "INFO : 11/02/2021 01:53:13 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:53:15 PM : Extracting top 200 weekly for 2018-11-02--2018-11-09 - gr\n",
      "INFO : 11/02/2021 01:53:15 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:53:15 PM : Appending data to the file ../data/greece_2018.csv...\n",
      "INFO : 11/02/2021 01:53:15 PM : Done appending to the file ../data/greece_2018.csv!!!\n",
      "INFO : 11/02/2021 01:53:15 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:53:15 PM : Extracting top 200 weekly for 2018-11-09--2018-11-16 - gr\n",
      "INFO : 11/02/2021 01:53:15 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:53:15 PM : Appending data to the file ../data/greece_2018.csv...\n",
      "INFO : 11/02/2021 01:53:15 PM : Done appending to the file ../data/greece_2018.csv!!!\n",
      "INFO : 11/02/2021 01:53:15 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:53:17 PM : Extracting top 200 weekly for 2018-11-16--2018-11-23 - gr\n",
      "INFO : 11/02/2021 01:53:17 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:53:17 PM : Appending data to the file ../data/greece_2018.csv...\n",
      "INFO : 11/02/2021 01:53:17 PM : Done appending to the file ../data/greece_2018.csv!!!\n",
      "INFO : 11/02/2021 01:53:17 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:53:18 PM : Extracting top 200 weekly for 2018-11-23--2018-11-30 - gr\n",
      "INFO : 11/02/2021 01:53:18 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:53:18 PM : Appending data to the file ../data/greece_2018.csv...\n",
      "INFO : 11/02/2021 01:53:18 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:53:18 PM : Done appending to the file ../data/greece_2018.csv!!!\n",
      "INFO : 11/02/2021 01:53:19 PM : Extracting top 200 weekly for 2018-11-30--2018-12-07 - gr\n",
      "INFO : 11/02/2021 01:53:19 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:53:19 PM : Appending data to the file ../data/greece_2018.csv...\n",
      "INFO : 11/02/2021 01:53:19 PM : Done appending to the file ../data/greece_2018.csv!!!\n",
      "INFO : 11/02/2021 01:53:19 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:53:20 PM : Extracting top 200 weekly for 2018-12-07--2018-12-14 - gr\n",
      "INFO : 11/02/2021 01:53:20 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:53:20 PM : Appending data to the file ../data/greece_2018.csv...\n",
      "INFO : 11/02/2021 01:53:20 PM : Done appending to the file ../data/greece_2018.csv!!!\n",
      "INFO : 11/02/2021 01:53:20 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:53:21 PM : Extracting top 200 weekly for 2018-12-14--2018-12-21 - gr\n",
      "INFO : 11/02/2021 01:53:21 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:53:21 PM : Appending data to the file ../data/greece_2018.csv...\n",
      "INFO : 11/02/2021 01:53:21 PM : Done appending to the file ../data/greece_2018.csv!!!\n",
      "INFO : 11/02/2021 01:53:21 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:53:22 PM : Extracting top 200 weekly for 2018-12-21--2018-12-28 - gr\n",
      "INFO : 11/02/2021 01:53:22 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:53:22 PM : Appending data to the file ../data/greece_2018.csv...\n",
      "INFO : 11/02/2021 01:53:22 PM : Done appending to the file ../data/greece_2018.csv!!!\n",
      "INFO : 11/02/2021 01:53:22 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:53:23 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:53:23 PM : Appending data to the file ../data/greece_2018.csv...\n",
      "INFO : 11/02/2021 01:53:23 PM : Done appending to the file ../data/greece_2018.csv!!!\n"
     ]
    }
   ],
   "source": [
    "api = SpotifyCharts()\n",
    "connector = sqlalchemy.create_engine(\"sqlite:///../data/greece_2018.db\", echo=False)\n",
    "api.top200Weekly(output_file = \"../data/greece_2018.csv\", output_db = connector, webhook = [\"https://mywebhookssite.com/post/\"], start = \"2018-01-01\", end = \"2018-12-31\", region = \"gr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : 11/02/2021 01:53:23 PM : Done appending to the table top_200_weekly!!!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Position</th>\n",
       "      <th>Track Name</th>\n",
       "      <th>Artist</th>\n",
       "      <th>Streams</th>\n",
       "      <th>date</th>\n",
       "      <th>region</th>\n",
       "      <th>spotify_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10195</th>\n",
       "      <td>196</td>\n",
       "      <td>I'll Be Home for Christmas</td>\n",
       "      <td>Michael Bublé</td>\n",
       "      <td>7831</td>\n",
       "      <td>2018-12-21--2018-12-28</td>\n",
       "      <td>gr</td>\n",
       "      <td>0tXPhc8LvM4dPvoRwI66XQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10196</th>\n",
       "      <td>197</td>\n",
       "      <td>In My Head</td>\n",
       "      <td>Peter Manos</td>\n",
       "      <td>7826</td>\n",
       "      <td>2018-12-21--2018-12-28</td>\n",
       "      <td>gr</td>\n",
       "      <td>1tT55K6VEyO6XFDxK4lDQe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10197</th>\n",
       "      <td>198</td>\n",
       "      <td>2002</td>\n",
       "      <td>Anne-Marie</td>\n",
       "      <td>7822</td>\n",
       "      <td>2018-12-21--2018-12-28</td>\n",
       "      <td>gr</td>\n",
       "      <td>2BgEsaKNfHUdlh97KmvFyo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10198</th>\n",
       "      <td>199</td>\n",
       "      <td>Splashin</td>\n",
       "      <td>Rich The Kid</td>\n",
       "      <td>7744</td>\n",
       "      <td>2018-12-21--2018-12-28</td>\n",
       "      <td>gr</td>\n",
       "      <td>79OEIr4J4FHV0O3KrhaXRb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10199</th>\n",
       "      <td>200</td>\n",
       "      <td>Nevermind</td>\n",
       "      <td>Dennis Lloyd</td>\n",
       "      <td>7701</td>\n",
       "      <td>2018-12-21--2018-12-28</td>\n",
       "      <td>gr</td>\n",
       "      <td>63SevszngYpZOwf63o61K4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Position                  Track Name         Artist  Streams  \\\n",
       "10195       196  I'll Be Home for Christmas  Michael Bublé     7831   \n",
       "10196       197                  In My Head    Peter Manos     7826   \n",
       "10197       198                        2002     Anne-Marie     7822   \n",
       "10198       199                    Splashin   Rich The Kid     7744   \n",
       "10199       200                   Nevermind   Dennis Lloyd     7701   \n",
       "\n",
       "                         date region              spotify_id  \n",
       "10195  2018-12-21--2018-12-28     gr  0tXPhc8LvM4dPvoRwI66XQ  \n",
       "10196  2018-12-21--2018-12-28     gr  1tT55K6VEyO6XFDxK4lDQe  \n",
       "10197  2018-12-21--2018-12-28     gr  2BgEsaKNfHUdlh97KmvFyo  \n",
       "10198  2018-12-21--2018-12-28     gr  79OEIr4J4FHV0O3KrhaXRb  \n",
       "10199  2018-12-21--2018-12-28     gr  63SevszngYpZOwf63o61K4  "
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reading in csv from output file  \n",
    "greece_2018 = pd.read_csv('../data/greece_2018.csv')\n",
    "greece_2018.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Greece Top 200 Weekly, 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : 11/02/2021 01:57:13 PM : The start date 2019-01-01 provided for top200Weekly is invalid. Wanna give one these a try? ['2019-01-04', '2019-01-11', '2019-01-18', '2019-01-25', '2019-02-01']\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter (1) to use the first suggestion, or (2) to quit and set yourself:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : 11/02/2021 01:57:17 PM : Extracting top 200 weekly for 2019-01-04--2019-01-11 - gr\n",
      "INFO : 11/02/2021 01:57:18 PM : Extracting top 200 weekly for 2019-01-11--2019-01-18 - gr\n",
      "INFO : 11/02/2021 01:57:18 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:57:18 PM : POSTing data to the endpoint https://mywebhookssite.com/post/\n",
      "INFO : 11/02/2021 01:57:18 PM : Appending data to the file ../data/greece_2019.csv...\n",
      "INFO : 11/02/2021 01:57:18 PM : Done appending to the file ../data/greece_2019.csv!!!\n",
      "INFO : 11/02/2021 01:57:18 PM : Done appending to the table top_200_weekly!!!\n",
      "Exception in thread Thread-40:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/urllib3/connection.py\", line 159, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/urllib3/util/connection.py\", line 61, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/socket.py\", line 918, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 8] nodename nor servname provided, or not known\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 670, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 381, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 978, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/urllib3/connection.py\", line 309, in connect\n",
      "    conn = self._new_conn()\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/urllib3/connection.py\", line 171, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x7f9869f5faf0>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/requests/adapters.py\", line 439, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 726, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/urllib3/util/retry.py\", line 446, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='mywebhookssite.com', port=443): Max retries exceeded with url: /post/ (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f9869f5faf0>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/fycharts/SpotifyCharts.py\", line 88, in __post_to_endpoint_from_queue\n",
      "    postToRestEndpoint(df, url, what_data)\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/fycharts/write_to_outputs.py\", line 61, in postToRestEndpoint\n",
      "    raise(e)\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/fycharts/write_to_outputs.py\", line 58, in postToRestEndpoint\n",
      "    requests.post(url, json = dump)\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/requests/api.py\", line 119, in post\n",
      "    return request('post', url, data=data, json=json, **kwargs)\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/requests/api.py\", line 61, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/requests/sessions.py\", line 530, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/requests/sessions.py\", line 643, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/requests/adapters.py\", line 516, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='mywebhookssite.com', port=443): Max retries exceeded with url: /post/ (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f9869f5faf0>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/threading.py\", line 932, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/fycharts/SpotifyCharts.py\", line 90, in __post_to_endpoint_from_queue\n",
      "    raise RuntimeError(e)\n",
      "RuntimeError: HTTPSConnectionPool(host='mywebhookssite.com', port=443): Max retries exceeded with url: /post/ (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f9869f5faf0>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\n",
      "INFO : 11/02/2021 01:57:19 PM : Extracting top 200 weekly for 2019-01-18--2019-01-25 - gr\n",
      "INFO : 11/02/2021 01:57:19 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:57:19 PM : Appending data to the file ../data/greece_2019.csv...\n",
      "INFO : 11/02/2021 01:57:19 PM : Done appending to the file ../data/greece_2019.csv!!!\n",
      "INFO : 11/02/2021 01:57:19 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:57:20 PM : Extracting top 200 weekly for 2019-01-25--2019-02-01 - gr\n",
      "INFO : 11/02/2021 01:57:20 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:57:20 PM : Appending data to the file ../data/greece_2019.csv...\n",
      "INFO : 11/02/2021 01:57:20 PM : Done appending to the file ../data/greece_2019.csv!!!\n",
      "INFO : 11/02/2021 01:57:20 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:57:21 PM : Extracting top 200 weekly for 2019-02-01--2019-02-08 - gr\n",
      "INFO : 11/02/2021 01:57:21 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:57:21 PM : Appending data to the file ../data/greece_2019.csv...\n",
      "INFO : 11/02/2021 01:57:21 PM : Done appending to the file ../data/greece_2019.csv!!!\n",
      "INFO : 11/02/2021 01:57:21 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:57:21 PM : Extracting top 200 weekly for 2019-02-08--2019-02-15 - gr\n",
      "INFO : 11/02/2021 01:57:21 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:57:21 PM : Appending data to the file ../data/greece_2019.csv...\n",
      "INFO : 11/02/2021 01:57:21 PM : Done appending to the file ../data/greece_2019.csv!!!\n",
      "INFO : 11/02/2021 01:57:21 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:57:22 PM : Extracting top 200 weekly for 2019-02-15--2019-02-22 - gr\n",
      "INFO : 11/02/2021 01:57:22 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:57:22 PM : Appending data to the file ../data/greece_2019.csv...\n",
      "INFO : 11/02/2021 01:57:22 PM : Done appending to the file ../data/greece_2019.csv!!!\n",
      "INFO : 11/02/2021 01:57:22 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:57:24 PM : Extracting top 200 weekly for 2019-02-22--2019-03-01 - gr\n",
      "INFO : 11/02/2021 01:57:24 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:57:24 PM : Appending data to the file ../data/greece_2019.csv...\n",
      "INFO : 11/02/2021 01:57:24 PM : Done appending to the file ../data/greece_2019.csv!!!\n",
      "INFO : 11/02/2021 01:57:24 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:57:24 PM : Extracting top 200 weekly for 2019-03-01--2019-03-08 - gr\n",
      "INFO : 11/02/2021 01:57:24 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:57:24 PM : Appending data to the file ../data/greece_2019.csv...\n",
      "INFO : 11/02/2021 01:57:24 PM : Done appending to the file ../data/greece_2019.csv!!!\n",
      "INFO : 11/02/2021 01:57:24 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:57:25 PM : Extracting top 200 weekly for 2019-03-08--2019-03-15 - gr\n",
      "INFO : 11/02/2021 01:57:25 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:57:25 PM : Appending data to the file ../data/greece_2019.csv...\n",
      "INFO : 11/02/2021 01:57:25 PM : Done appending to the file ../data/greece_2019.csv!!!\n",
      "INFO : 11/02/2021 01:57:25 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:57:26 PM : Extracting top 200 weekly for 2019-03-15--2019-03-22 - gr\n",
      "INFO : 11/02/2021 01:57:26 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:57:26 PM : Appending data to the file ../data/greece_2019.csv...\n",
      "INFO : 11/02/2021 01:57:26 PM : Done appending to the file ../data/greece_2019.csv!!!\n",
      "INFO : 11/02/2021 01:57:26 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:57:27 PM : Extracting top 200 weekly for 2019-03-22--2019-03-29 - gr\n",
      "INFO : 11/02/2021 01:57:27 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:57:27 PM : Appending data to the file ../data/greece_2019.csv...\n",
      "INFO : 11/02/2021 01:57:27 PM : Done appending to the file ../data/greece_2019.csv!!!\n",
      "INFO : 11/02/2021 01:57:27 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:57:28 PM : Extracting top 200 weekly for 2019-03-29--2019-04-05 - gr\n",
      "INFO : 11/02/2021 01:57:28 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:57:28 PM : Appending data to the file ../data/greece_2019.csv...\n",
      "INFO : 11/02/2021 01:57:28 PM : Done appending to the file ../data/greece_2019.csv!!!\n",
      "INFO : 11/02/2021 01:57:28 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:57:29 PM : Extracting top 200 weekly for 2019-04-05--2019-04-12 - gr\n",
      "INFO : 11/02/2021 01:57:29 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:57:29 PM : Appending data to the file ../data/greece_2019.csv...\n",
      "INFO : 11/02/2021 01:57:29 PM : Done appending to the file ../data/greece_2019.csv!!!\n",
      "INFO : 11/02/2021 01:57:29 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:57:30 PM : Extracting top 200 weekly for 2019-04-12--2019-04-19 - gr\n",
      "INFO : 11/02/2021 01:57:30 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:57:30 PM : Appending data to the file ../data/greece_2019.csv...\n",
      "INFO : 11/02/2021 01:57:30 PM : Done appending to the file ../data/greece_2019.csv!!!\n",
      "INFO : 11/02/2021 01:57:30 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:57:30 PM : Extracting top 200 weekly for 2019-04-19--2019-04-26 - gr\n",
      "INFO : 11/02/2021 01:57:30 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:57:30 PM : Appending data to the file ../data/greece_2019.csv...\n",
      "INFO : 11/02/2021 01:57:30 PM : Done appending to the file ../data/greece_2019.csv!!!\n",
      "INFO : 11/02/2021 01:57:30 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:57:31 PM : Extracting top 200 weekly for 2019-04-26--2019-05-03 - gr\n",
      "INFO : 11/02/2021 01:57:31 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:57:31 PM : Appending data to the file ../data/greece_2019.csv...\n",
      "INFO : 11/02/2021 01:57:31 PM : Done appending to the file ../data/greece_2019.csv!!!\n",
      "INFO : 11/02/2021 01:57:31 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:57:32 PM : Extracting top 200 weekly for 2019-05-03--2019-05-10 - gr\n",
      "INFO : 11/02/2021 01:57:32 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:57:32 PM : Appending data to the file ../data/greece_2019.csv...\n",
      "INFO : 11/02/2021 01:57:32 PM : Done appending to the file ../data/greece_2019.csv!!!\n",
      "INFO : 11/02/2021 01:57:32 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:57:33 PM : Extracting top 200 weekly for 2019-05-10--2019-05-17 - gr\n",
      "INFO : 11/02/2021 01:57:33 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:57:33 PM : Appending data to the file ../data/greece_2019.csv...\n",
      "INFO : 11/02/2021 01:57:33 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:57:33 PM : Done appending to the file ../data/greece_2019.csv!!!\n",
      "INFO : 11/02/2021 01:57:34 PM : Extracting top 200 weekly for 2019-05-17--2019-05-24 - gr\n",
      "INFO : 11/02/2021 01:57:34 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:57:34 PM : Appending data to the file ../data/greece_2019.csv...\n",
      "INFO : 11/02/2021 01:57:34 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:57:34 PM : Done appending to the file ../data/greece_2019.csv!!!\n",
      "INFO : 11/02/2021 01:57:34 PM : Extracting top 200 weekly for 2019-05-24--2019-05-31 - gr\n",
      "INFO : 11/02/2021 01:57:34 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:57:34 PM : Appending data to the file ../data/greece_2019.csv...\n",
      "INFO : 11/02/2021 01:57:34 PM : Done appending to the file ../data/greece_2019.csv!!!\n",
      "INFO : 11/02/2021 01:57:34 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:57:35 PM : Extracting top 200 weekly for 2019-05-31--2019-06-07 - gr\n",
      "INFO : 11/02/2021 01:57:35 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:57:35 PM : Appending data to the file ../data/greece_2019.csv...\n",
      "INFO : 11/02/2021 01:57:35 PM : Done appending to the file ../data/greece_2019.csv!!!\n",
      "INFO : 11/02/2021 01:57:35 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:57:36 PM : Extracting top 200 weekly for 2019-06-07--2019-06-14 - gr\n",
      "INFO : 11/02/2021 01:57:36 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:57:36 PM : Appending data to the file ../data/greece_2019.csv...\n",
      "INFO : 11/02/2021 01:57:36 PM : Done appending to the file ../data/greece_2019.csv!!!\n",
      "INFO : 11/02/2021 01:57:36 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:57:37 PM : Extracting top 200 weekly for 2019-06-14--2019-06-21 - gr\n",
      "INFO : 11/02/2021 01:57:37 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:57:37 PM : Appending data to the file ../data/greece_2019.csv...\n",
      "INFO : 11/02/2021 01:57:37 PM : Done appending to the file ../data/greece_2019.csv!!!\n",
      "INFO : 11/02/2021 01:57:37 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:57:38 PM : Extracting top 200 weekly for 2019-06-21--2019-06-28 - gr\n",
      "INFO : 11/02/2021 01:57:38 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:57:38 PM : Appending data to the file ../data/greece_2019.csv...\n",
      "INFO : 11/02/2021 01:57:38 PM : Done appending to the file ../data/greece_2019.csv!!!\n",
      "INFO : 11/02/2021 01:57:38 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:57:39 PM : Extracting top 200 weekly for 2019-06-28--2019-07-05 - gr\n",
      "INFO : 11/02/2021 01:57:39 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:57:39 PM : Appending data to the file ../data/greece_2019.csv...\n",
      "INFO : 11/02/2021 01:57:39 PM : Done appending to the file ../data/greece_2019.csv!!!\n",
      "INFO : 11/02/2021 01:57:39 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:57:40 PM : Extracting top 200 weekly for 2019-07-05--2019-07-12 - gr\n",
      "INFO : 11/02/2021 01:57:40 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:57:40 PM : Appending data to the file ../data/greece_2019.csv...\n",
      "INFO : 11/02/2021 01:57:40 PM : Done appending to the file ../data/greece_2019.csv!!!\n",
      "INFO : 11/02/2021 01:57:40 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:57:40 PM : Extracting top 200 weekly for 2019-07-12--2019-07-19 - gr\n",
      "INFO : 11/02/2021 01:57:40 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:57:40 PM : Appending data to the file ../data/greece_2019.csv...\n",
      "INFO : 11/02/2021 01:57:40 PM : Done appending to the file ../data/greece_2019.csv!!!\n",
      "INFO : 11/02/2021 01:57:40 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:57:41 PM : Extracting top 200 weekly for 2019-07-19--2019-07-26 - gr\n",
      "INFO : 11/02/2021 01:57:41 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:57:41 PM : Appending data to the file ../data/greece_2019.csv...\n",
      "INFO : 11/02/2021 01:57:41 PM : Done appending to the file ../data/greece_2019.csv!!!\n",
      "INFO : 11/02/2021 01:57:41 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:57:42 PM : Extracting top 200 weekly for 2019-07-26--2019-08-02 - gr\n",
      "INFO : 11/02/2021 01:57:42 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:57:42 PM : Appending data to the file ../data/greece_2019.csv...\n",
      "INFO : 11/02/2021 01:57:42 PM : Done appending to the file ../data/greece_2019.csv!!!\n",
      "INFO : 11/02/2021 01:57:42 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:57:43 PM : Extracting top 200 weekly for 2019-08-02--2019-08-09 - gr\n",
      "INFO : 11/02/2021 01:57:43 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:57:43 PM : Appending data to the file ../data/greece_2019.csv...\n",
      "INFO : 11/02/2021 01:57:43 PM : Done appending to the file ../data/greece_2019.csv!!!\n",
      "INFO : 11/02/2021 01:57:43 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:57:44 PM : Extracting top 200 weekly for 2019-08-09--2019-08-16 - gr\n",
      "INFO : 11/02/2021 01:57:44 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:57:44 PM : Appending data to the file ../data/greece_2019.csv...\n",
      "INFO : 11/02/2021 01:57:44 PM : Done appending to the file ../data/greece_2019.csv!!!\n",
      "INFO : 11/02/2021 01:57:44 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:57:44 PM : Extracting top 200 weekly for 2019-08-16--2019-08-23 - gr\n",
      "INFO : 11/02/2021 01:57:44 PM : Appending data to the file ../data/greece_2019.csv...\n",
      "INFO : 11/02/2021 01:57:44 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:57:44 PM : Done appending to the file ../data/greece_2019.csv!!!\n",
      "INFO : 11/02/2021 01:57:44 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:57:45 PM : Extracting top 200 weekly for 2019-08-23--2019-08-30 - gr\n",
      "INFO : 11/02/2021 01:57:45 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:57:45 PM : Appending data to the file ../data/greece_2019.csv...\n",
      "INFO : 11/02/2021 01:57:45 PM : Done appending to the file ../data/greece_2019.csv!!!\n",
      "INFO : 11/02/2021 01:57:45 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:57:46 PM : Extracting top 200 weekly for 2019-08-30--2019-09-06 - gr\n",
      "INFO : 11/02/2021 01:57:46 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:57:46 PM : Appending data to the file ../data/greece_2019.csv...\n",
      "INFO : 11/02/2021 01:57:46 PM : Done appending to the file ../data/greece_2019.csv!!!\n",
      "INFO : 11/02/2021 01:57:46 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:57:47 PM : Extracting top 200 weekly for 2019-09-06--2019-09-13 - gr\n",
      "INFO : 11/02/2021 01:57:47 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:57:47 PM : Appending data to the file ../data/greece_2019.csv...\n",
      "INFO : 11/02/2021 01:57:47 PM : Done appending to the file ../data/greece_2019.csv!!!\n",
      "INFO : 11/02/2021 01:57:47 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:57:48 PM : Extracting top 200 weekly for 2019-09-13--2019-09-20 - gr\n",
      "INFO : 11/02/2021 01:57:48 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:57:48 PM : Appending data to the file ../data/greece_2019.csv...\n",
      "INFO : 11/02/2021 01:57:48 PM : Done appending to the file ../data/greece_2019.csv!!!\n",
      "INFO : 11/02/2021 01:57:48 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:57:49 PM : Extracting top 200 weekly for 2019-09-20--2019-09-27 - gr\n",
      "INFO : 11/02/2021 01:57:49 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:57:49 PM : Appending data to the file ../data/greece_2019.csv...\n",
      "INFO : 11/02/2021 01:57:49 PM : Done appending to the file ../data/greece_2019.csv!!!\n",
      "INFO : 11/02/2021 01:57:49 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:57:50 PM : Extracting top 200 weekly for 2019-09-27--2019-10-04 - gr\n",
      "INFO : 11/02/2021 01:57:50 PM : Appending data to the file ../data/greece_2019.csv...\n",
      "INFO : 11/02/2021 01:57:50 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:57:50 PM : Done appending to the file ../data/greece_2019.csv!!!\n",
      "INFO : 11/02/2021 01:57:50 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:57:51 PM : Extracting top 200 weekly for 2019-10-04--2019-10-11 - gr\n",
      "INFO : 11/02/2021 01:57:51 PM : Appending data to the file ../data/greece_2019.csv...\n",
      "INFO : 11/02/2021 01:57:51 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:57:51 PM : Done appending to the file ../data/greece_2019.csv!!!\n",
      "INFO : 11/02/2021 01:57:51 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:57:52 PM : Extracting top 200 weekly for 2019-10-11--2019-10-18 - gr\n",
      "INFO : 11/02/2021 01:57:52 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:57:52 PM : Appending data to the file ../data/greece_2019.csv...\n",
      "INFO : 11/02/2021 01:57:52 PM : Done appending to the file ../data/greece_2019.csv!!!\n",
      "INFO : 11/02/2021 01:57:52 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:57:53 PM : Extracting top 200 weekly for 2019-10-18--2019-10-25 - gr\n",
      "INFO : 11/02/2021 01:57:53 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:57:53 PM : Appending data to the file ../data/greece_2019.csv...\n",
      "INFO : 11/02/2021 01:57:53 PM : Done appending to the file ../data/greece_2019.csv!!!\n",
      "INFO : 11/02/2021 01:57:53 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:57:53 PM : Extracting top 200 weekly for 2019-10-25--2019-11-01 - gr\n",
      "INFO : 11/02/2021 01:57:53 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:57:53 PM : Appending data to the file ../data/greece_2019.csv...\n",
      "INFO : 11/02/2021 01:57:53 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:57:53 PM : Done appending to the file ../data/greece_2019.csv!!!\n",
      "INFO : 11/02/2021 01:57:54 PM : Extracting top 200 weekly for 2019-11-01--2019-11-08 - gr\n",
      "INFO : 11/02/2021 01:57:54 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:57:54 PM : Appending data to the file ../data/greece_2019.csv...\n",
      "INFO : 11/02/2021 01:57:54 PM : Done appending to the file ../data/greece_2019.csv!!!\n",
      "INFO : 11/02/2021 01:57:54 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:57:55 PM : Extracting top 200 weekly for 2019-11-08--2019-11-15 - gr\n",
      "INFO : 11/02/2021 01:57:55 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:57:55 PM : Appending data to the file ../data/greece_2019.csv...\n",
      "INFO : 11/02/2021 01:57:55 PM : Done appending to the file ../data/greece_2019.csv!!!\n",
      "INFO : 11/02/2021 01:57:55 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:57:56 PM : Extracting top 200 weekly for 2019-11-15--2019-11-22 - gr\n",
      "INFO : 11/02/2021 01:57:56 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:57:56 PM : Appending data to the file ../data/greece_2019.csv...\n",
      "INFO : 11/02/2021 01:57:56 PM : Done appending to the file ../data/greece_2019.csv!!!\n",
      "INFO : 11/02/2021 01:57:56 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:57:57 PM : Extracting top 200 weekly for 2019-11-22--2019-11-29 - gr\n",
      "INFO : 11/02/2021 01:57:57 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:57:57 PM : Appending data to the file ../data/greece_2019.csv...\n",
      "INFO : 11/02/2021 01:57:57 PM : Done appending to the file ../data/greece_2019.csv!!!\n",
      "INFO : 11/02/2021 01:57:57 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:57:58 PM : Extracting top 200 weekly for 2019-11-29--2019-12-06 - gr\n",
      "INFO : 11/02/2021 01:57:58 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:57:58 PM : Appending data to the file ../data/greece_2019.csv...\n",
      "INFO : 11/02/2021 01:57:58 PM : Done appending to the file ../data/greece_2019.csv!!!\n",
      "INFO : 11/02/2021 01:57:58 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:57:59 PM : Extracting top 200 weekly for 2019-12-06--2019-12-13 - gr\n",
      "INFO : 11/02/2021 01:57:59 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:57:59 PM : Appending data to the file ../data/greece_2019.csv...\n",
      "INFO : 11/02/2021 01:57:59 PM : Done appending to the file ../data/greece_2019.csv!!!\n",
      "INFO : 11/02/2021 01:57:59 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:58:00 PM : Extracting top 200 weekly for 2019-12-13--2019-12-20 - gr\n",
      "INFO : 11/02/2021 01:58:00 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:58:00 PM : Appending data to the file ../data/greece_2019.csv...\n",
      "INFO : 11/02/2021 01:58:00 PM : Done appending to the file ../data/greece_2019.csv!!!\n",
      "INFO : 11/02/2021 01:58:00 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:58:01 PM : Extracting top 200 weekly for 2019-12-20--2019-12-27 - gr\n",
      "INFO : 11/02/2021 01:58:01 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:58:01 PM : Appending data to the file ../data/greece_2019.csv...\n",
      "INFO : 11/02/2021 01:58:01 PM : Done appending to the file ../data/greece_2019.csv!!!\n",
      "INFO : 11/02/2021 01:58:01 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:58:02 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:58:02 PM : Appending data to the file ../data/greece_2019.csv...\n",
      "INFO : 11/02/2021 01:58:02 PM : Done appending to the file ../data/greece_2019.csv!!!\n",
      "INFO : 11/02/2021 01:58:02 PM : Done appending to the table top_200_weekly!!!\n"
     ]
    }
   ],
   "source": [
    "api = SpotifyCharts()\n",
    "connector = sqlalchemy.create_engine(\"sqlite:///../data/greece_2019.db\", echo=False)\n",
    "api.top200Weekly(output_file = \"../data/greece_2019.csv\", output_db = connector, webhook = [\"https://mywebhookssite.com/post/\"], start = \"2019-01-01\", end = \"2019-12-31\", region = \"gr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Position</th>\n",
       "      <th>Track Name</th>\n",
       "      <th>Artist</th>\n",
       "      <th>Streams</th>\n",
       "      <th>date</th>\n",
       "      <th>region</th>\n",
       "      <th>spotify_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10195</th>\n",
       "      <td>196</td>\n",
       "      <td>Without Me</td>\n",
       "      <td>Halsey</td>\n",
       "      <td>14669</td>\n",
       "      <td>2019-12-20--2019-12-27</td>\n",
       "      <td>gr</td>\n",
       "      <td>5p7ujcrUXASCNwRaWNHR1C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10196</th>\n",
       "      <td>197</td>\n",
       "      <td>Hate Me (with Juice WRLD)</td>\n",
       "      <td>Ellie Goulding</td>\n",
       "      <td>14646</td>\n",
       "      <td>2019-12-20--2019-12-27</td>\n",
       "      <td>gr</td>\n",
       "      <td>6kls8cSlUyHW2BUOkDJIZE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10197</th>\n",
       "      <td>198</td>\n",
       "      <td>Santa's Coming for Us</td>\n",
       "      <td>Sia</td>\n",
       "      <td>14645</td>\n",
       "      <td>2019-12-20--2019-12-27</td>\n",
       "      <td>gr</td>\n",
       "      <td>3AlpkljBS1AU7HFVPms8K6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10198</th>\n",
       "      <td>199</td>\n",
       "      <td>Suge</td>\n",
       "      <td>DaBaby</td>\n",
       "      <td>14612</td>\n",
       "      <td>2019-12-20--2019-12-27</td>\n",
       "      <td>gr</td>\n",
       "      <td>2gwkD6igEhQbDQegRCcdoB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10199</th>\n",
       "      <td>200</td>\n",
       "      <td>Korakia</td>\n",
       "      <td>LEX</td>\n",
       "      <td>14543</td>\n",
       "      <td>2019-12-20--2019-12-27</td>\n",
       "      <td>gr</td>\n",
       "      <td>4BCjxdM0hXbCEFf0Ck1Dod</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Position                 Track Name          Artist  Streams  \\\n",
       "10195       196                 Without Me          Halsey    14669   \n",
       "10196       197  Hate Me (with Juice WRLD)  Ellie Goulding    14646   \n",
       "10197       198      Santa's Coming for Us             Sia    14645   \n",
       "10198       199                       Suge          DaBaby    14612   \n",
       "10199       200                    Korakia             LEX    14543   \n",
       "\n",
       "                         date region              spotify_id  \n",
       "10195  2019-12-20--2019-12-27     gr  5p7ujcrUXASCNwRaWNHR1C  \n",
       "10196  2019-12-20--2019-12-27     gr  6kls8cSlUyHW2BUOkDJIZE  \n",
       "10197  2019-12-20--2019-12-27     gr  3AlpkljBS1AU7HFVPms8K6  \n",
       "10198  2019-12-20--2019-12-27     gr  2gwkD6igEhQbDQegRCcdoB  \n",
       "10199  2019-12-20--2019-12-27     gr  4BCjxdM0hXbCEFf0Ck1Dod  "
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reading in csv from output file  \n",
    "greece_2019 = pd.read_csv('../data/greece_2019.csv')\n",
    "greece_2019.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Greece Top 200 Weekly, 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : 11/02/2021 01:58:37 PM : The start date 2020-01-01 provided for top200Weekly is invalid. Wanna give one these a try? ['2020-01-03', '2020-01-10', '2020-01-17', '2020-01-24', '2020-01-31']\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter (1) to use the first suggestion, or (2) to quit and set yourself:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : 11/02/2021 01:58:41 PM : Extracting top 200 weekly for 2020-01-03--2020-01-10 - gr\n",
      "INFO : 11/02/2021 01:58:41 PM : Extracting top 200 weekly for 2020-01-10--2020-01-17 - gr\n",
      "INFO : 11/02/2021 01:58:41 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:58:41 PM : POSTing data to the endpoint https://mywebhookssite.com/post/\n",
      "INFO : 11/02/2021 01:58:41 PM : Appending data to the file ../data/greece_2020.csv...\n",
      "INFO : 11/02/2021 01:58:41 PM : Done appending to the file ../data/greece_2020.csv!!!\n",
      "Exception in thread Thread-43:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/urllib3/connection.py\", line 159, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/urllib3/util/connection.py\", line 61, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/socket.py\", line 918, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 8] nodename nor servname provided, or not known\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 670, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 381, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 978, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/urllib3/connection.py\", line 309, in connect\n",
      "    conn = self._new_conn()\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/urllib3/connection.py\", line 171, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x7f9877c6ef70>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/requests/adapters.py\", line 439, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 726, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/urllib3/util/retry.py\", line 446, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='mywebhookssite.com', port=443): Max retries exceeded with url: /post/ (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f9877c6ef70>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/fycharts/SpotifyCharts.py\", line 88, in __post_to_endpoint_from_queue\n",
      "    postToRestEndpoint(df, url, what_data)\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/fycharts/write_to_outputs.py\", line 61, in postToRestEndpoint\n",
      "    raise(e)\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/fycharts/write_to_outputs.py\", line 58, in postToRestEndpoint\n",
      "    requests.post(url, json = dump)\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/requests/api.py\", line 119, in post\n",
      "    return request('post', url, data=data, json=json, **kwargs)\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/requests/api.py\", line 61, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/requests/sessions.py\", line 530, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/requests/sessions.py\", line 643, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/requests/adapters.py\", line 516, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='mywebhookssite.com', port=443): Max retries exceeded with url: /post/ (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f9877c6ef70>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/threading.py\", line 932, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/fycharts/SpotifyCharts.py\", line 90, in __post_to_endpoint_from_queue\n",
      "    raise RuntimeError(e)\n",
      "RuntimeError: HTTPSConnectionPool(host='mywebhookssite.com', port=443): Max retries exceeded with url: /post/ (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f9877c6ef70>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\n",
      "INFO : 11/02/2021 01:58:41 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:58:42 PM : Extracting top 200 weekly for 2020-01-17--2020-01-24 - gr\n",
      "INFO : 11/02/2021 01:58:42 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:58:42 PM : Appending data to the file ../data/greece_2020.csv...\n",
      "INFO : 11/02/2021 01:58:42 PM : Done appending to the file ../data/greece_2020.csv!!!\n",
      "INFO : 11/02/2021 01:58:42 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:58:43 PM : Extracting top 200 weekly for 2020-01-24--2020-01-31 - gr\n",
      "INFO : 11/02/2021 01:58:43 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:58:43 PM : Appending data to the file ../data/greece_2020.csv...\n",
      "INFO : 11/02/2021 01:58:43 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:58:43 PM : Done appending to the file ../data/greece_2020.csv!!!\n",
      "INFO : 11/02/2021 01:58:44 PM : Extracting top 200 weekly for 2020-01-31--2020-02-07 - gr\n",
      "INFO : 11/02/2021 01:58:44 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:58:44 PM : Appending data to the file ../data/greece_2020.csv...\n",
      "INFO : 11/02/2021 01:58:44 PM : Done appending to the file ../data/greece_2020.csv!!!\n",
      "INFO : 11/02/2021 01:58:44 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:58:45 PM : Extracting top 200 weekly for 2020-02-07--2020-02-14 - gr\n",
      "INFO : 11/02/2021 01:58:45 PM : Appending data to the file ../data/greece_2020.csv...\n",
      "INFO : 11/02/2021 01:58:45 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:58:45 PM : Done appending to the file ../data/greece_2020.csv!!!\n",
      "INFO : 11/02/2021 01:58:45 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:58:46 PM : Extracting top 200 weekly for 2020-02-14--2020-02-21 - gr\n",
      "INFO : 11/02/2021 01:58:46 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:58:46 PM : Appending data to the file ../data/greece_2020.csv...\n",
      "INFO : 11/02/2021 01:58:46 PM : Done appending to the file ../data/greece_2020.csv!!!\n",
      "INFO : 11/02/2021 01:58:46 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:58:47 PM : Extracting top 200 weekly for 2020-02-21--2020-02-28 - gr\n",
      "INFO : 11/02/2021 01:58:47 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:58:47 PM : Appending data to the file ../data/greece_2020.csv...\n",
      "INFO : 11/02/2021 01:58:47 PM : Done appending to the file ../data/greece_2020.csv!!!\n",
      "INFO : 11/02/2021 01:58:47 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:58:47 PM : Extracting top 200 weekly for 2020-02-28--2020-03-06 - gr\n",
      "INFO : 11/02/2021 01:58:47 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:58:47 PM : Appending data to the file ../data/greece_2020.csv...\n",
      "INFO : 11/02/2021 01:58:47 PM : Done appending to the file ../data/greece_2020.csv!!!\n",
      "INFO : 11/02/2021 01:58:47 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:58:48 PM : Extracting top 200 weekly for 2020-03-06--2020-03-13 - gr\n",
      "INFO : 11/02/2021 01:58:48 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:58:48 PM : Appending data to the file ../data/greece_2020.csv...\n",
      "INFO : 11/02/2021 01:58:48 PM : Done appending to the file ../data/greece_2020.csv!!!\n",
      "INFO : 11/02/2021 01:58:48 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:58:49 PM : Extracting top 200 weekly for 2020-03-13--2020-03-20 - gr\n",
      "INFO : 11/02/2021 01:58:49 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:58:49 PM : Appending data to the file ../data/greece_2020.csv...\n",
      "INFO : 11/02/2021 01:58:49 PM : Done appending to the file ../data/greece_2020.csv!!!\n",
      "INFO : 11/02/2021 01:58:49 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:58:50 PM : Extracting top 200 weekly for 2020-03-20--2020-03-27 - gr\n",
      "INFO : 11/02/2021 01:58:50 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:58:50 PM : Appending data to the file ../data/greece_2020.csv...\n",
      "INFO : 11/02/2021 01:58:50 PM : Done appending to the file ../data/greece_2020.csv!!!\n",
      "INFO : 11/02/2021 01:58:50 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:58:51 PM : Extracting top 200 weekly for 2020-03-27--2020-04-03 - gr\n",
      "INFO : 11/02/2021 01:58:51 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:58:51 PM : Appending data to the file ../data/greece_2020.csv...\n",
      "INFO : 11/02/2021 01:58:51 PM : Done appending to the file ../data/greece_2020.csv!!!\n",
      "INFO : 11/02/2021 01:58:51 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:58:52 PM : Extracting top 200 weekly for 2020-04-03--2020-04-10 - gr\n",
      "INFO : 11/02/2021 01:58:52 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:58:52 PM : Appending data to the file ../data/greece_2020.csv...\n",
      "INFO : 11/02/2021 01:58:52 PM : Done appending to the file ../data/greece_2020.csv!!!\n",
      "INFO : 11/02/2021 01:58:52 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:58:53 PM : Extracting top 200 weekly for 2020-04-10--2020-04-17 - gr\n",
      "INFO : 11/02/2021 01:58:53 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:58:53 PM : Appending data to the file ../data/greece_2020.csv...\n",
      "INFO : 11/02/2021 01:58:53 PM : Done appending to the file ../data/greece_2020.csv!!!\n",
      "INFO : 11/02/2021 01:58:53 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:58:53 PM : Extracting top 200 weekly for 2020-04-17--2020-04-24 - gr\n",
      "INFO : 11/02/2021 01:58:53 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:58:53 PM : Appending data to the file ../data/greece_2020.csv...\n",
      "INFO : 11/02/2021 01:58:53 PM : Done appending to the file ../data/greece_2020.csv!!!\n",
      "INFO : 11/02/2021 01:58:53 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:58:54 PM : Extracting top 200 weekly for 2020-04-24--2020-05-01 - gr\n",
      "INFO : 11/02/2021 01:58:54 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:58:54 PM : Appending data to the file ../data/greece_2020.csv...\n",
      "INFO : 11/02/2021 01:58:54 PM : Done appending to the file ../data/greece_2020.csv!!!\n",
      "INFO : 11/02/2021 01:58:54 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:58:55 PM : Extracting top 200 weekly for 2020-05-01--2020-05-08 - gr\n",
      "INFO : 11/02/2021 01:58:55 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:58:55 PM : Appending data to the file ../data/greece_2020.csv...\n",
      "INFO : 11/02/2021 01:58:55 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:58:55 PM : Done appending to the file ../data/greece_2020.csv!!!\n",
      "INFO : 11/02/2021 01:58:56 PM : Extracting top 200 weekly for 2020-05-08--2020-05-15 - gr\n",
      "INFO : 11/02/2021 01:58:56 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:58:56 PM : Appending data to the file ../data/greece_2020.csv...\n",
      "INFO : 11/02/2021 01:58:56 PM : Done appending to the file ../data/greece_2020.csv!!!\n",
      "INFO : 11/02/2021 01:58:56 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:58:57 PM : Extracting top 200 weekly for 2020-05-15--2020-05-22 - gr\n",
      "INFO : 11/02/2021 01:58:57 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:58:57 PM : Appending data to the file ../data/greece_2020.csv...\n",
      "INFO : 11/02/2021 01:58:57 PM : Done appending to the file ../data/greece_2020.csv!!!\n",
      "INFO : 11/02/2021 01:58:57 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:58:58 PM : Extracting top 200 weekly for 2020-05-22--2020-05-29 - gr\n",
      "INFO : 11/02/2021 01:58:58 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:58:58 PM : Appending data to the file ../data/greece_2020.csv...\n",
      "INFO : 11/02/2021 01:58:58 PM : Done appending to the file ../data/greece_2020.csv!!!\n",
      "INFO : 11/02/2021 01:58:58 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:58:59 PM : Extracting top 200 weekly for 2020-05-29--2020-06-05 - gr\n",
      "INFO : 11/02/2021 01:58:59 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:58:59 PM : Appending data to the file ../data/greece_2020.csv...\n",
      "INFO : 11/02/2021 01:58:59 PM : Done appending to the file ../data/greece_2020.csv!!!\n",
      "INFO : 11/02/2021 01:58:59 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:59:00 PM : Extracting top 200 weekly for 2020-06-05--2020-06-12 - gr\n",
      "INFO : 11/02/2021 01:59:00 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:59:00 PM : Appending data to the file ../data/greece_2020.csv...\n",
      "INFO : 11/02/2021 01:59:00 PM : Done appending to the file ../data/greece_2020.csv!!!\n",
      "INFO : 11/02/2021 01:59:00 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:59:01 PM : Extracting top 200 weekly for 2020-06-12--2020-06-19 - gr\n",
      "INFO : 11/02/2021 01:59:01 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:59:01 PM : Appending data to the file ../data/greece_2020.csv...\n",
      "INFO : 11/02/2021 01:59:01 PM : Done appending to the file ../data/greece_2020.csv!!!\n",
      "INFO : 11/02/2021 01:59:01 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:59:01 PM : Extracting top 200 weekly for 2020-06-19--2020-06-26 - gr\n",
      "INFO : 11/02/2021 01:59:01 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:59:01 PM : Appending data to the file ../data/greece_2020.csv...\n",
      "INFO : 11/02/2021 01:59:01 PM : Done appending to the file ../data/greece_2020.csv!!!\n",
      "INFO : 11/02/2021 01:59:01 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:59:02 PM : Extracting top 200 weekly for 2020-06-26--2020-07-03 - gr\n",
      "INFO : 11/02/2021 01:59:02 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:59:02 PM : Appending data to the file ../data/greece_2020.csv...\n",
      "INFO : 11/02/2021 01:59:02 PM : Done appending to the file ../data/greece_2020.csv!!!\n",
      "INFO : 11/02/2021 01:59:02 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:59:03 PM : Extracting top 200 weekly for 2020-07-03--2020-07-10 - gr\n",
      "INFO : 11/02/2021 01:59:03 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:59:03 PM : Appending data to the file ../data/greece_2020.csv...\n",
      "INFO : 11/02/2021 01:59:03 PM : Done appending to the file ../data/greece_2020.csv!!!\n",
      "INFO : 11/02/2021 01:59:03 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:59:04 PM : Extracting top 200 weekly for 2020-07-10--2020-07-17 - gr\n",
      "INFO : 11/02/2021 01:59:04 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:59:04 PM : Appending data to the file ../data/greece_2020.csv...\n",
      "INFO : 11/02/2021 01:59:04 PM : Done appending to the file ../data/greece_2020.csv!!!\n",
      "INFO : 11/02/2021 01:59:04 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:59:05 PM : Extracting top 200 weekly for 2020-07-17--2020-07-24 - gr\n",
      "INFO : 11/02/2021 01:59:05 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:59:05 PM : Appending data to the file ../data/greece_2020.csv...\n",
      "INFO : 11/02/2021 01:59:05 PM : Done appending to the file ../data/greece_2020.csv!!!\n",
      "INFO : 11/02/2021 01:59:05 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:59:06 PM : Extracting top 200 weekly for 2020-07-24--2020-07-31 - gr\n",
      "INFO : 11/02/2021 01:59:06 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:59:06 PM : Appending data to the file ../data/greece_2020.csv...\n",
      "INFO : 11/02/2021 01:59:06 PM : Done appending to the file ../data/greece_2020.csv!!!\n",
      "INFO : 11/02/2021 01:59:06 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:59:07 PM : Extracting top 200 weekly for 2020-07-31--2020-08-07 - gr\n",
      "INFO : 11/02/2021 01:59:07 PM : Appending data to the file ../data/greece_2020.csv...\n",
      "INFO : 11/02/2021 01:59:07 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:59:07 PM : Done appending to the file ../data/greece_2020.csv!!!\n",
      "INFO : 11/02/2021 01:59:07 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:59:07 PM : Extracting top 200 weekly for 2020-08-07--2020-08-14 - gr\n",
      "INFO : 11/02/2021 01:59:07 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:59:07 PM : Appending data to the file ../data/greece_2020.csv...\n",
      "INFO : 11/02/2021 01:59:07 PM : Done appending to the file ../data/greece_2020.csv!!!\n",
      "INFO : 11/02/2021 01:59:07 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:59:08 PM : Extracting top 200 weekly for 2020-08-14--2020-08-21 - gr\n",
      "INFO : 11/02/2021 01:59:08 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:59:08 PM : Appending data to the file ../data/greece_2020.csv...\n",
      "INFO : 11/02/2021 01:59:08 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:59:08 PM : Done appending to the file ../data/greece_2020.csv!!!\n",
      "INFO : 11/02/2021 01:59:09 PM : Extracting top 200 weekly for 2020-08-21--2020-08-28 - gr\n",
      "INFO : 11/02/2021 01:59:09 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:59:09 PM : Appending data to the file ../data/greece_2020.csv...\n",
      "INFO : 11/02/2021 01:59:09 PM : Done appending to the file ../data/greece_2020.csv!!!\n",
      "INFO : 11/02/2021 01:59:09 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:59:10 PM : Extracting top 200 weekly for 2020-08-28--2020-09-04 - gr\n",
      "INFO : 11/02/2021 01:59:10 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:59:10 PM : Appending data to the file ../data/greece_2020.csv...\n",
      "INFO : 11/02/2021 01:59:10 PM : Done appending to the file ../data/greece_2020.csv!!!\n",
      "INFO : 11/02/2021 01:59:10 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:59:11 PM : Extracting top 200 weekly for 2020-09-04--2020-09-11 - gr\n",
      "INFO : 11/02/2021 01:59:11 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:59:11 PM : Appending data to the file ../data/greece_2020.csv...\n",
      "INFO : 11/02/2021 01:59:11 PM : Done appending to the file ../data/greece_2020.csv!!!\n",
      "INFO : 11/02/2021 01:59:11 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:59:12 PM : Extracting top 200 weekly for 2020-09-11--2020-09-18 - gr\n",
      "INFO : 11/02/2021 01:59:12 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:59:12 PM : Appending data to the file ../data/greece_2020.csv...\n",
      "INFO : 11/02/2021 01:59:12 PM : Done appending to the file ../data/greece_2020.csv!!!\n",
      "INFO : 11/02/2021 01:59:12 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:59:12 PM : Extracting top 200 weekly for 2020-09-18--2020-09-25 - gr\n",
      "INFO : 11/02/2021 01:59:12 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:59:12 PM : Appending data to the file ../data/greece_2020.csv...\n",
      "INFO : 11/02/2021 01:59:12 PM : Done appending to the file ../data/greece_2020.csv!!!\n",
      "INFO : 11/02/2021 01:59:12 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:59:13 PM : Extracting top 200 weekly for 2020-09-25--2020-10-02 - gr\n",
      "INFO : 11/02/2021 01:59:13 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:59:13 PM : Appending data to the file ../data/greece_2020.csv...\n",
      "INFO : 11/02/2021 01:59:13 PM : Done appending to the file ../data/greece_2020.csv!!!\n",
      "INFO : 11/02/2021 01:59:13 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:59:14 PM : Extracting top 200 weekly for 2020-10-02--2020-10-09 - gr\n",
      "INFO : 11/02/2021 01:59:14 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:59:14 PM : Appending data to the file ../data/greece_2020.csv...\n",
      "INFO : 11/02/2021 01:59:14 PM : Done appending to the file ../data/greece_2020.csv!!!\n",
      "INFO : 11/02/2021 01:59:14 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:59:15 PM : Extracting top 200 weekly for 2020-10-09--2020-10-16 - gr\n",
      "INFO : 11/02/2021 01:59:15 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:59:15 PM : Appending data to the file ../data/greece_2020.csv...\n",
      "INFO : 11/02/2021 01:59:15 PM : Done appending to the file ../data/greece_2020.csv!!!\n",
      "INFO : 11/02/2021 01:59:15 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:59:16 PM : Extracting top 200 weekly for 2020-10-16--2020-10-23 - gr\n",
      "INFO : 11/02/2021 01:59:16 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:59:16 PM : Appending data to the file ../data/greece_2020.csv...\n",
      "INFO : 11/02/2021 01:59:16 PM : Done appending to the file ../data/greece_2020.csv!!!\n",
      "INFO : 11/02/2021 01:59:16 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:59:17 PM : Extracting top 200 weekly for 2020-10-23--2020-10-30 - gr\n",
      "INFO : 11/02/2021 01:59:17 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:59:17 PM : Appending data to the file ../data/greece_2020.csv...\n",
      "INFO : 11/02/2021 01:59:17 PM : Done appending to the file ../data/greece_2020.csv!!!\n",
      "INFO : 11/02/2021 01:59:17 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:59:18 PM : Extracting top 200 weekly for 2020-10-30--2020-11-06 - gr\n",
      "INFO : 11/02/2021 01:59:18 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:59:18 PM : Appending data to the file ../data/greece_2020.csv...\n",
      "INFO : 11/02/2021 01:59:18 PM : Done appending to the file ../data/greece_2020.csv!!!\n",
      "INFO : 11/02/2021 01:59:18 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:59:19 PM : Extracting top 200 weekly for 2020-11-06--2020-11-13 - gr\n",
      "INFO : 11/02/2021 01:59:19 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:59:19 PM : Appending data to the file ../data/greece_2020.csv...\n",
      "INFO : 11/02/2021 01:59:19 PM : Done appending to the file ../data/greece_2020.csv!!!\n",
      "INFO : 11/02/2021 01:59:19 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:59:20 PM : Extracting top 200 weekly for 2020-11-13--2020-11-20 - gr\n",
      "INFO : 11/02/2021 01:59:20 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:59:20 PM : Appending data to the file ../data/greece_2020.csv...\n",
      "INFO : 11/02/2021 01:59:20 PM : Done appending to the file ../data/greece_2020.csv!!!\n",
      "INFO : 11/02/2021 01:59:20 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:59:21 PM : Extracting top 200 weekly for 2020-11-20--2020-11-27 - gr\n",
      "INFO : 11/02/2021 01:59:21 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:59:21 PM : Appending data to the file ../data/greece_2020.csv...\n",
      "INFO : 11/02/2021 01:59:21 PM : Done appending to the file ../data/greece_2020.csv!!!\n",
      "INFO : 11/02/2021 01:59:21 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:59:22 PM : Extracting top 200 weekly for 2020-11-27--2020-12-04 - gr\n",
      "INFO : 11/02/2021 01:59:22 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:59:22 PM : Appending data to the file ../data/greece_2020.csv...\n",
      "INFO : 11/02/2021 01:59:22 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:59:22 PM : Done appending to the file ../data/greece_2020.csv!!!\n",
      "INFO : 11/02/2021 01:59:22 PM : Extracting top 200 weekly for 2020-12-04--2020-12-11 - gr\n",
      "INFO : 11/02/2021 01:59:22 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:59:22 PM : Appending data to the file ../data/greece_2020.csv...\n",
      "INFO : 11/02/2021 01:59:22 PM : Done appending to the file ../data/greece_2020.csv!!!\n",
      "INFO : 11/02/2021 01:59:22 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:59:23 PM : Extracting top 200 weekly for 2020-12-11--2020-12-18 - gr\n",
      "INFO : 11/02/2021 01:59:23 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:59:23 PM : Appending data to the file ../data/greece_2020.csv...\n",
      "INFO : 11/02/2021 01:59:23 PM : Done appending to the file ../data/greece_2020.csv!!!\n",
      "INFO : 11/02/2021 01:59:23 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:59:24 PM : Extracting top 200 weekly for 2020-12-18--2020-12-25 - gr\n",
      "INFO : 11/02/2021 01:59:24 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:59:24 PM : Appending data to the file ../data/greece_2020.csv...\n",
      "INFO : 11/02/2021 01:59:24 PM : Done appending to the table top_200_weekly!!!\n",
      "INFO : 11/02/2021 01:59:24 PM : Done appending to the file ../data/greece_2020.csv!!!\n",
      "INFO : 11/02/2021 01:59:25 PM : Appending data to the table top_200_weekly\n",
      "INFO : 11/02/2021 01:59:25 PM : Appending data to the file ../data/greece_2020.csv...\n",
      "INFO : 11/02/2021 01:59:25 PM : Done appending to the file ../data/greece_2020.csv!!!\n",
      "INFO : 11/02/2021 01:59:25 PM : Done appending to the table top_200_weekly!!!\n"
     ]
    }
   ],
   "source": [
    "api = SpotifyCharts()\n",
    "connector = sqlalchemy.create_engine(\"sqlite:///../data/greece_2020.db\", echo=False)\n",
    "api.top200Weekly(output_file = \"../data/greece_2020.csv\", output_db = connector, webhook = [\"https://mywebhookssite.com/post/\"], start = \"2020-01-01\", end = \"2020-12-31\", region = \"gr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Position</th>\n",
       "      <th>Track Name</th>\n",
       "      <th>Artist</th>\n",
       "      <th>Streams</th>\n",
       "      <th>date</th>\n",
       "      <th>region</th>\n",
       "      <th>spotify_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>BIG MAN</td>\n",
       "      <td>SNIK</td>\n",
       "      <td>89921</td>\n",
       "      <td>2020-01-03--2020-01-10</td>\n",
       "      <td>gr</td>\n",
       "      <td>7qd01xrME77eyWqGPumC2J</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Dance Monkey</td>\n",
       "      <td>Tones And I</td>\n",
       "      <td>87829</td>\n",
       "      <td>2020-01-03--2020-01-10</td>\n",
       "      <td>gr</td>\n",
       "      <td>1rgnBhdG2JDFTbYkYRZAku</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Oxi Simera</td>\n",
       "      <td>LEX</td>\n",
       "      <td>76427</td>\n",
       "      <td>2020-01-03--2020-01-10</td>\n",
       "      <td>gr</td>\n",
       "      <td>0dD17UWACV5aYTyeS6af1k</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Falling</td>\n",
       "      <td>Trevor Daniel</td>\n",
       "      <td>75299</td>\n",
       "      <td>2020-01-03--2020-01-10</td>\n",
       "      <td>gr</td>\n",
       "      <td>4TnjEaWOeW0eKTKIEvJyCa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>HIGHEST IN THE ROOM</td>\n",
       "      <td>Travis Scott</td>\n",
       "      <td>71702</td>\n",
       "      <td>2020-01-03--2020-01-10</td>\n",
       "      <td>gr</td>\n",
       "      <td>3eekarcy7kvN4yt5ZFzltW</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Position           Track Name         Artist  Streams  \\\n",
       "0         1              BIG MAN           SNIK    89921   \n",
       "1         2         Dance Monkey    Tones And I    87829   \n",
       "2         3           Oxi Simera            LEX    76427   \n",
       "3         4              Falling  Trevor Daniel    75299   \n",
       "4         5  HIGHEST IN THE ROOM   Travis Scott    71702   \n",
       "\n",
       "                     date region              spotify_id  \n",
       "0  2020-01-03--2020-01-10     gr  7qd01xrME77eyWqGPumC2J  \n",
       "1  2020-01-03--2020-01-10     gr  1rgnBhdG2JDFTbYkYRZAku  \n",
       "2  2020-01-03--2020-01-10     gr  0dD17UWACV5aYTyeS6af1k  \n",
       "3  2020-01-03--2020-01-10     gr  4TnjEaWOeW0eKTKIEvJyCa  \n",
       "4  2020-01-03--2020-01-10     gr  3eekarcy7kvN4yt5ZFzltW  "
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reading in csv from output file  \n",
    "greece_2020 = pd.read_csv('../data/greece_2020.csv')\n",
    "greece_2020.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10200, 7)"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "greece_2017.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10200, 7)"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "greece_2018.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10200, 7)"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "greece_2019.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10200, 7)"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "greece_2020.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_I scraped the charts for each separately in case I wanted to separate DataFrames, but now I will now merge into one DataFrame years 2017-2019 because this is more useful for EDA and the start of time series modeling._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Position</th>\n",
       "      <th>Track Name</th>\n",
       "      <th>Artist</th>\n",
       "      <th>Streams</th>\n",
       "      <th>date</th>\n",
       "      <th>region</th>\n",
       "      <th>spotify_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Shape of You</td>\n",
       "      <td>Ed Sheeran</td>\n",
       "      <td>76164</td>\n",
       "      <td>2017-01-06--2017-01-13</td>\n",
       "      <td>gr</td>\n",
       "      <td>7qiZfU4dY1lWllzX7mPBI3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Castle on the Hill</td>\n",
       "      <td>Ed Sheeran</td>\n",
       "      <td>37779</td>\n",
       "      <td>2017-01-06--2017-01-13</td>\n",
       "      <td>gr</td>\n",
       "      <td>6PCUP3dWmTjcTtXY02oFdT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Starboy</td>\n",
       "      <td>The Weeknd</td>\n",
       "      <td>34824</td>\n",
       "      <td>2017-01-06--2017-01-13</td>\n",
       "      <td>gr</td>\n",
       "      <td>5aAx2yezTd8zXrkmtKl66Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>I Don’t Wanna Live Forever (Fifty Shades Darke...</td>\n",
       "      <td>ZAYN</td>\n",
       "      <td>24781</td>\n",
       "      <td>2017-01-06--2017-01-13</td>\n",
       "      <td>gr</td>\n",
       "      <td>3NdDpSvN911VPGivFlV5d0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Rockabye (feat. Sean Paul &amp; Anne-Marie)</td>\n",
       "      <td>Clean Bandit</td>\n",
       "      <td>22380</td>\n",
       "      <td>2017-01-06--2017-01-13</td>\n",
       "      <td>gr</td>\n",
       "      <td>5knuzwU65gJK7IF5yJsuaW</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Position                                         Track Name        Artist  \\\n",
       "0         1                                       Shape of You    Ed Sheeran   \n",
       "1         2                                 Castle on the Hill    Ed Sheeran   \n",
       "2         3                                            Starboy    The Weeknd   \n",
       "3         4  I Don’t Wanna Live Forever (Fifty Shades Darke...          ZAYN   \n",
       "4         5            Rockabye (feat. Sean Paul & Anne-Marie)  Clean Bandit   \n",
       "\n",
       "   Streams                    date region              spotify_id  \n",
       "0    76164  2017-01-06--2017-01-13     gr  7qiZfU4dY1lWllzX7mPBI3  \n",
       "1    37779  2017-01-06--2017-01-13     gr  6PCUP3dWmTjcTtXY02oFdT  \n",
       "2    34824  2017-01-06--2017-01-13     gr  5aAx2yezTd8zXrkmtKl66Z  \n",
       "3    24781  2017-01-06--2017-01-13     gr  3NdDpSvN911VPGivFlV5d0  \n",
       "4    22380  2017-01-06--2017-01-13     gr  5knuzwU65gJK7IF5yJsuaW  "
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "greece_17_19 = pd.concat([greece_2017, greece_2018, greece_2019])\n",
    "greece_17_19.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Position</th>\n",
       "      <th>Track Name</th>\n",
       "      <th>Artist</th>\n",
       "      <th>Streams</th>\n",
       "      <th>date</th>\n",
       "      <th>region</th>\n",
       "      <th>spotify_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30595</th>\n",
       "      <td>196</td>\n",
       "      <td>Without Me</td>\n",
       "      <td>Halsey</td>\n",
       "      <td>14669</td>\n",
       "      <td>2019-12-20--2019-12-27</td>\n",
       "      <td>gr</td>\n",
       "      <td>5p7ujcrUXASCNwRaWNHR1C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30596</th>\n",
       "      <td>197</td>\n",
       "      <td>Hate Me (with Juice WRLD)</td>\n",
       "      <td>Ellie Goulding</td>\n",
       "      <td>14646</td>\n",
       "      <td>2019-12-20--2019-12-27</td>\n",
       "      <td>gr</td>\n",
       "      <td>6kls8cSlUyHW2BUOkDJIZE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30597</th>\n",
       "      <td>198</td>\n",
       "      <td>Santa's Coming for Us</td>\n",
       "      <td>Sia</td>\n",
       "      <td>14645</td>\n",
       "      <td>2019-12-20--2019-12-27</td>\n",
       "      <td>gr</td>\n",
       "      <td>3AlpkljBS1AU7HFVPms8K6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30598</th>\n",
       "      <td>199</td>\n",
       "      <td>Suge</td>\n",
       "      <td>DaBaby</td>\n",
       "      <td>14612</td>\n",
       "      <td>2019-12-20--2019-12-27</td>\n",
       "      <td>gr</td>\n",
       "      <td>2gwkD6igEhQbDQegRCcdoB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30599</th>\n",
       "      <td>200</td>\n",
       "      <td>Korakia</td>\n",
       "      <td>LEX</td>\n",
       "      <td>14543</td>\n",
       "      <td>2019-12-20--2019-12-27</td>\n",
       "      <td>gr</td>\n",
       "      <td>4BCjxdM0hXbCEFf0Ck1Dod</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Position                 Track Name          Artist  Streams  \\\n",
       "30595       196                 Without Me          Halsey    14669   \n",
       "30596       197  Hate Me (with Juice WRLD)  Ellie Goulding    14646   \n",
       "30597       198      Santa's Coming for Us             Sia    14645   \n",
       "30598       199                       Suge          DaBaby    14612   \n",
       "30599       200                    Korakia             LEX    14543   \n",
       "\n",
       "                         date region              spotify_id  \n",
       "30595  2019-12-20--2019-12-27     gr  5p7ujcrUXASCNwRaWNHR1C  \n",
       "30596  2019-12-20--2019-12-27     gr  6kls8cSlUyHW2BUOkDJIZE  \n",
       "30597  2019-12-20--2019-12-27     gr  3AlpkljBS1AU7HFVPms8K6  \n",
       "30598  2019-12-20--2019-12-27     gr  2gwkD6igEhQbDQegRCcdoB  \n",
       "30599  2019-12-20--2019-12-27     gr  4BCjxdM0hXbCEFf0Ck1Dod  "
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# resettting index so it can be concatenated with features dataframe later on\n",
    "greece_17_19.reset_index(drop=True, inplace=True)\n",
    "greece_17_19.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30600, 7)"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "greece_17_19.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 30600 entries, 0 to 30599\n",
      "Data columns (total 7 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   Position    30600 non-null  int64 \n",
      " 1   Track Name  30596 non-null  object\n",
      " 2   Artist      30596 non-null  object\n",
      " 3   Streams     30600 non-null  int64 \n",
      " 4   date        30600 non-null  object\n",
      " 5   region      30600 non-null  object\n",
      " 6   spotify_id  30600 non-null  object\n",
      "dtypes: int64(2), object(5)\n",
      "memory usage: 1.6+ MB\n"
     ]
    }
   ],
   "source": [
    "greece_17_19.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audio Features "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtaining access to audio features for each track using Spotify's Client Credentials Flow (see above) and a wrapper library [Spotipy](https://spotipy.readthedocs.io/en/2.16.1/). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_**Citation: Code for scraping audio features borrowed from [CNN_for_Dance_Music_Classification](https://github.com/amytaylor330/CNN_for_Dance_Music_Classification)._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tracks where no audio features were available: 0\n",
      "Number of usable tracks: 30600\n"
     ]
    }
   ],
   "source": [
    "features_list_greece_17_19 = []          # will create a list of each track's audio features, each appended as a separate dictionary \n",
    "batchsize = 100    # max number of track ids we're allowed to submit per query\n",
    "None_counter = 0   # count if there are any songs without any audio features\n",
    "\n",
    "for i in range(0,len(greece_17_19['spotify_id']), batchsize):    \n",
    "    batch = greece_17_19['spotify_id'][i:i + batchsize]           # offsetting batchsize to acquire more tracks \n",
    "    \n",
    "    feature_results = sp.audio_features(batch)              #begins querying the audio features endpoint\n",
    "\n",
    "    for i, t in enumerate(feature_results):\n",
    "        if t == None:                               #if the audio features for a song are missing, count 1        \n",
    "            None_counter += 1          \n",
    "        else:\n",
    "            features_list_greece_17_19.append(t)  \n",
    "            \n",
    "print('Number of tracks where no audio features were available:', None_counter)\n",
    "print('Number of usable tracks:', len(features_list_greece_17_19))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>key</th>\n",
       "      <th>loudness</th>\n",
       "      <th>mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>valence</th>\n",
       "      <th>tempo</th>\n",
       "      <th>type</th>\n",
       "      <th>id</th>\n",
       "      <th>uri</th>\n",
       "      <th>track_href</th>\n",
       "      <th>analysis_url</th>\n",
       "      <th>duration_ms</th>\n",
       "      <th>time_signature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.825</td>\n",
       "      <td>0.652</td>\n",
       "      <td>1</td>\n",
       "      <td>-3.183</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0802</td>\n",
       "      <td>0.5810</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0931</td>\n",
       "      <td>0.9310</td>\n",
       "      <td>95.977</td>\n",
       "      <td>audio_features</td>\n",
       "      <td>7qiZfU4dY1lWllzX7mPBI3</td>\n",
       "      <td>spotify:track:7qiZfU4dY1lWllzX7mPBI3</td>\n",
       "      <td>https://api.spotify.com/v1/tracks/7qiZfU4dY1lW...</td>\n",
       "      <td>https://api.spotify.com/v1/audio-analysis/7qiZ...</td>\n",
       "      <td>233713</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.461</td>\n",
       "      <td>0.834</td>\n",
       "      <td>2</td>\n",
       "      <td>-4.868</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0989</td>\n",
       "      <td>0.0232</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.1400</td>\n",
       "      <td>0.4710</td>\n",
       "      <td>135.007</td>\n",
       "      <td>audio_features</td>\n",
       "      <td>6PCUP3dWmTjcTtXY02oFdT</td>\n",
       "      <td>spotify:track:6PCUP3dWmTjcTtXY02oFdT</td>\n",
       "      <td>https://api.spotify.com/v1/tracks/6PCUP3dWmTjc...</td>\n",
       "      <td>https://api.spotify.com/v1/audio-analysis/6PCU...</td>\n",
       "      <td>261154</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.681</td>\n",
       "      <td>0.594</td>\n",
       "      <td>7</td>\n",
       "      <td>-7.028</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2820</td>\n",
       "      <td>0.1650</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.1340</td>\n",
       "      <td>0.5350</td>\n",
       "      <td>186.054</td>\n",
       "      <td>audio_features</td>\n",
       "      <td>5aAx2yezTd8zXrkmtKl66Z</td>\n",
       "      <td>spotify:track:5aAx2yezTd8zXrkmtKl66Z</td>\n",
       "      <td>https://api.spotify.com/v1/tracks/5aAx2yezTd8z...</td>\n",
       "      <td>https://api.spotify.com/v1/audio-analysis/5aAx...</td>\n",
       "      <td>230453</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.735</td>\n",
       "      <td>0.451</td>\n",
       "      <td>0</td>\n",
       "      <td>-8.374</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0585</td>\n",
       "      <td>0.0631</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.3250</td>\n",
       "      <td>0.0862</td>\n",
       "      <td>117.973</td>\n",
       "      <td>audio_features</td>\n",
       "      <td>3NdDpSvN911VPGivFlV5d0</td>\n",
       "      <td>spotify:track:3NdDpSvN911VPGivFlV5d0</td>\n",
       "      <td>https://api.spotify.com/v1/tracks/3NdDpSvN911V...</td>\n",
       "      <td>https://api.spotify.com/v1/audio-analysis/3NdD...</td>\n",
       "      <td>245200</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.720</td>\n",
       "      <td>0.763</td>\n",
       "      <td>9</td>\n",
       "      <td>-4.068</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0523</td>\n",
       "      <td>0.4060</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1800</td>\n",
       "      <td>0.7420</td>\n",
       "      <td>101.965</td>\n",
       "      <td>audio_features</td>\n",
       "      <td>5knuzwU65gJK7IF5yJsuaW</td>\n",
       "      <td>spotify:track:5knuzwU65gJK7IF5yJsuaW</td>\n",
       "      <td>https://api.spotify.com/v1/tracks/5knuzwU65gJK...</td>\n",
       "      <td>https://api.spotify.com/v1/audio-analysis/5knu...</td>\n",
       "      <td>251088</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   danceability  energy  key  loudness  mode  speechiness  acousticness  \\\n",
       "0         0.825   0.652    1    -3.183     0       0.0802        0.5810   \n",
       "1         0.461   0.834    2    -4.868     1       0.0989        0.0232   \n",
       "2         0.681   0.594    7    -7.028     1       0.2820        0.1650   \n",
       "3         0.735   0.451    0    -8.374     1       0.0585        0.0631   \n",
       "4         0.720   0.763    9    -4.068     0       0.0523        0.4060   \n",
       "\n",
       "   instrumentalness  liveness  valence    tempo            type  \\\n",
       "0          0.000000    0.0931   0.9310   95.977  audio_features   \n",
       "1          0.000011    0.1400   0.4710  135.007  audio_features   \n",
       "2          0.000003    0.1340   0.5350  186.054  audio_features   \n",
       "3          0.000013    0.3250   0.0862  117.973  audio_features   \n",
       "4          0.000000    0.1800   0.7420  101.965  audio_features   \n",
       "\n",
       "                       id                                   uri  \\\n",
       "0  7qiZfU4dY1lWllzX7mPBI3  spotify:track:7qiZfU4dY1lWllzX7mPBI3   \n",
       "1  6PCUP3dWmTjcTtXY02oFdT  spotify:track:6PCUP3dWmTjcTtXY02oFdT   \n",
       "2  5aAx2yezTd8zXrkmtKl66Z  spotify:track:5aAx2yezTd8zXrkmtKl66Z   \n",
       "3  3NdDpSvN911VPGivFlV5d0  spotify:track:3NdDpSvN911VPGivFlV5d0   \n",
       "4  5knuzwU65gJK7IF5yJsuaW  spotify:track:5knuzwU65gJK7IF5yJsuaW   \n",
       "\n",
       "                                          track_href  \\\n",
       "0  https://api.spotify.com/v1/tracks/7qiZfU4dY1lW...   \n",
       "1  https://api.spotify.com/v1/tracks/6PCUP3dWmTjc...   \n",
       "2  https://api.spotify.com/v1/tracks/5aAx2yezTd8z...   \n",
       "3  https://api.spotify.com/v1/tracks/3NdDpSvN911V...   \n",
       "4  https://api.spotify.com/v1/tracks/5knuzwU65gJK...   \n",
       "\n",
       "                                        analysis_url  duration_ms  \\\n",
       "0  https://api.spotify.com/v1/audio-analysis/7qiZ...       233713   \n",
       "1  https://api.spotify.com/v1/audio-analysis/6PCU...       261154   \n",
       "2  https://api.spotify.com/v1/audio-analysis/5aAx...       230453   \n",
       "3  https://api.spotify.com/v1/audio-analysis/3NdD...       245200   \n",
       "4  https://api.spotify.com/v1/audio-analysis/5knu...       251088   \n",
       "\n",
       "   time_signature  \n",
       "0               4  \n",
       "1               4  \n",
       "2               4  \n",
       "3               4  \n",
       "4               4  "
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# building dataframe from features_list, which is a list of dictionaries (each dictionary represents the audio features from one song)\n",
    "greece_17_19_features = pd.DataFrame(features_list_greece_17_19)\n",
    "greece_17_19_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merging audio features into song charts DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30600, 7)"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "greece_17_19.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30600, 18)"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "greece_17_19_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Position</th>\n",
       "      <th>Track Name</th>\n",
       "      <th>Artist</th>\n",
       "      <th>Streams</th>\n",
       "      <th>date</th>\n",
       "      <th>region</th>\n",
       "      <th>spotify_id</th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>key</th>\n",
       "      <th>...</th>\n",
       "      <th>liveness</th>\n",
       "      <th>valence</th>\n",
       "      <th>tempo</th>\n",
       "      <th>type</th>\n",
       "      <th>id</th>\n",
       "      <th>uri</th>\n",
       "      <th>track_href</th>\n",
       "      <th>analysis_url</th>\n",
       "      <th>duration_ms</th>\n",
       "      <th>time_signature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Shape of You</td>\n",
       "      <td>Ed Sheeran</td>\n",
       "      <td>76164</td>\n",
       "      <td>2017-01-06--2017-01-13</td>\n",
       "      <td>gr</td>\n",
       "      <td>7qiZfU4dY1lWllzX7mPBI3</td>\n",
       "      <td>0.825</td>\n",
       "      <td>0.652</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0931</td>\n",
       "      <td>0.9310</td>\n",
       "      <td>95.977</td>\n",
       "      <td>audio_features</td>\n",
       "      <td>7qiZfU4dY1lWllzX7mPBI3</td>\n",
       "      <td>spotify:track:7qiZfU4dY1lWllzX7mPBI3</td>\n",
       "      <td>https://api.spotify.com/v1/tracks/7qiZfU4dY1lW...</td>\n",
       "      <td>https://api.spotify.com/v1/audio-analysis/7qiZ...</td>\n",
       "      <td>233713</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Castle on the Hill</td>\n",
       "      <td>Ed Sheeran</td>\n",
       "      <td>37779</td>\n",
       "      <td>2017-01-06--2017-01-13</td>\n",
       "      <td>gr</td>\n",
       "      <td>6PCUP3dWmTjcTtXY02oFdT</td>\n",
       "      <td>0.461</td>\n",
       "      <td>0.834</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1400</td>\n",
       "      <td>0.4710</td>\n",
       "      <td>135.007</td>\n",
       "      <td>audio_features</td>\n",
       "      <td>6PCUP3dWmTjcTtXY02oFdT</td>\n",
       "      <td>spotify:track:6PCUP3dWmTjcTtXY02oFdT</td>\n",
       "      <td>https://api.spotify.com/v1/tracks/6PCUP3dWmTjc...</td>\n",
       "      <td>https://api.spotify.com/v1/audio-analysis/6PCU...</td>\n",
       "      <td>261154</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Starboy</td>\n",
       "      <td>The Weeknd</td>\n",
       "      <td>34824</td>\n",
       "      <td>2017-01-06--2017-01-13</td>\n",
       "      <td>gr</td>\n",
       "      <td>5aAx2yezTd8zXrkmtKl66Z</td>\n",
       "      <td>0.681</td>\n",
       "      <td>0.594</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1340</td>\n",
       "      <td>0.5350</td>\n",
       "      <td>186.054</td>\n",
       "      <td>audio_features</td>\n",
       "      <td>5aAx2yezTd8zXrkmtKl66Z</td>\n",
       "      <td>spotify:track:5aAx2yezTd8zXrkmtKl66Z</td>\n",
       "      <td>https://api.spotify.com/v1/tracks/5aAx2yezTd8z...</td>\n",
       "      <td>https://api.spotify.com/v1/audio-analysis/5aAx...</td>\n",
       "      <td>230453</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>I Don’t Wanna Live Forever (Fifty Shades Darke...</td>\n",
       "      <td>ZAYN</td>\n",
       "      <td>24781</td>\n",
       "      <td>2017-01-06--2017-01-13</td>\n",
       "      <td>gr</td>\n",
       "      <td>3NdDpSvN911VPGivFlV5d0</td>\n",
       "      <td>0.735</td>\n",
       "      <td>0.451</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.3250</td>\n",
       "      <td>0.0862</td>\n",
       "      <td>117.973</td>\n",
       "      <td>audio_features</td>\n",
       "      <td>3NdDpSvN911VPGivFlV5d0</td>\n",
       "      <td>spotify:track:3NdDpSvN911VPGivFlV5d0</td>\n",
       "      <td>https://api.spotify.com/v1/tracks/3NdDpSvN911V...</td>\n",
       "      <td>https://api.spotify.com/v1/audio-analysis/3NdD...</td>\n",
       "      <td>245200</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Rockabye (feat. Sean Paul &amp; Anne-Marie)</td>\n",
       "      <td>Clean Bandit</td>\n",
       "      <td>22380</td>\n",
       "      <td>2017-01-06--2017-01-13</td>\n",
       "      <td>gr</td>\n",
       "      <td>5knuzwU65gJK7IF5yJsuaW</td>\n",
       "      <td>0.720</td>\n",
       "      <td>0.763</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1800</td>\n",
       "      <td>0.7420</td>\n",
       "      <td>101.965</td>\n",
       "      <td>audio_features</td>\n",
       "      <td>5knuzwU65gJK7IF5yJsuaW</td>\n",
       "      <td>spotify:track:5knuzwU65gJK7IF5yJsuaW</td>\n",
       "      <td>https://api.spotify.com/v1/tracks/5knuzwU65gJK...</td>\n",
       "      <td>https://api.spotify.com/v1/audio-analysis/5knu...</td>\n",
       "      <td>251088</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Position                                         Track Name        Artist  \\\n",
       "0         1                                       Shape of You    Ed Sheeran   \n",
       "1         2                                 Castle on the Hill    Ed Sheeran   \n",
       "2         3                                            Starboy    The Weeknd   \n",
       "3         4  I Don’t Wanna Live Forever (Fifty Shades Darke...          ZAYN   \n",
       "4         5            Rockabye (feat. Sean Paul & Anne-Marie)  Clean Bandit   \n",
       "\n",
       "   Streams                    date region              spotify_id  \\\n",
       "0    76164  2017-01-06--2017-01-13     gr  7qiZfU4dY1lWllzX7mPBI3   \n",
       "1    37779  2017-01-06--2017-01-13     gr  6PCUP3dWmTjcTtXY02oFdT   \n",
       "2    34824  2017-01-06--2017-01-13     gr  5aAx2yezTd8zXrkmtKl66Z   \n",
       "3    24781  2017-01-06--2017-01-13     gr  3NdDpSvN911VPGivFlV5d0   \n",
       "4    22380  2017-01-06--2017-01-13     gr  5knuzwU65gJK7IF5yJsuaW   \n",
       "\n",
       "   danceability  energy  key  ...  liveness  valence    tempo            type  \\\n",
       "0         0.825   0.652    1  ...    0.0931   0.9310   95.977  audio_features   \n",
       "1         0.461   0.834    2  ...    0.1400   0.4710  135.007  audio_features   \n",
       "2         0.681   0.594    7  ...    0.1340   0.5350  186.054  audio_features   \n",
       "3         0.735   0.451    0  ...    0.3250   0.0862  117.973  audio_features   \n",
       "4         0.720   0.763    9  ...    0.1800   0.7420  101.965  audio_features   \n",
       "\n",
       "                       id                                   uri  \\\n",
       "0  7qiZfU4dY1lWllzX7mPBI3  spotify:track:7qiZfU4dY1lWllzX7mPBI3   \n",
       "1  6PCUP3dWmTjcTtXY02oFdT  spotify:track:6PCUP3dWmTjcTtXY02oFdT   \n",
       "2  5aAx2yezTd8zXrkmtKl66Z  spotify:track:5aAx2yezTd8zXrkmtKl66Z   \n",
       "3  3NdDpSvN911VPGivFlV5d0  spotify:track:3NdDpSvN911VPGivFlV5d0   \n",
       "4  5knuzwU65gJK7IF5yJsuaW  spotify:track:5knuzwU65gJK7IF5yJsuaW   \n",
       "\n",
       "                                          track_href  \\\n",
       "0  https://api.spotify.com/v1/tracks/7qiZfU4dY1lW...   \n",
       "1  https://api.spotify.com/v1/tracks/6PCUP3dWmTjc...   \n",
       "2  https://api.spotify.com/v1/tracks/5aAx2yezTd8z...   \n",
       "3  https://api.spotify.com/v1/tracks/3NdDpSvN911V...   \n",
       "4  https://api.spotify.com/v1/tracks/5knuzwU65gJK...   \n",
       "\n",
       "                                        analysis_url duration_ms  \\\n",
       "0  https://api.spotify.com/v1/audio-analysis/7qiZ...      233713   \n",
       "1  https://api.spotify.com/v1/audio-analysis/6PCU...      261154   \n",
       "2  https://api.spotify.com/v1/audio-analysis/5aAx...      230453   \n",
       "3  https://api.spotify.com/v1/audio-analysis/3NdD...      245200   \n",
       "4  https://api.spotify.com/v1/audio-analysis/5knu...      251088   \n",
       "\n",
       "  time_signature  \n",
       "0              4  \n",
       "1              4  \n",
       "2              4  \n",
       "3              4  \n",
       "4              4  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "greece_17_19_df = pd.concat([greece_17_19, greece_17_19_features], axis=1)\n",
    "greece_17_19_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Position</th>\n",
       "      <th>Track Name</th>\n",
       "      <th>Artist</th>\n",
       "      <th>Streams</th>\n",
       "      <th>date</th>\n",
       "      <th>region</th>\n",
       "      <th>spotify_id</th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>key</th>\n",
       "      <th>...</th>\n",
       "      <th>liveness</th>\n",
       "      <th>valence</th>\n",
       "      <th>tempo</th>\n",
       "      <th>type</th>\n",
       "      <th>id</th>\n",
       "      <th>uri</th>\n",
       "      <th>track_href</th>\n",
       "      <th>analysis_url</th>\n",
       "      <th>duration_ms</th>\n",
       "      <th>time_signature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30595</th>\n",
       "      <td>196</td>\n",
       "      <td>Without Me</td>\n",
       "      <td>Halsey</td>\n",
       "      <td>14669</td>\n",
       "      <td>2019-12-20--2019-12-27</td>\n",
       "      <td>gr</td>\n",
       "      <td>5p7ujcrUXASCNwRaWNHR1C</td>\n",
       "      <td>0.752</td>\n",
       "      <td>0.488</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0936</td>\n",
       "      <td>0.533</td>\n",
       "      <td>136.041</td>\n",
       "      <td>audio_features</td>\n",
       "      <td>5p7ujcrUXASCNwRaWNHR1C</td>\n",
       "      <td>spotify:track:5p7ujcrUXASCNwRaWNHR1C</td>\n",
       "      <td>https://api.spotify.com/v1/tracks/5p7ujcrUXASC...</td>\n",
       "      <td>https://api.spotify.com/v1/audio-analysis/5p7u...</td>\n",
       "      <td>201661</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30596</th>\n",
       "      <td>197</td>\n",
       "      <td>Hate Me (with Juice WRLD)</td>\n",
       "      <td>Ellie Goulding</td>\n",
       "      <td>14646</td>\n",
       "      <td>2019-12-20--2019-12-27</td>\n",
       "      <td>gr</td>\n",
       "      <td>6kls8cSlUyHW2BUOkDJIZE</td>\n",
       "      <td>0.657</td>\n",
       "      <td>0.768</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1440</td>\n",
       "      <td>0.759</td>\n",
       "      <td>75.025</td>\n",
       "      <td>audio_features</td>\n",
       "      <td>6kls8cSlUyHW2BUOkDJIZE</td>\n",
       "      <td>spotify:track:6kls8cSlUyHW2BUOkDJIZE</td>\n",
       "      <td>https://api.spotify.com/v1/tracks/6kls8cSlUyHW...</td>\n",
       "      <td>https://api.spotify.com/v1/audio-analysis/6kls...</td>\n",
       "      <td>186223</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30597</th>\n",
       "      <td>198</td>\n",
       "      <td>Santa's Coming for Us</td>\n",
       "      <td>Sia</td>\n",
       "      <td>14645</td>\n",
       "      <td>2019-12-20--2019-12-27</td>\n",
       "      <td>gr</td>\n",
       "      <td>3AlpkljBS1AU7HFVPms8K6</td>\n",
       "      <td>0.669</td>\n",
       "      <td>0.854</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0470</td>\n",
       "      <td>0.707</td>\n",
       "      <td>93.057</td>\n",
       "      <td>audio_features</td>\n",
       "      <td>3AlpkljBS1AU7HFVPms8K6</td>\n",
       "      <td>spotify:track:3AlpkljBS1AU7HFVPms8K6</td>\n",
       "      <td>https://api.spotify.com/v1/tracks/3AlpkljBS1AU...</td>\n",
       "      <td>https://api.spotify.com/v1/audio-analysis/3Alp...</td>\n",
       "      <td>206533</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30598</th>\n",
       "      <td>199</td>\n",
       "      <td>Suge</td>\n",
       "      <td>DaBaby</td>\n",
       "      <td>14612</td>\n",
       "      <td>2019-12-20--2019-12-27</td>\n",
       "      <td>gr</td>\n",
       "      <td>2gwkD6igEhQbDQegRCcdoB</td>\n",
       "      <td>0.876</td>\n",
       "      <td>0.662</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1270</td>\n",
       "      <td>0.844</td>\n",
       "      <td>75.445</td>\n",
       "      <td>audio_features</td>\n",
       "      <td>2gwkD6igEhQbDQegRCcdoB</td>\n",
       "      <td>spotify:track:2gwkD6igEhQbDQegRCcdoB</td>\n",
       "      <td>https://api.spotify.com/v1/tracks/2gwkD6igEhQb...</td>\n",
       "      <td>https://api.spotify.com/v1/audio-analysis/2gwk...</td>\n",
       "      <td>163320</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30599</th>\n",
       "      <td>200</td>\n",
       "      <td>Korakia</td>\n",
       "      <td>LEX</td>\n",
       "      <td>14543</td>\n",
       "      <td>2019-12-20--2019-12-27</td>\n",
       "      <td>gr</td>\n",
       "      <td>4BCjxdM0hXbCEFf0Ck1Dod</td>\n",
       "      <td>0.703</td>\n",
       "      <td>0.655</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1020</td>\n",
       "      <td>0.725</td>\n",
       "      <td>171.980</td>\n",
       "      <td>audio_features</td>\n",
       "      <td>4BCjxdM0hXbCEFf0Ck1Dod</td>\n",
       "      <td>spotify:track:4BCjxdM0hXbCEFf0Ck1Dod</td>\n",
       "      <td>https://api.spotify.com/v1/tracks/4BCjxdM0hXbC...</td>\n",
       "      <td>https://api.spotify.com/v1/audio-analysis/4BCj...</td>\n",
       "      <td>224651</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Position                 Track Name          Artist  Streams  \\\n",
       "30595       196                 Without Me          Halsey    14669   \n",
       "30596       197  Hate Me (with Juice WRLD)  Ellie Goulding    14646   \n",
       "30597       198      Santa's Coming for Us             Sia    14645   \n",
       "30598       199                       Suge          DaBaby    14612   \n",
       "30599       200                    Korakia             LEX    14543   \n",
       "\n",
       "                         date region              spotify_id  danceability  \\\n",
       "30595  2019-12-20--2019-12-27     gr  5p7ujcrUXASCNwRaWNHR1C         0.752   \n",
       "30596  2019-12-20--2019-12-27     gr  6kls8cSlUyHW2BUOkDJIZE         0.657   \n",
       "30597  2019-12-20--2019-12-27     gr  3AlpkljBS1AU7HFVPms8K6         0.669   \n",
       "30598  2019-12-20--2019-12-27     gr  2gwkD6igEhQbDQegRCcdoB         0.876   \n",
       "30599  2019-12-20--2019-12-27     gr  4BCjxdM0hXbCEFf0Ck1Dod         0.703   \n",
       "\n",
       "       energy  key  ...  liveness  valence    tempo            type  \\\n",
       "30595   0.488    6  ...    0.0936    0.533  136.041  audio_features   \n",
       "30596   0.768    8  ...    0.1440    0.759   75.025  audio_features   \n",
       "30597   0.854    1  ...    0.0470    0.707   93.057  audio_features   \n",
       "30598   0.662    2  ...    0.1270    0.844   75.445  audio_features   \n",
       "30599   0.655    3  ...    0.1020    0.725  171.980  audio_features   \n",
       "\n",
       "                           id                                   uri  \\\n",
       "30595  5p7ujcrUXASCNwRaWNHR1C  spotify:track:5p7ujcrUXASCNwRaWNHR1C   \n",
       "30596  6kls8cSlUyHW2BUOkDJIZE  spotify:track:6kls8cSlUyHW2BUOkDJIZE   \n",
       "30597  3AlpkljBS1AU7HFVPms8K6  spotify:track:3AlpkljBS1AU7HFVPms8K6   \n",
       "30598  2gwkD6igEhQbDQegRCcdoB  spotify:track:2gwkD6igEhQbDQegRCcdoB   \n",
       "30599  4BCjxdM0hXbCEFf0Ck1Dod  spotify:track:4BCjxdM0hXbCEFf0Ck1Dod   \n",
       "\n",
       "                                              track_href  \\\n",
       "30595  https://api.spotify.com/v1/tracks/5p7ujcrUXASC...   \n",
       "30596  https://api.spotify.com/v1/tracks/6kls8cSlUyHW...   \n",
       "30597  https://api.spotify.com/v1/tracks/3AlpkljBS1AU...   \n",
       "30598  https://api.spotify.com/v1/tracks/2gwkD6igEhQb...   \n",
       "30599  https://api.spotify.com/v1/tracks/4BCjxdM0hXbC...   \n",
       "\n",
       "                                            analysis_url duration_ms  \\\n",
       "30595  https://api.spotify.com/v1/audio-analysis/5p7u...      201661   \n",
       "30596  https://api.spotify.com/v1/audio-analysis/6kls...      186223   \n",
       "30597  https://api.spotify.com/v1/audio-analysis/3Alp...      206533   \n",
       "30598  https://api.spotify.com/v1/audio-analysis/2gwk...      163320   \n",
       "30599  https://api.spotify.com/v1/audio-analysis/4BCj...      224651   \n",
       "\n",
       "      time_signature  \n",
       "30595              4  \n",
       "30596              4  \n",
       "30597              4  \n",
       "30598              4  \n",
       "30599              4  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "greece_17_19_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 30600 entries, 0 to 30599\n",
      "Data columns (total 25 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   Position          30600 non-null  int64  \n",
      " 1   Track Name        30596 non-null  object \n",
      " 2   Artist            30596 non-null  object \n",
      " 3   Streams           30600 non-null  int64  \n",
      " 4   date              30600 non-null  object \n",
      " 5   region            30600 non-null  object \n",
      " 6   spotify_id        30600 non-null  object \n",
      " 7   danceability      30600 non-null  float64\n",
      " 8   energy            30600 non-null  float64\n",
      " 9   key               30600 non-null  int64  \n",
      " 10  loudness          30600 non-null  float64\n",
      " 11  mode              30600 non-null  int64  \n",
      " 12  speechiness       30600 non-null  float64\n",
      " 13  acousticness      30600 non-null  float64\n",
      " 14  instrumentalness  30600 non-null  float64\n",
      " 15  liveness          30600 non-null  float64\n",
      " 16  valence           30600 non-null  float64\n",
      " 17  tempo             30600 non-null  float64\n",
      " 18  type              30600 non-null  object \n",
      " 19  id                30600 non-null  object \n",
      " 20  uri               30600 non-null  object \n",
      " 21  track_href        30600 non-null  object \n",
      " 22  analysis_url      30600 non-null  object \n",
      " 23  duration_ms       30600 non-null  int64  \n",
      " 24  time_signature    30600 non-null  int64  \n",
      "dtypes: float64(9), int64(6), object(10)\n",
      "memory usage: 5.8+ MB\n"
     ]
    }
   ],
   "source": [
    "greece_17_19_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to clean and pickle combined dataframe! \n",
    "\n",
    "def clean_song_features_df(df, cols_to_drop, pickle_path):\n",
    "    \n",
    "    # copy original dataframe so that it is not altered \n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    # drop unnecessary columns \n",
    "    df_clean.drop(columns=cols_to_drop, inplace=True)\n",
    "    \n",
    "    # convert date column date range to a single day that is the first date in the range (happens to be the Friday of that week)\n",
    "    df_clean['date'] = df_clean['date'].apply(lambda x: x[:10])\n",
    "    \n",
    "    # converting date column to datetime format\n",
    "    df_clean['date'] = pd.to_datetime(df_clean['date'], errors='coerce')\n",
    "    \n",
    "    # setting date column as df index\n",
    "    df_clean.set_index('date', inplace=True)\n",
    "    \n",
    "    # pickle clean dataframe to use in other notebooks \n",
    "    df_clean.to_pickle(pickle_path)\n",
    "    \n",
    "    return df_clean "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = ['type', 'id', 'uri', 'track_href', 'analysis_url']\n",
    "\n",
    "greece_17_19_df = clean_song_features_df(greece_17_19_df, cols_to_drop, '../data/gr_17_19_feat.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 30600 entries, 2017-01-06 to 2019-12-20\n",
      "Data columns (total 19 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   Position          30600 non-null  int64  \n",
      " 1   Track Name        30596 non-null  object \n",
      " 2   Artist            30596 non-null  object \n",
      " 3   Streams           30600 non-null  int64  \n",
      " 4   region            30600 non-null  object \n",
      " 5   spotify_id        30600 non-null  object \n",
      " 6   danceability      30600 non-null  float64\n",
      " 7   energy            30600 non-null  float64\n",
      " 8   key               30600 non-null  int64  \n",
      " 9   loudness          30600 non-null  float64\n",
      " 10  mode              30600 non-null  int64  \n",
      " 11  speechiness       30600 non-null  float64\n",
      " 12  acousticness      30600 non-null  float64\n",
      " 13  instrumentalness  30600 non-null  float64\n",
      " 14  liveness          30600 non-null  float64\n",
      " 15  valence           30600 non-null  float64\n",
      " 16  tempo             30600 non-null  float64\n",
      " 17  duration_ms       30600 non-null  int64  \n",
      " 18  time_signature    30600 non-null  int64  \n",
      "dtypes: float64(9), int64(6), object(4)\n",
      "memory usage: 4.7+ MB\n"
     ]
    }
   ],
   "source": [
    "greece_17_19_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _Now, a function to request the audio features and append them to the dataframe_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_merge_audio_features(song_df, id_col, batchsize=100):\n",
    "    \n",
    "    features_list = []\n",
    "    \n",
    "    None_counter = 0\n",
    "    \n",
    "    for i in range(0, len(song_df[id_col]), batchsize):\n",
    "        \n",
    "        batch = song_df[id_col][i:i+batchsize]\n",
    "        \n",
    "        feature_results = sp.audio_features(batch)\n",
    "        \n",
    "        for i, t in enumerate(feature_results):\n",
    "            if t == None: \n",
    "                None_counter += 1\n",
    "            else: \n",
    "                features_list.append(t)\n",
    "                \n",
    "    print('Number of tracks where no audio features were available:', None_counter)\n",
    "    print('Number of usable tracks:', len(features_list))\n",
    "    \n",
    "    features_df = pd.DataFrame(features_list)\n",
    "    \n",
    "    combined_df = pd.concat([song_df, features_df], axis=1)\n",
    "    \n",
    "    return combined_df "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2020 Data: Requesting audio features, merging to dataframe, cleaning & pickling dataframes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, I will use the `get_merge_audio_features` function to create  new combined DataFrames for 2020 for each of the three countries (Italy, Spain, Greece). Then I will use the `clean_song_features` function to clean the DataFrames so that they are ready for visualization & modeling and also pickle them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Italy 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tracks where no audio features were available: 0\n",
      "Number of usable tracks: 10200\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Position</th>\n",
       "      <th>Track Name</th>\n",
       "      <th>Artist</th>\n",
       "      <th>Streams</th>\n",
       "      <th>date</th>\n",
       "      <th>region</th>\n",
       "      <th>spotify_id</th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>key</th>\n",
       "      <th>...</th>\n",
       "      <th>liveness</th>\n",
       "      <th>valence</th>\n",
       "      <th>tempo</th>\n",
       "      <th>type</th>\n",
       "      <th>id</th>\n",
       "      <th>uri</th>\n",
       "      <th>track_href</th>\n",
       "      <th>analysis_url</th>\n",
       "      <th>duration_ms</th>\n",
       "      <th>time_signature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>blun7 a swishland</td>\n",
       "      <td>tha Supreme</td>\n",
       "      <td>2759917</td>\n",
       "      <td>2020-01-03--2020-01-10</td>\n",
       "      <td>it</td>\n",
       "      <td>7HwvPmK74MBRDhCIyMXReP</td>\n",
       "      <td>0.692</td>\n",
       "      <td>0.792</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>0.255</td>\n",
       "      <td>0.566</td>\n",
       "      <td>129.883</td>\n",
       "      <td>audio_features</td>\n",
       "      <td>7HwvPmK74MBRDhCIyMXReP</td>\n",
       "      <td>spotify:track:7HwvPmK74MBRDhCIyMXReP</td>\n",
       "      <td>https://api.spotify.com/v1/tracks/7HwvPmK74MBR...</td>\n",
       "      <td>https://api.spotify.com/v1/audio-analysis/7Hwv...</td>\n",
       "      <td>167760</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>fuck 3x</td>\n",
       "      <td>tha Supreme</td>\n",
       "      <td>1794877</td>\n",
       "      <td>2020-01-03--2020-01-10</td>\n",
       "      <td>it</td>\n",
       "      <td>5YxP1CkunbhUQVvctFOHa7</td>\n",
       "      <td>0.655</td>\n",
       "      <td>0.814</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.107</td>\n",
       "      <td>0.690</td>\n",
       "      <td>180.082</td>\n",
       "      <td>audio_features</td>\n",
       "      <td>5YxP1CkunbhUQVvctFOHa7</td>\n",
       "      <td>spotify:track:5YxP1CkunbhUQVvctFOHa7</td>\n",
       "      <td>https://api.spotify.com/v1/tracks/5YxP1CkunbhU...</td>\n",
       "      <td>https://api.spotify.com/v1/audio-analysis/5YxP...</td>\n",
       "      <td>161266</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Position         Track Name       Artist  Streams                    date  \\\n",
       "0         1  blun7 a swishland  tha Supreme  2759917  2020-01-03--2020-01-10   \n",
       "1         2            fuck 3x  tha Supreme  1794877  2020-01-03--2020-01-10   \n",
       "\n",
       "  region              spotify_id  danceability  energy  key  ...  liveness  \\\n",
       "0     it  7HwvPmK74MBRDhCIyMXReP         0.692   0.792    7  ...     0.255   \n",
       "1     it  5YxP1CkunbhUQVvctFOHa7         0.655   0.814    5  ...     0.107   \n",
       "\n",
       "   valence    tempo            type                      id  \\\n",
       "0    0.566  129.883  audio_features  7HwvPmK74MBRDhCIyMXReP   \n",
       "1    0.690  180.082  audio_features  5YxP1CkunbhUQVvctFOHa7   \n",
       "\n",
       "                                    uri  \\\n",
       "0  spotify:track:7HwvPmK74MBRDhCIyMXReP   \n",
       "1  spotify:track:5YxP1CkunbhUQVvctFOHa7   \n",
       "\n",
       "                                          track_href  \\\n",
       "0  https://api.spotify.com/v1/tracks/7HwvPmK74MBR...   \n",
       "1  https://api.spotify.com/v1/tracks/5YxP1CkunbhU...   \n",
       "\n",
       "                                        analysis_url duration_ms  \\\n",
       "0  https://api.spotify.com/v1/audio-analysis/7Hwv...      167760   \n",
       "1  https://api.spotify.com/v1/audio-analysis/5YxP...      161266   \n",
       "\n",
       "  time_signature  \n",
       "0              4  \n",
       "1              4  \n",
       "\n",
       "[2 rows x 25 columns]"
      ]
     },
     "execution_count": 371,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "italy_20_df_raw = get_merge_audio_features(italy_2020, 'spotify_id')\n",
    "italy_20_df_raw.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Position</th>\n",
       "      <th>Track Name</th>\n",
       "      <th>Artist</th>\n",
       "      <th>Streams</th>\n",
       "      <th>region</th>\n",
       "      <th>spotify_id</th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>key</th>\n",
       "      <th>loudness</th>\n",
       "      <th>mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>valence</th>\n",
       "      <th>tempo</th>\n",
       "      <th>duration_ms</th>\n",
       "      <th>time_signature</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-01-03</th>\n",
       "      <td>1</td>\n",
       "      <td>blun7 a swishland</td>\n",
       "      <td>tha Supreme</td>\n",
       "      <td>2759917</td>\n",
       "      <td>it</td>\n",
       "      <td>7HwvPmK74MBRDhCIyMXReP</td>\n",
       "      <td>0.692</td>\n",
       "      <td>0.792</td>\n",
       "      <td>7</td>\n",
       "      <td>-5.984</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2450</td>\n",
       "      <td>0.1300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.255</td>\n",
       "      <td>0.566</td>\n",
       "      <td>129.883</td>\n",
       "      <td>167760</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-03</th>\n",
       "      <td>2</td>\n",
       "      <td>fuck 3x</td>\n",
       "      <td>tha Supreme</td>\n",
       "      <td>1794877</td>\n",
       "      <td>it</td>\n",
       "      <td>5YxP1CkunbhUQVvctFOHa7</td>\n",
       "      <td>0.655</td>\n",
       "      <td>0.814</td>\n",
       "      <td>5</td>\n",
       "      <td>-5.472</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0823</td>\n",
       "      <td>0.0698</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.107</td>\n",
       "      <td>0.690</td>\n",
       "      <td>180.082</td>\n",
       "      <td>161266</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-03</th>\n",
       "      <td>3</td>\n",
       "      <td>Dance Monkey</td>\n",
       "      <td>Tones And I</td>\n",
       "      <td>1758744</td>\n",
       "      <td>it</td>\n",
       "      <td>1rgnBhdG2JDFTbYkYRZAku</td>\n",
       "      <td>0.825</td>\n",
       "      <td>0.593</td>\n",
       "      <td>6</td>\n",
       "      <td>-6.401</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0988</td>\n",
       "      <td>0.6880</td>\n",
       "      <td>0.000161</td>\n",
       "      <td>0.170</td>\n",
       "      <td>0.540</td>\n",
       "      <td>98.078</td>\n",
       "      <td>209755</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-03</th>\n",
       "      <td>4</td>\n",
       "      <td>Ti volevo dedicare (feat. J-AX &amp; Boomdabash)</td>\n",
       "      <td>Rocco Hunt</td>\n",
       "      <td>1551080</td>\n",
       "      <td>it</td>\n",
       "      <td>00GxbkrW4m1Tac5xySEJ4M</td>\n",
       "      <td>0.754</td>\n",
       "      <td>0.725</td>\n",
       "      <td>8</td>\n",
       "      <td>-6.058</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0661</td>\n",
       "      <td>0.0104</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.192</td>\n",
       "      <td>0.271</td>\n",
       "      <td>120.002</td>\n",
       "      <td>208133</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-03</th>\n",
       "      <td>5</td>\n",
       "      <td>ANSIA NO</td>\n",
       "      <td>FSK SATELLITE</td>\n",
       "      <td>1548224</td>\n",
       "      <td>it</td>\n",
       "      <td>2yuYI5NFhevxa05se7Qht9</td>\n",
       "      <td>0.823</td>\n",
       "      <td>0.678</td>\n",
       "      <td>1</td>\n",
       "      <td>-6.664</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5600</td>\n",
       "      <td>0.1530</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.218</td>\n",
       "      <td>0.706</td>\n",
       "      <td>160.133</td>\n",
       "      <td>148500</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Position                                    Track Name  \\\n",
       "date                                                                 \n",
       "2020-01-03         1                             blun7 a swishland   \n",
       "2020-01-03         2                                       fuck 3x   \n",
       "2020-01-03         3                                  Dance Monkey   \n",
       "2020-01-03         4  Ti volevo dedicare (feat. J-AX & Boomdabash)   \n",
       "2020-01-03         5                                      ANSIA NO   \n",
       "\n",
       "                   Artist  Streams region              spotify_id  \\\n",
       "date                                                                \n",
       "2020-01-03    tha Supreme  2759917     it  7HwvPmK74MBRDhCIyMXReP   \n",
       "2020-01-03    tha Supreme  1794877     it  5YxP1CkunbhUQVvctFOHa7   \n",
       "2020-01-03    Tones And I  1758744     it  1rgnBhdG2JDFTbYkYRZAku   \n",
       "2020-01-03     Rocco Hunt  1551080     it  00GxbkrW4m1Tac5xySEJ4M   \n",
       "2020-01-03  FSK SATELLITE  1548224     it  2yuYI5NFhevxa05se7Qht9   \n",
       "\n",
       "            danceability  energy  key  loudness  mode  speechiness  \\\n",
       "date                                                                 \n",
       "2020-01-03         0.692   0.792    7    -5.984     1       0.2450   \n",
       "2020-01-03         0.655   0.814    5    -5.472     0       0.0823   \n",
       "2020-01-03         0.825   0.593    6    -6.401     0       0.0988   \n",
       "2020-01-03         0.754   0.725    8    -6.058     1       0.0661   \n",
       "2020-01-03         0.823   0.678    1    -6.664     1       0.5600   \n",
       "\n",
       "            acousticness  instrumentalness  liveness  valence    tempo  \\\n",
       "date                                                                     \n",
       "2020-01-03        0.1300          0.000000     0.255    0.566  129.883   \n",
       "2020-01-03        0.0698          0.000000     0.107    0.690  180.082   \n",
       "2020-01-03        0.6880          0.000161     0.170    0.540   98.078   \n",
       "2020-01-03        0.0104          0.000000     0.192    0.271  120.002   \n",
       "2020-01-03        0.1530          0.000000     0.218    0.706  160.133   \n",
       "\n",
       "            duration_ms  time_signature  \n",
       "date                                     \n",
       "2020-01-03       167760               4  \n",
       "2020-01-03       161266               4  \n",
       "2020-01-03       209755               4  \n",
       "2020-01-03       208133               4  \n",
       "2020-01-03       148500               4  "
      ]
     },
     "execution_count": 373,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "italy_20_df = clean_song_features_df(italy_20_df_raw, cols_to_drop, '../data/it_20_feat.pkl')\n",
    "italy_20_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spain 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tracks where no audio features were available: 1\n",
      "Number of usable tracks: 20400\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Position</th>\n",
       "      <th>Track Name</th>\n",
       "      <th>Artist</th>\n",
       "      <th>Streams</th>\n",
       "      <th>date</th>\n",
       "      <th>region</th>\n",
       "      <th>spotify_id</th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>key</th>\n",
       "      <th>...</th>\n",
       "      <th>liveness</th>\n",
       "      <th>valence</th>\n",
       "      <th>tempo</th>\n",
       "      <th>type</th>\n",
       "      <th>id</th>\n",
       "      <th>uri</th>\n",
       "      <th>track_href</th>\n",
       "      <th>analysis_url</th>\n",
       "      <th>duration_ms</th>\n",
       "      <th>time_signature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Tusa</td>\n",
       "      <td>KAROL G</td>\n",
       "      <td>3041607</td>\n",
       "      <td>2020-01-03--2020-01-10</td>\n",
       "      <td>es</td>\n",
       "      <td>7k4t7uLgtOxPwTpFmtJNTY</td>\n",
       "      <td>0.803</td>\n",
       "      <td>0.715</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0574</td>\n",
       "      <td>0.574</td>\n",
       "      <td>101.085</td>\n",
       "      <td>audio_features</td>\n",
       "      <td>7k4t7uLgtOxPwTpFmtJNTY</td>\n",
       "      <td>spotify:track:7k4t7uLgtOxPwTpFmtJNTY</td>\n",
       "      <td>https://api.spotify.com/v1/tracks/7k4t7uLgtOxP...</td>\n",
       "      <td>https://api.spotify.com/v1/audio-analysis/7k4t...</td>\n",
       "      <td>200960.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Alocao (With Bad Gyal)</td>\n",
       "      <td>Omar Montes</td>\n",
       "      <td>1960463</td>\n",
       "      <td>2020-01-03--2020-01-10</td>\n",
       "      <td>es</td>\n",
       "      <td>6RyuoOJXNzlVWpfC5xQyeI</td>\n",
       "      <td>0.673</td>\n",
       "      <td>0.752</td>\n",
       "      <td>11.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1060</td>\n",
       "      <td>0.699</td>\n",
       "      <td>178.005</td>\n",
       "      <td>audio_features</td>\n",
       "      <td>6RyuoOJXNzlVWpfC5xQyeI</td>\n",
       "      <td>spotify:track:6RyuoOJXNzlVWpfC5xQyeI</td>\n",
       "      <td>https://api.spotify.com/v1/tracks/6RyuoOJXNzlV...</td>\n",
       "      <td>https://api.spotify.com/v1/audio-analysis/6Ryu...</td>\n",
       "      <td>209320.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Position              Track Name       Artist  Streams  \\\n",
       "0        1                    Tusa      KAROL G  3041607   \n",
       "1        2  Alocao (With Bad Gyal)  Omar Montes  1960463   \n",
       "\n",
       "                     date region              spotify_id  danceability  \\\n",
       "0  2020-01-03--2020-01-10     es  7k4t7uLgtOxPwTpFmtJNTY         0.803   \n",
       "1  2020-01-03--2020-01-10     es  6RyuoOJXNzlVWpfC5xQyeI         0.673   \n",
       "\n",
       "   energy   key  ...  liveness  valence    tempo            type  \\\n",
       "0   0.715   2.0  ...    0.0574    0.574  101.085  audio_features   \n",
       "1   0.752  11.0  ...    0.1060    0.699  178.005  audio_features   \n",
       "\n",
       "                       id                                   uri  \\\n",
       "0  7k4t7uLgtOxPwTpFmtJNTY  spotify:track:7k4t7uLgtOxPwTpFmtJNTY   \n",
       "1  6RyuoOJXNzlVWpfC5xQyeI  spotify:track:6RyuoOJXNzlVWpfC5xQyeI   \n",
       "\n",
       "                                          track_href  \\\n",
       "0  https://api.spotify.com/v1/tracks/7k4t7uLgtOxP...   \n",
       "1  https://api.spotify.com/v1/tracks/6RyuoOJXNzlV...   \n",
       "\n",
       "                                        analysis_url duration_ms  \\\n",
       "0  https://api.spotify.com/v1/audio-analysis/7k4t...    200960.0   \n",
       "1  https://api.spotify.com/v1/audio-analysis/6Ryu...    209320.0   \n",
       "\n",
       "  time_signature  \n",
       "0            4.0  \n",
       "1            4.0  \n",
       "\n",
       "[2 rows x 25 columns]"
      ]
     },
     "execution_count": 379,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spain_20_df_raw = get_merge_audio_features(spain_2020, 'spotify_id')\n",
    "spain_20_df_raw.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Position</th>\n",
       "      <th>Track Name</th>\n",
       "      <th>Artist</th>\n",
       "      <th>Streams</th>\n",
       "      <th>region</th>\n",
       "      <th>spotify_id</th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>key</th>\n",
       "      <th>loudness</th>\n",
       "      <th>mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>valence</th>\n",
       "      <th>tempo</th>\n",
       "      <th>duration_ms</th>\n",
       "      <th>time_signature</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-01-03</th>\n",
       "      <td>1</td>\n",
       "      <td>Tusa</td>\n",
       "      <td>KAROL G</td>\n",
       "      <td>3041607</td>\n",
       "      <td>es</td>\n",
       "      <td>7k4t7uLgtOxPwTpFmtJNTY</td>\n",
       "      <td>0.803</td>\n",
       "      <td>0.715</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-3.280</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2980</td>\n",
       "      <td>0.295</td>\n",
       "      <td>0.000134</td>\n",
       "      <td>0.0574</td>\n",
       "      <td>0.574</td>\n",
       "      <td>101.085</td>\n",
       "      <td>200960.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-03</th>\n",
       "      <td>2</td>\n",
       "      <td>Alocao (With Bad Gyal)</td>\n",
       "      <td>Omar Montes</td>\n",
       "      <td>1960463</td>\n",
       "      <td>es</td>\n",
       "      <td>6RyuoOJXNzlVWpfC5xQyeI</td>\n",
       "      <td>0.673</td>\n",
       "      <td>0.752</td>\n",
       "      <td>11.0</td>\n",
       "      <td>-4.705</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0567</td>\n",
       "      <td>0.214</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.1060</td>\n",
       "      <td>0.699</td>\n",
       "      <td>178.005</td>\n",
       "      <td>209320.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Position              Track Name       Artist  Streams region  \\\n",
       "date                                                                       \n",
       "2020-01-03        1                    Tusa      KAROL G  3041607     es   \n",
       "2020-01-03        2  Alocao (With Bad Gyal)  Omar Montes  1960463     es   \n",
       "\n",
       "                        spotify_id  danceability  energy   key  loudness  \\\n",
       "date                                                                       \n",
       "2020-01-03  7k4t7uLgtOxPwTpFmtJNTY         0.803   0.715   2.0    -3.280   \n",
       "2020-01-03  6RyuoOJXNzlVWpfC5xQyeI         0.673   0.752  11.0    -4.705   \n",
       "\n",
       "            mode  speechiness  acousticness  instrumentalness  liveness  \\\n",
       "date                                                                      \n",
       "2020-01-03   1.0       0.2980         0.295          0.000134    0.0574   \n",
       "2020-01-03   0.0       0.0567         0.214          0.000002    0.1060   \n",
       "\n",
       "            valence    tempo  duration_ms  time_signature  \n",
       "date                                                       \n",
       "2020-01-03    0.574  101.085     200960.0             4.0  \n",
       "2020-01-03    0.699  178.005     209320.0             4.0  "
      ]
     },
     "execution_count": 402,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spain_20_df = clean_song_features_df(spain_20_df_raw, cols_to_drop, '../data/sp_20_feat.pkl')\n",
    "spain_20_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Greece 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tracks where no audio features were available: 0\n",
      "Number of usable tracks: 10200\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Position</th>\n",
       "      <th>Track Name</th>\n",
       "      <th>Artist</th>\n",
       "      <th>Streams</th>\n",
       "      <th>date</th>\n",
       "      <th>region</th>\n",
       "      <th>spotify_id</th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>key</th>\n",
       "      <th>...</th>\n",
       "      <th>liveness</th>\n",
       "      <th>valence</th>\n",
       "      <th>tempo</th>\n",
       "      <th>type</th>\n",
       "      <th>id</th>\n",
       "      <th>uri</th>\n",
       "      <th>track_href</th>\n",
       "      <th>analysis_url</th>\n",
       "      <th>duration_ms</th>\n",
       "      <th>time_signature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>BIG MAN</td>\n",
       "      <td>SNIK</td>\n",
       "      <td>89921</td>\n",
       "      <td>2020-01-03--2020-01-10</td>\n",
       "      <td>gr</td>\n",
       "      <td>7qd01xrME77eyWqGPumC2J</td>\n",
       "      <td>0.850</td>\n",
       "      <td>0.854</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0976</td>\n",
       "      <td>0.481</td>\n",
       "      <td>160.040</td>\n",
       "      <td>audio_features</td>\n",
       "      <td>7qd01xrME77eyWqGPumC2J</td>\n",
       "      <td>spotify:track:7qd01xrME77eyWqGPumC2J</td>\n",
       "      <td>https://api.spotify.com/v1/tracks/7qd01xrME77e...</td>\n",
       "      <td>https://api.spotify.com/v1/audio-analysis/7qd0...</td>\n",
       "      <td>181500</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Dance Monkey</td>\n",
       "      <td>Tones And I</td>\n",
       "      <td>87829</td>\n",
       "      <td>2020-01-03--2020-01-10</td>\n",
       "      <td>gr</td>\n",
       "      <td>1rgnBhdG2JDFTbYkYRZAku</td>\n",
       "      <td>0.825</td>\n",
       "      <td>0.593</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1700</td>\n",
       "      <td>0.540</td>\n",
       "      <td>98.078</td>\n",
       "      <td>audio_features</td>\n",
       "      <td>1rgnBhdG2JDFTbYkYRZAku</td>\n",
       "      <td>spotify:track:1rgnBhdG2JDFTbYkYRZAku</td>\n",
       "      <td>https://api.spotify.com/v1/tracks/1rgnBhdG2JDF...</td>\n",
       "      <td>https://api.spotify.com/v1/audio-analysis/1rgn...</td>\n",
       "      <td>209755</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Position    Track Name       Artist  Streams                    date  \\\n",
       "0         1       BIG MAN         SNIK    89921  2020-01-03--2020-01-10   \n",
       "1         2  Dance Monkey  Tones And I    87829  2020-01-03--2020-01-10   \n",
       "\n",
       "  region              spotify_id  danceability  energy  key  ...  liveness  \\\n",
       "0     gr  7qd01xrME77eyWqGPumC2J         0.850   0.854    1  ...    0.0976   \n",
       "1     gr  1rgnBhdG2JDFTbYkYRZAku         0.825   0.593    6  ...    0.1700   \n",
       "\n",
       "   valence    tempo            type                      id  \\\n",
       "0    0.481  160.040  audio_features  7qd01xrME77eyWqGPumC2J   \n",
       "1    0.540   98.078  audio_features  1rgnBhdG2JDFTbYkYRZAku   \n",
       "\n",
       "                                    uri  \\\n",
       "0  spotify:track:7qd01xrME77eyWqGPumC2J   \n",
       "1  spotify:track:1rgnBhdG2JDFTbYkYRZAku   \n",
       "\n",
       "                                          track_href  \\\n",
       "0  https://api.spotify.com/v1/tracks/7qd01xrME77e...   \n",
       "1  https://api.spotify.com/v1/tracks/1rgnBhdG2JDF...   \n",
       "\n",
       "                                        analysis_url duration_ms  \\\n",
       "0  https://api.spotify.com/v1/audio-analysis/7qd0...      181500   \n",
       "1  https://api.spotify.com/v1/audio-analysis/1rgn...      209755   \n",
       "\n",
       "  time_signature  \n",
       "0              4  \n",
       "1              4  \n",
       "\n",
       "[2 rows x 25 columns]"
      ]
     },
     "execution_count": 400,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "greece_20_df_raw = get_merge_audio_features(greece_2020, 'spotify_id')\n",
    "greece_20_df_raw.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Position</th>\n",
       "      <th>Track Name</th>\n",
       "      <th>Artist</th>\n",
       "      <th>Streams</th>\n",
       "      <th>region</th>\n",
       "      <th>spotify_id</th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>key</th>\n",
       "      <th>loudness</th>\n",
       "      <th>mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>valence</th>\n",
       "      <th>tempo</th>\n",
       "      <th>duration_ms</th>\n",
       "      <th>time_signature</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-01-03</th>\n",
       "      <td>1</td>\n",
       "      <td>BIG MAN</td>\n",
       "      <td>SNIK</td>\n",
       "      <td>89921</td>\n",
       "      <td>gr</td>\n",
       "      <td>7qd01xrME77eyWqGPumC2J</td>\n",
       "      <td>0.850</td>\n",
       "      <td>0.854</td>\n",
       "      <td>1</td>\n",
       "      <td>-4.176</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2340</td>\n",
       "      <td>0.0932</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0976</td>\n",
       "      <td>0.481</td>\n",
       "      <td>160.040</td>\n",
       "      <td>181500</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-03</th>\n",
       "      <td>2</td>\n",
       "      <td>Dance Monkey</td>\n",
       "      <td>Tones And I</td>\n",
       "      <td>87829</td>\n",
       "      <td>gr</td>\n",
       "      <td>1rgnBhdG2JDFTbYkYRZAku</td>\n",
       "      <td>0.825</td>\n",
       "      <td>0.593</td>\n",
       "      <td>6</td>\n",
       "      <td>-6.401</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0988</td>\n",
       "      <td>0.6880</td>\n",
       "      <td>0.000161</td>\n",
       "      <td>0.1700</td>\n",
       "      <td>0.540</td>\n",
       "      <td>98.078</td>\n",
       "      <td>209755</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Position    Track Name       Artist  Streams region  \\\n",
       "date                                                              \n",
       "2020-01-03         1       BIG MAN         SNIK    89921     gr   \n",
       "2020-01-03         2  Dance Monkey  Tones And I    87829     gr   \n",
       "\n",
       "                        spotify_id  danceability  energy  key  loudness  mode  \\\n",
       "date                                                                            \n",
       "2020-01-03  7qd01xrME77eyWqGPumC2J         0.850   0.854    1    -4.176     0   \n",
       "2020-01-03  1rgnBhdG2JDFTbYkYRZAku         0.825   0.593    6    -6.401     0   \n",
       "\n",
       "            speechiness  acousticness  instrumentalness  liveness  valence  \\\n",
       "date                                                                         \n",
       "2020-01-03       0.2340        0.0932          0.000000    0.0976    0.481   \n",
       "2020-01-03       0.0988        0.6880          0.000161    0.1700    0.540   \n",
       "\n",
       "              tempo  duration_ms  time_signature  \n",
       "date                                              \n",
       "2020-01-03  160.040       181500               4  \n",
       "2020-01-03   98.078       209755               4  "
      ]
     },
     "execution_count": 403,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "greece_20_df = clean_song_features_df(greece_20_df_raw, cols_to_drop, '../data/gr_20_feat.pkl')\n",
    "greece_20_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
